{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# GNN con TODOS los Biomarcadores (LEFT JOIN + Missing Indicators)\n",
    "Este notebook usa **TODOS los pacientes (6,210 visitas)** con una estrategia inteligente:\n",
    "\n",
    "**Estrategia LEFT JOIN:**\n",
    "- Empezamos con TODAS las visitas demogrÃ¡ficas\n",
    "- AÃ±adimos CSF, PET, MRI cuando estÃ¡n disponibles\n",
    "- Cuando faltan: usamos 0 + aÃ±adimos indicadores binarios (HAS_CSF, HAS_PET, HAS_MRI)\n",
    "- El GNN aprende a usar biomarcadores cuando estÃ¡n disponibles\n",
    "\n",
    "**Features (~30):**\n",
    "- Demographics: AGE, EDUCATION, GENDER, MARITAL_STATUS\n",
    "- CSF: ABETA42, TAU, PTAU + ratios (cuando disponible)\n",
    "- PET: CENTILOIDS, SUMMARY_SUVR (cuando disponible)\n",
    "- MRI: 7 volÃºmenes cerebrales (cuando disponible)\n",
    "- Missing indicators: HAS_CSF, HAS_PET, HAS_MRI\n",
    "\n",
    "**Ventaja**: MÃ¡ximo dataset (6,210 visitas) = Mejores resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics loaded: (6210, 84)\n",
      "Columns: ['PHASE', 'PTID', 'RID', 'VISCODE', 'VISCODE2', 'VISDATE', 'PTSOURCE', 'PTGENDER', 'PTDOB', 'PTDOBYY']...\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Demographics loaded: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "utilities",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined.\n"
     ]
    }
   ],
   "source": [
    "def norm_codes_to_labels(s: pd.Series, mapping: dict) -> pd.Series:\n",
    "    out = s.astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    out = out.map(mapping)\n",
    "    return out\n",
    "\n",
    "gender_map = {\"1\":\"male\",\"2\":\"female\",\"male\":\"male\",\"female\":\"female\",\"m\":\"male\",\"f\":\"female\"}\n",
    "marry_map  = {\"1\":\"married\",\"2\":\"widowed\",\"3\":\"divorced\",\"4\":\"never_married\",\"6\":\"domestic_partnership\"}\n",
    "\n",
    "def to_year(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s.where((s >= 1900) & (s <= 2100))\n",
    "    return s\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "process_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data processed. Shape: (6210, 85)\n",
      "Patients with YEAR_ONSET: 2908\n"
     ]
    }
   ],
   "source": [
    "\n",
    "onset_cols = [c for c in [\"PTCOGBEG\",\"PTADBEG\",\"PTADDX\"] if c in df.columns]\n",
    "for c in onset_cols:\n",
    "    df[c] = to_year(df[c])\n",
    "\n",
    "def row_min_nonnull(row):\n",
    "    vals = [row[c] for c in onset_cols if pd.notna(row[c])]\n",
    "    return min(vals) if vals else np.nan\n",
    "\n",
    "df[\"YEAR_ONSET\"] = df.apply(row_min_nonnull, axis=1) if onset_cols else np.nan\n",
    "df[\"YEAR_ONSET\"] = to_year(df[\"YEAR_ONSET\"])\n",
    "\n",
    "for c in [\"PTDOBYY\",\"PTEDUCAT\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "if \"PTGENDER\" in df.columns:\n",
    "    df[\"PTGENDER\"] = norm_codes_to_labels(df[\"PTGENDER\"], gender_map)\n",
    "if \"PTMARRY\" in df.columns:\n",
    "    df[\"PTMARRY\"]  = norm_codes_to_labels(df[\"PTMARRY\"], marry_map)\n",
    "\n",
    "print(f\"\\nData processed. Shape: {df.shape}\")\n",
    "print(f\"Patients with YEAR_ONSET: {df['YEAR_ONSET'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "jejmznw2mgn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CSF merged (LEFT JOIN): 1780/6210 visitas con CSF (28.7%)\n"
     ]
    }
   ],
   "source": [
    "biomarker_path = \"./data/adni/demographics/UPENNBIOMK_ROCHE_ELECSYS_11Oct2025.csv\"\n",
    "df_csf = pd.read_csv(biomarker_path)\n",
    "\n",
    "df_csf['VISCODE_NORMALIZED'] = df_csf['VISCODE2'].astype(str).str.strip()\n",
    "df_csf = df_csf.dropna(subset=['ABETA42', 'TAU', 'PTAU'])\n",
    "\n",
    "df_csf['TAU_ABETA42_RATIO'] = df_csf['TAU'] / (df_csf['ABETA42'] + 1e-6)\n",
    "df_csf['PTAU_ABETA42_RATIO'] = df_csf['PTAU'] / (df_csf['ABETA42'] + 1e-6)\n",
    "df_csf['PTAU_TAU_RATIO'] = df_csf['PTAU'] / (df_csf['TAU'] + 1e-6)\n",
    "\n",
    "df['VISCODE_NORMALIZED'] = df['VISCODE2'].astype(str).str.strip().replace({'sc': 'bl', 'f': 'bl', 'nan': ''})\n",
    "df = df.merge(\n",
    "    df_csf[['RID', 'VISCODE_NORMALIZED', 'ABETA42', 'TAU', 'PTAU', \n",
    "            'TAU_ABETA42_RATIO', 'PTAU_ABETA42_RATIO', 'PTAU_TAU_RATIO']],\n",
    "    on=['RID', 'VISCODE_NORMALIZED'],\n",
    "    how='left'  # LEFT JOIN - mantener todas las visitas\n",
    ")\n",
    "\n",
    "df['HAS_CSF'] = df['ABETA42'].notna().astype(float)\n",
    "\n",
    "print(f\"âœ… CSF merged (LEFT JOIN): {df['HAS_CSF'].sum():.0f}/{len(df)} visitas con CSF ({100*df['HAS_CSF'].mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bc4xqv58cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PET merged (LEFT JOIN): 810/6212 visitas con PET (13.0%)\n"
     ]
    }
   ],
   "source": [
    "pet_path = \"./data/adni/demographics/All_Subjects_UCBERKELEY_AMY_6MM_11Oct2025.csv\"\n",
    "df_pet = pd.read_csv(pet_path, low_memory=False)\n",
    "\n",
    "df_pet['VISCODE_NORMALIZED'] = df_pet['VISCODE'].astype(str).str.strip().replace({'sc': 'bl', 'f': 'bl', 'nan': ''})\n",
    "df_pet = df_pet[['RID', 'VISCODE_NORMALIZED', 'CENTILOIDS', 'SUMMARY_SUVR', 'COMPOSITE_REF_SUVR']].copy()\n",
    "df_pet.columns = ['RID', 'VISCODE_NORMALIZED', 'PET_CENTILOIDS', 'PET_SUVR', 'PET_COMPOSITE']\n",
    "df_pet = df_pet.dropna(subset=['PET_CENTILOIDS', 'PET_SUVR'])\n",
    "\n",
    "df = df.merge(df_pet, on=['RID', 'VISCODE_NORMALIZED'], how='left')\n",
    "df['HAS_PET'] = df['PET_CENTILOIDS'].notna().astype(float)\n",
    "\n",
    "print(f\"âœ… PET merged (LEFT JOIN): {df['HAS_PET'].sum():.0f}/{len(df)} visitas con PET ({100*df['HAS_PET'].mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "o5e0itu8pw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MRI merged (LEFT JOIN): 3303/6488 visitas con MRI (50.9%)\n",
      "\n",
      "ðŸ“Š Dataset final: 6488 visitas con todos los biomarcadores opcionales\n"
     ]
    }
   ],
   "source": [
    "mri_path = \"./data/adni/demographics/All_Subjects_UCSFFSX7_11Oct2025.csv\"\n",
    "df_mri = pd.read_csv(mri_path, low_memory=False)\n",
    "\n",
    "df_mri['VISCODE_NORMALIZED'] = df_mri['VISCODE2'].astype(str).str.strip().replace({'sc': 'bl', 'f': 'bl', 'nan': ''})\n",
    "df_mri = df_mri[['RID', 'VISCODE_NORMALIZED', 'ST101SV', 'ST11SV', 'ST12SV', \n",
    "                 'ST4SV', 'ST5SV', 'ST17SV', 'ST18SV']].copy()\n",
    "df_mri.columns = ['RID', 'VISCODE_NORMALIZED', 'MRI_eTIV', 'MRI_Vol1', 'MRI_Vol2', \n",
    "                  'MRI_Vol3', 'MRI_Vol4', 'MRI_Vol5', 'MRI_Vol6']\n",
    "df_mri = df_mri.dropna(subset=['MRI_eTIV', 'MRI_Vol1'])\n",
    "\n",
    "df = df.merge(df_mri, on=['RID', 'VISCODE_NORMALIZED'], how='left')\n",
    "df['HAS_MRI'] = df['MRI_eTIV'].notna().astype(float)\n",
    "\n",
    "df = df.drop(columns=['VISCODE_NORMALIZED'])\n",
    "\n",
    "print(f\"âœ… MRI merged (LEFT JOIN): {df['HAS_MRI'].sum():.0f}/{len(df)} visitas con MRI ({100*df['HAS_MRI'].mean():.1f}%)\")\n",
    "print(f\"\\nðŸ“Š Dataset final: {len(df)} visitas con todos los biomarcadores opcionales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "prepare_visits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visits prepared: 6488\n",
      "Labeled visits (with YEARS_TO_ONSET): 83\n",
      "Using date column: VISDATE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "date_col = \"EXAMDATE\" if \"EXAMDATE\" in df.columns else (\"VISDATE\" if \"VISDATE\" in df.columns else None)\n",
    "if not date_col:\n",
    "    raise ValueError(\"No se encuentra columna de fecha (VISDATE/EXAMDATE)\")\n",
    "\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "df[\"EXAM_YEAR\"] = to_year(df[date_col].dt.year)\n",
    "\n",
    "df[\"AGE_AT_VISIT\"] = np.where(\n",
    "    df[\"EXAM_YEAR\"].notna() & df[\"PTDOBYY\"].notna(),\n",
    "    df[\"EXAM_YEAR\"] - df[\"PTDOBYY\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df[\"YEARS_TO_ONSET\"] = np.where(\n",
    "    df[\"YEAR_ONSET\"].notna() & df[\"EXAM_YEAR\"].notna(),\n",
    "    df[\"YEAR_ONSET\"] - df[\"EXAM_YEAR\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df.loc[(df[\"YEARS_TO_ONSET\"] < 0) & df[\"YEAR_ONSET\"].notna(), \"YEARS_TO_ONSET\"] = np.nan\n",
    "df.loc[df[\"YEARS_TO_ONSET\"] > 50, \"YEARS_TO_ONSET\"] = np.nan\n",
    "\n",
    "df[\"HAS_LABEL\"] = df[\"YEARS_TO_ONSET\"].notna()\n",
    "\n",
    "print(f\"\\nVisits prepared: {len(df)}\")\n",
    "print(f\"Labeled visits (with YEARS_TO_ONSET): {df['HAS_LABEL'].sum()}\")\n",
    "print(f\"Using date column: {date_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "build_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features (21):\n",
      "  Demographics: 2\n",
      "  CSF biomarkers: 6\n",
      "  PET imaging: 3\n",
      "  MRI volumetric: 7\n",
      "  Missing indicators: 3\n",
      "Categorical (one-hot): 7 columns\n",
      "\n",
      "âœ… Feature matrix (ALL BIOMARKERS with missing handling): (6488, 28)\n",
      "Total features: 28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "csf_cols = ['ABETA42', 'TAU', 'PTAU', 'TAU_ABETA42_RATIO', 'PTAU_ABETA42_RATIO', 'PTAU_TAU_RATIO']\n",
    "pet_cols = ['PET_CENTILOIDS', 'PET_SUVR', 'PET_COMPOSITE']\n",
    "mri_cols = ['MRI_eTIV', 'MRI_Vol1', 'MRI_Vol2', 'MRI_Vol3', 'MRI_Vol4', 'MRI_Vol5', 'MRI_Vol6']\n",
    "missing_indicators = ['HAS_CSF', 'HAS_PET', 'HAS_MRI']\n",
    "\n",
    "num_cols = [c for c in [\"AGE_AT_VISIT\", \"PTEDUCAT\"] + csf_cols + pet_cols + mri_cols + missing_indicators if c in df.columns]\n",
    "\n",
    "cat_cols = [c for c in [\"PTGENDER\", \"PTMARRY\"] if c in df.columns]\n",
    "\n",
    "for c in csf_cols + pet_cols + mri_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "for c in [\"AGE_AT_VISIT\", \"PTEDUCAT\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(df[c].median())\n",
    "\n",
    "parts = []\n",
    "\n",
    "if num_cols:\n",
    "    scaler = StandardScaler()\n",
    "    X_num = pd.DataFrame(\n",
    "        scaler.fit_transform(df[num_cols]),\n",
    "        columns=num_cols,\n",
    "        index=df.index\n",
    "    )\n",
    "    parts.append(X_num)\n",
    "    print(f\"Numeric features ({len(num_cols)}):\")\n",
    "    print(f\"  Demographics: 2\")\n",
    "    print(f\"  CSF biomarkers: {len(csf_cols)}\")\n",
    "    print(f\"  PET imaging: {len(pet_cols)}\")\n",
    "    print(f\"  MRI volumetric: {len(mri_cols)}\")\n",
    "    print(f\"  Missing indicators: {len(missing_indicators)}\")\n",
    "\n",
    "if cat_cols:\n",
    "    X_cat = pd.get_dummies(df[cat_cols], prefix=cat_cols, drop_first=False, dtype=float)\n",
    "    parts.append(X_cat)\n",
    "    print(f\"Categorical (one-hot): {X_cat.shape[1]} columns\")\n",
    "\n",
    "if not parts:\n",
    "    raise ValueError(\"No features available\")\n",
    "\n",
    "X = pd.concat(parts, axis=1).astype(float)\n",
    "X_clean = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "print(f\"\\nâœ… Feature matrix (ALL BIOMARKERS with missing handling): {X_clean.shape}\")\n",
    "print(f\"Total features: {X_clean.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "build_graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph constructed:\n",
      "  Nodes: 6488\n",
      "  kNN edges: 51904\n",
      "  Temporal edges: 1542\n",
      "  Total edges (undirected, unique): 73722\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_samples = X_clean.shape[0]\n",
    "k = min(8, max(1, n_samples - 1))\n",
    "n_neighbors = min(n_samples, k + 1)\n",
    "\n",
    "if n_samples >= 2:\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "    nbrs.fit(X_clean.values)\n",
    "    _, idx = nbrs.kneighbors(X_clean.values)\n",
    "    src_knn, dst_knn = [], []\n",
    "    for i in range(idx.shape[0]):\n",
    "        for j in idx[i, 1:]:\n",
    "            src_knn.append(i)\n",
    "            dst_knn.append(j)\n",
    "    edge_knn = torch.tensor([src_knn, dst_knn], dtype=torch.long)\n",
    "else:\n",
    "    edge_knn = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "tmp = df.reset_index()[[\"index\", \"RID\", date_col]].dropna(subset=[date_col]).sort_values([\"RID\", date_col])\n",
    "src_tmp, dst_tmp = [], []\n",
    "for rid, g in tmp.groupby(\"RID\"):\n",
    "    ids = g[\"index\"].tolist()\n",
    "    for a, b in zip(ids[:-1], ids[1:]):\n",
    "        src_tmp.append(a)\n",
    "        dst_tmp.append(b)\n",
    "edge_tmp = torch.tensor([src_tmp, dst_tmp], dtype=torch.long) if src_tmp else torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "def undirected(e):\n",
    "    return torch.cat([e, e.flip(0)], dim=1) if e.numel() else e\n",
    "\n",
    "edges = []\n",
    "if edge_knn.numel(): edges.append(undirected(edge_knn))\n",
    "if edge_tmp.numel(): edges.append(undirected(edge_tmp))\n",
    "edge_index = torch.cat(edges, dim=1) if edges else torch.empty((2,0), dtype=torch.long)\n",
    "if edge_index.numel():\n",
    "    edge_index = torch.unique(edge_index, dim=1)\n",
    "\n",
    "print(f\"\\nGraph constructed:\")\n",
    "print(f\"  Nodes: {len(df)}\")\n",
    "print(f\"  kNN edges: {edge_knn.size(1)}\")\n",
    "print(f\"  Temporal edges: {edge_tmp.size(1)}\")\n",
    "print(f\"  Total edges (undirected, unique): {edge_index.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "splits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splits created:\n",
      "  RIDs with labels: 82\n",
      "  Train RIDs: 57\n",
      "  Val RIDs: 12\n",
      "  Test RIDs: 13\n",
      "\n",
      "  Labeled nodes:\n",
      "    Train: 57\n",
      "    Val: 12\n",
      "    Test: 13\n",
      "\n",
      "  Edges (intra-split): 73074\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df[\"USE_FOR_LABEL\"] = False\n",
    "if df[\"HAS_LABEL\"].any():\n",
    "    idx_last_pre = df.loc[df[\"HAS_LABEL\"]].groupby(\"RID\")[date_col].idxmax()\n",
    "    df.loc[idx_last_pre, \"USE_FOR_LABEL\"] = True\n",
    "\n",
    "rids_with_label = df.loc[df[\"USE_FOR_LABEL\"], \"RID\"].dropna().unique()\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(rids_with_label)\n",
    "n_lab_rids = len(rids_with_label)\n",
    "\n",
    "tr_n = max(1, int(0.7 * n_lab_rids))\n",
    "va_n = max(0, int(0.15 * n_lab_rids))\n",
    "if tr_n + va_n > max(0, n_lab_rids - 1):\n",
    "    va_n = max(0, n_lab_rids - 1 - tr_n)\n",
    "\n",
    "train_rids = set(rids_with_label[:tr_n])\n",
    "val_rids   = set(rids_with_label[tr_n:tr_n+va_n])\n",
    "test_rids  = set(rids_with_label[tr_n+va_n:])\n",
    "\n",
    "node_split = np.full(len(df), \"train\", dtype=object)\n",
    "node_rids = df[\"RID\"].to_numpy()\n",
    "node_split[np.isin(node_rids, list(val_rids))]  = \"val\"\n",
    "node_split[np.isin(node_rids, list(test_rids))] = \"test\"\n",
    "\n",
    "use_for_label = df[\"USE_FOR_LABEL\"].to_numpy()\n",
    "train_mask_np = (node_split == \"train\") & use_for_label\n",
    "val_mask_np   = (node_split == \"val\")   & use_for_label\n",
    "test_mask_np  = (node_split == \"test\")  & use_for_label\n",
    "\n",
    "split_map = {\"train\":0, \"val\":1, \"test\":2}\n",
    "split_idx = np.vectorize(split_map.get)(node_split)\n",
    "src_np = edge_index[0].cpu().numpy()\n",
    "dst_np = edge_index[1].cpu().numpy()\n",
    "keep_edges = split_idx[src_np] == split_idx[dst_np]\n",
    "edge_index = edge_index[:, torch.tensor(keep_edges)]\n",
    "\n",
    "print(f\"\\nSplits created:\")\n",
    "print(f\"  RIDs with labels: {n_lab_rids}\")\n",
    "print(f\"  Train RIDs: {len(train_rids)}\")\n",
    "print(f\"  Val RIDs: {len(val_rids)}\")\n",
    "print(f\"  Test RIDs: {len(test_rids)}\")\n",
    "print(f\"\\n  Labeled nodes:\")\n",
    "print(f\"    Train: {train_mask_np.sum()}\")\n",
    "print(f\"    Val: {val_mask_np.sum()}\")\n",
    "print(f\"    Test: {test_mask_np.sum()}\")\n",
    "print(f\"\\n  Edges (intra-split): {edge_index.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "prepare_tensors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data object created:\n",
      "  x.shape: torch.Size([6488, 28])\n",
      "  edge_index.shape: torch.Size([2, 73074])\n",
      "  y.shape: torch.Size([6488])\n",
      "  Target scaling: mean=0.018, std=0.131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_full = df[\"YEARS_TO_ONSET\"].astype(float)\n",
    "y_mu  = float(y_full[train_mask_np].mean()) if train_mask_np.any() else 0.0\n",
    "y_std = float(y_full[train_mask_np].std(ddof=0)) if train_mask_np.any() else 1.0\n",
    "if not np.isfinite(y_std) or y_std == 0.0:\n",
    "    y_std = 1.0\n",
    "\n",
    "y_scaled = (y_full - y_mu) / y_std\n",
    "y_t = torch.tensor(y_scaled.fillna(0).values, dtype=torch.float32)\n",
    "\n",
    "x = torch.tensor(X_clean.values, dtype=torch.float32)\n",
    "train_mask = torch.tensor(train_mask_np, dtype=torch.bool)\n",
    "val_mask   = torch.tensor(val_mask_np,   dtype=torch.bool)\n",
    "test_mask  = torch.tensor(test_mask_np,  dtype=torch.bool)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "print(f\"\\nData object created:\")\n",
    "print(f\"  x.shape: {data.x.shape}\")\n",
    "print(f\"  edge_index.shape: {data.edge_index.shape}\")\n",
    "print(f\"  y.shape: {data.y.shape}\")\n",
    "print(f\"  Target scaling: mean={y_mu:.3f}, std={y_std:.3f}\")\n",
    "\n",
    "Y_MEAN_TRAIN, Y_STD_TRAIN = y_mu, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Model created (BASELINE - Demographics only):\n",
      "  Input features: 28\n",
      "  Hidden dim: 64\n",
      "  Architecture: GCNConv -> ReLU -> Dropout -> GCNConv -> Output\n",
      "  Model parameters: 1921\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = GNNRegressor(in_ch=data.num_node_features, hid=64, dropout=0.3).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "print(f\"\\nðŸ§  Model created (BASELINE - Demographics only):\")\n",
    "print(f\"  Input features: {data.num_node_features}\")\n",
    "print(f\"  Hidden dim: 64\")\n",
    "print(f\"  Architecture: GCNConv -> ReLU -> Dropout -> GCNConv -> Output\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_mae': [],\n",
    "    'train_rmse': [],\n",
    "    'val_mae': [],\n",
    "    'val_rmse': [],\n",
    "    'test_mae': [],\n",
    "    'test_rmse': []\n",
    "}\n",
    "\n",
    "def eval_metrics(split=\"val\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        mask = data.train_mask if split==\"train\" else (data.val_mask if split==\"val\" else data.test_mask)\n",
    "        if not mask.any():\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "        mae_scaled = torch.mean(torch.abs(out[mask] - data.y[mask])).item()\n",
    "        rmse_scaled = torch.sqrt(loss_fn(out[mask], data.y[mask])).item()\n",
    "        return mae_scaled * Y_STD_TRAIN, rmse_scaled * Y_STD_TRAIN\n",
    "\n",
    "print(\"\\nðŸš€ Starting training...\\n\")\n",
    "\n",
    "for ep in range(1, 101):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    \n",
    "    if data.train_mask.any():\n",
    "        loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            raise RuntimeError(\"Loss NaN/Inf\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_val = loss.detach().item()\n",
    "    else:\n",
    "        loss_val = float(\"nan\")\n",
    "    \n",
    "    tr_mae, tr_rmse = eval_metrics(\"train\")\n",
    "    va_mae, va_rmse = eval_metrics(\"val\")\n",
    "    te_mae, te_rmse = eval_metrics(\"test\")\n",
    "    \n",
    "    history['epoch'].append(ep)\n",
    "    history['train_loss'].append(loss_val)\n",
    "    history['train_mae'].append(tr_mae)\n",
    "    history['train_rmse'].append(tr_rmse)\n",
    "    history['val_mae'].append(va_mae)\n",
    "    history['val_rmse'].append(va_rmse)\n",
    "    history['test_mae'].append(te_mae)\n",
    "    history['test_rmse'].append(te_rmse)\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "        print(\n",
    "            f\"ep {ep:03d} | train_loss(MSE) {loss_val:.4f} \"\n",
    "            f\"| TR MAE {tr_mae:.3f}y RMSE {tr_rmse:.3f}y \"\n",
    "            f\"| VAL MAE {va_mae:.3f}y RMSE {va_rmse:.3f}y \"\n",
    "            f\"| TEST MAE {te_mae:.3f}y RMSE {te_rmse:.3f}y\"\n",
    "        )\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")\n",
    "\n",
    "import json\n",
    "with open('training_history_all_biomarkers.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(\"âœ… Training history saved to: training_history_all_biomarkers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "final_eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“Š ALL BIOMARKERS RESULTS (LEFT JOIN + Missing Indicators)\n",
      "======================================================================\n",
      "Dataset: 6488 visits (ALL demographics)\n",
      "Features: 28 (demographics + CSF + PET + MRI + indicators)\n",
      "\n",
      "Biomarker availability:\n",
      "  CSF: 1872 visits (28.9%)\n",
      "  PET: 818 visits (12.6%)\n",
      "  MRI: 3303 visits (50.9%)\n",
      "\n",
      "Performance:\n",
      "  TRAIN | MAE: 0.037 years | RMSE: 0.071 years\n",
      "  VAL   | MAE: 0.086 years | RMSE: 0.116 years\n",
      "  TEST  | MAE: 0.064 years | RMSE: 0.088 years\n",
      "======================================================================\n",
      "\n",
      "ðŸ’¡ This model uses ALL visits and learns to handle missing biomarkers!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_out = model(data.x, data.edge_index)\n",
    "\n",
    "tr_mae, tr_rmse = eval_metrics(\"train\")\n",
    "va_mae, va_rmse = eval_metrics(\"val\")\n",
    "te_mae, te_rmse = eval_metrics(\"test\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š ALL BIOMARKERS RESULTS (LEFT JOIN + Missing Indicators)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Dataset: {len(df)} visits (ALL demographics)\")\n",
    "print(f\"Features: {data.num_node_features} (demographics + CSF + PET + MRI + indicators)\")\n",
    "print(f\"\\nBiomarker availability:\")\n",
    "print(f\"  CSF: {df['HAS_CSF'].sum():.0f} visits ({100*df['HAS_CSF'].mean():.1f}%)\")\n",
    "print(f\"  PET: {df['HAS_PET'].sum():.0f} visits ({100*df['HAS_PET'].mean():.1f}%)\")\n",
    "print(f\"  MRI: {df['HAS_MRI'].sum():.0f} visits ({100*df['HAS_MRI'].mean():.1f}%)\")\n",
    "print(\"\\nPerformance:\")\n",
    "print(f\"  TRAIN | MAE: {tr_mae:.3f} years | RMSE: {tr_rmse:.3f} years\")\n",
    "print(f\"  VAL   | MAE: {va_mae:.3f} years | RMSE: {va_rmse:.3f} years\")\n",
    "print(f\"  TEST  | MAE: {te_mae:.3f} years | RMSE: {te_rmse:.3f} years\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_biomarkers_results = {\n",
    "    'model': 'All Biomarkers (Demographics + CSF + PET + MRI - LEFT JOIN)',\n",
    "    'features': data.num_node_features,\n",
    "    'train_mae': tr_mae,\n",
    "    'train_rmse': tr_rmse,\n",
    "    'val_mae': va_mae,\n",
    "    'val_rmse': va_rmse,\n",
    "    'test_mae': te_mae,\n",
    "    'test_rmse': te_rmse,\n",
    "    'n_visits': len(df),\n",
    "    'csf_coverage': float(df['HAS_CSF'].mean()),\n",
    "    'pet_coverage': float(df['HAS_PET'].mean()),\n",
    "    'mri_coverage': float(df['HAS_MRI'].mean())\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ’¡ This model uses ALL visits and learns to handle missing biomarkers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "save_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Results saved to: all_biomarkers_results.json\n",
      "\n",
      "ðŸŽ¯ Compare all 4 models using CompareModels.ipynb to see the improvement!\n",
      "\n",
      "ðŸ“ˆ Expected: This model should perform BEST because:\n",
      "   - Uses ALL 6488 visits (maximum training data)\n",
      "   - Leverages biomarkers when available (~29% CSF, ~13% PET, ~53% MRI)\n",
      "   - GNN learns to predict even without complete biomarkers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "with open('all_biomarkers_results.json', 'w') as f:\n",
    "    json.dump(all_biomarkers_results, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Results saved to: all_biomarkers_results.json\")\n",
    "print(\"\\nðŸŽ¯ Compare all 4 models using CompareModels.ipynb to see the improvement!\")\n",
    "print(f\"\\nðŸ“ˆ Expected: This model should perform BEST because:\")\n",
    "print(f\"   - Uses ALL {len(df)} visits (maximum training data)\")\n",
    "print(f\"   - Leverages biomarkers when available (~29% CSF, ~13% PET, ~53% MRI)\")\n",
    "print(f\"   - GNN learns to predict even without complete biomarkers\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}