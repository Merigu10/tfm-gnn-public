{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# K-Fold Cross-Validation Estratificado por Paciente\n",
    "\n",
    "## Objetivo\n",
    "Este notebook implementa **K-Fold Cross-Validation estratificada por paciente** para datos mÃ©dicos longitudinales.\n",
    "\n",
    "## Ventajas sobre LOPO-CV\n",
    "- âœ… **Test mÃ¡s robusto:** 8 pacientes por fold (vs 1 en LOPO)\n",
    "- âœ… **MÃ¡s rÃ¡pido:** 10 folds (vs 82 en LOPO)\n",
    "- âœ… **Mejor balance train/test:** 74/8 pacientes (vs 81/1)\n",
    "- âœ… **Intervalos de confianza:** IC 95% sobre 10 folds\n",
    "- âœ… **Sin data leakage:** Todas las visitas de un paciente en el mismo split\n",
    "\n",
    "## ConfiguraciÃ³n\n",
    "- **Dataset:** 6,488 visitas de ~5,000 pacientes\n",
    "- **Pacientes etiquetados:** 82 pacientes con YEARS_TO_ONSET\n",
    "- **K-Folds:** 10 folds estratificados\n",
    "- **Test por fold:** ~8 pacientes (~64 visitas)\n",
    "- **Train por fold:** ~74 pacientes (~6,424 visitas)\n",
    "\n",
    "## Tiempo estimado\n",
    "- **10 folds Ã— 100 Ã©pocas Ã— ~20 seg/fold = ~3-5 minutos** (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\merit\\Documents\\tfm-gnn\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "utilities",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined.\n"
     ]
    }
   ],
   "source": [
    "def norm_codes_to_labels(s: pd.Series, mapping: dict) -> pd.Series:\n",
    "    out = s.astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    out = out.map(mapping)\n",
    "    return out\n",
    "\n",
    "gender_map = {\"1\":\"male\",\"2\":\"female\",\"male\":\"male\",\"female\":\"female\",\"m\":\"male\",\"f\":\"female\"}\n",
    "marry_map  = {\"1\":\"married\",\"2\":\"widowed\",\"3\":\"divorced\",\"4\":\"never_married\",\"6\":\"domestic_partnership\"}\n",
    "\n",
    "def to_year(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s.where((s >= 1900) & (s <= 2100))\n",
    "    return s\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ADNI data...\n",
      "\n",
      "âœ… Demographics loaded: (6210, 84)\n",
      "Patients with YEAR_ONSET: 2908\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading ADNI data...\\n\")\n",
    "\n",
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"âœ… Demographics loaded: {df.shape}\")\n",
    "\n",
    "onset_cols = [c for c in [\"PTCOGBEG\",\"PTADBEG\",\"PTADDX\"] if c in df.columns]\n",
    "for c in onset_cols:\n",
    "    df[c] = to_year(df[c])\n",
    "\n",
    "def row_min_nonnull(row):\n",
    "    vals = [row[c] for c in onset_cols if pd.notna(row[c])]\n",
    "    return min(vals) if vals else np.nan\n",
    "\n",
    "df[\"YEAR_ONSET\"] = df.apply(row_min_nonnull, axis=1) if onset_cols else np.nan\n",
    "df[\"YEAR_ONSET\"] = to_year(df[\"YEAR_ONSET\"])\n",
    "\n",
    "for c in [\"PTDOBYY\",\"PTEDUCAT\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "if \"PTGENDER\" in df.columns:\n",
    "    df[\"PTGENDER\"] = norm_codes_to_labels(df[\"PTGENDER\"], gender_map)\n",
    "if \"PTMARRY\" in df.columns:\n",
    "    df[\"PTMARRY\"]  = norm_codes_to_labels(df[\"PTMARRY\"], marry_map)\n",
    "\n",
    "print(f\"Patients with YEAR_ONSET: {df['YEAR_ONSET'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "merge_biomarkers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging biomarkers (LEFT JOIN strategy)...\n",
      "\n",
      "âœ… CSF: 1780/6210 (28.7%)\n",
      "âœ… PET: 810/6212 (13.0%)\n",
      "âœ… MRI: 3303/6488 (50.9%)\n",
      "\n",
      "ðŸ“Š Dataset final: 6488 visitas\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMerging biomarkers (LEFT JOIN strategy)...\\n\")\n",
    "\n",
    "df['VISCODE_NORMALIZED'] = df['VISCODE2'].astype(str).str.strip().replace({'sc': 'bl', 'f': 'bl', 'nan': ''})\n",
    "\n",
    "biomarker_path = \"./data/adni/demographics/UPENNBIOMK_ROCHE_ELECSYS_11Oct2025.csv\"\n",
    "df_csf = pd.read_csv(biomarker_path)\n",
    "df_csf['VISCODE_NORMALIZED'] = df_csf['VISCODE2'].astype(str).str.strip()\n",
    "df_csf = df_csf.dropna(subset=['ABETA42', 'TAU', 'PTAU'])\n",
    "df_csf['TAU_ABETA42_RATIO'] = df_csf['TAU'] / (df_csf['ABETA42'] + 1e-6)\n",
    "df_csf['PTAU_ABETA42_RATIO'] = df_csf['PTAU'] / (df_csf['ABETA42'] + 1e-6)\n",
    "df_csf['PTAU_TAU_RATIO'] = df_csf['PTAU'] / (df_csf['TAU'] + 1e-6)\n",
    "\n",
    "df = df.merge(\n",
    "    df_csf[['RID', 'VISCODE_NORMALIZED', 'ABETA42', 'TAU', 'PTAU', \n",
    "            'TAU_ABETA42_RATIO', 'PTAU_ABETA42_RATIO', 'PTAU_TAU_RATIO']],\n",
    "    on=['RID', 'VISCODE_NORMALIZED'], how='left'\n",
    ")\n",
    "df['HAS_CSF'] = df['ABETA42'].notna().astype(float)\n",
    "print(f\"âœ… CSF: {df['HAS_CSF'].sum():.0f}/{len(df)} ({100*df['HAS_CSF'].mean():.1f}%)\")\n",
    "\n",
    "pet_path = \"./data/adni/demographics/All_Subjects_UCBERKELEY_AMY_6MM_11Oct2025.csv\"\n",
    "df_pet = pd.read_csv(pet_path, low_memory=False)\n",
    "df_pet['VISCODE_NORMALIZED'] = df_pet['VISCODE'].astype(str).str.strip().replace({'sc': 'bl', 'f': 'bl', 'nan': ''})\n",
    "df_pet = df_pet[['RID', 'VISCODE_NORMALIZED', 'CENTILOIDS', 'SUMMARY_SUVR', 'COMPOSITE_REF_SUVR']].copy()\n",
    "df_pet.columns = ['RID', 'VISCODE_NORMALIZED', 'PET_CENTILOIDS', 'PET_SUVR', 'PET_COMPOSITE']\n",
    "df_pet = df_pet.dropna(subset=['PET_CENTILOIDS', 'PET_SUVR'])\n",
    "\n",
    "df = df.merge(df_pet, on=['RID', 'VISCODE_NORMALIZED'], how='left')\n",
    "df['HAS_PET'] = df['PET_CENTILOIDS'].notna().astype(float)\n",
    "print(f\"âœ… PET: {df['HAS_PET'].sum():.0f}/{len(df)} ({100*df['HAS_PET'].mean():.1f}%)\")\n",
    "\n",
    "mri_path = \"./data/adni/demographics/All_Subjects_UCSFFSX7_11Oct2025.csv\"\n",
    "df_mri = pd.read_csv(mri_path, low_memory=False)\n",
    "df_mri['VISCODE_NORMALIZED'] = df_mri['VISCODE2'].astype(str).str.strip().replace({'sc': 'bl', 'f': 'bl', 'nan': ''})\n",
    "df_mri = df_mri[['RID', 'VISCODE_NORMALIZED', 'ST101SV', 'ST11SV', 'ST12SV', \n",
    "                 'ST4SV', 'ST5SV', 'ST17SV', 'ST18SV']].copy()\n",
    "df_mri.columns = ['RID', 'VISCODE_NORMALIZED', 'MRI_eTIV', 'MRI_Vol1', 'MRI_Vol2', \n",
    "                  'MRI_Vol3', 'MRI_Vol4', 'MRI_Vol5', 'MRI_Vol6']\n",
    "df_mri = df_mri.dropna(subset=['MRI_eTIV', 'MRI_Vol1'])\n",
    "\n",
    "df = df.merge(df_mri, on=['RID', 'VISCODE_NORMALIZED'], how='left')\n",
    "df['HAS_MRI'] = df['MRI_eTIV'].notna().astype(float)\n",
    "df = df.drop(columns=['VISCODE_NORMALIZED'])\n",
    "\n",
    "print(f\"âœ… MRI: {df['HAS_MRI'].sum():.0f}/{len(df)} ({100*df['HAS_MRI'].mean():.1f}%)\")\n",
    "print(f\"\\nðŸ“Š Dataset final: {len(df)} visitas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prepare_target",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Dataset preparado:\n",
      "  Total visitas: 6488\n",
      "  Pacientes Ãºnicos: 4945\n",
      "  Pacientes con etiqueta: 82\n",
      "  Pacientes sin etiqueta: 4863\n"
     ]
    }
   ],
   "source": [
    "date_col = \"EXAMDATE\" if \"EXAMDATE\" in df.columns else \"VISDATE\"\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "df[\"EXAM_YEAR\"] = to_year(df[date_col].dt.year)\n",
    "df[\"AGE_AT_VISIT\"] = np.where(\n",
    "    df[\"EXAM_YEAR\"].notna() & df[\"PTDOBYY\"].notna(),\n",
    "    df[\"EXAM_YEAR\"] - df[\"PTDOBYY\"], np.nan\n",
    ")\n",
    "\n",
    "df[\"YEARS_TO_ONSET\"] = np.where(\n",
    "    df[\"YEAR_ONSET\"].notna() & df[\"EXAM_YEAR\"].notna(),\n",
    "    df[\"YEAR_ONSET\"] - df[\"EXAM_YEAR\"], np.nan\n",
    ")\n",
    "\n",
    "df.loc[(df[\"YEARS_TO_ONSET\"] < 0) & df[\"YEAR_ONSET\"].notna(), \"YEARS_TO_ONSET\"] = np.nan\n",
    "df.loc[df[\"YEARS_TO_ONSET\"] > 50, \"YEARS_TO_ONSET\"] = np.nan\n",
    "df[\"HAS_LABEL\"] = df[\"YEARS_TO_ONSET\"].notna()\n",
    "\n",
    "df[\"USE_FOR_LABEL\"] = False\n",
    "if df[\"HAS_LABEL\"].any():\n",
    "    idx_last_pre = df.loc[df[\"HAS_LABEL\"]].groupby(\"RID\")[date_col].idxmax()\n",
    "    df.loc[idx_last_pre, \"USE_FOR_LABEL\"] = True\n",
    "\n",
    "rids_with_label = df.loc[df[\"USE_FOR_LABEL\"], \"RID\"].dropna().unique()\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset preparado:\")\n",
    "print(f\"  Total visitas: {len(df)}\")\n",
    "print(f\"  Pacientes Ãºnicos: {df['RID'].nunique()}\")\n",
    "print(f\"  Pacientes con etiqueta: {len(rids_with_label)}\")\n",
    "print(f\"  Pacientes sin etiqueta: {df['RID'].nunique() - len(rids_with_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "define_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model defined\n"
     ]
    }
   ],
   "source": [
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "print(\"âœ… Model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "kfold_function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… K-Fold function defined\n"
     ]
    }
   ],
   "source": [
    "def train_kfold(df_fold, test_rids, n_epochs=100):\n",
    "    \"\"\"\n",
    "    Entrena un fold con mÃºltiples pacientes de test.\n",
    "    \n",
    "    Args:\n",
    "        df_fold: DataFrame completo\n",
    "        test_rids: Lista de RIDs para test\n",
    "        n_epochs: Ã‰pocas de entrenamiento\n",
    "    \"\"\"\n",
    "    csf_cols = ['ABETA42', 'TAU', 'PTAU', 'TAU_ABETA42_RATIO', 'PTAU_ABETA42_RATIO', 'PTAU_TAU_RATIO']\n",
    "    pet_cols = ['PET_CENTILOIDS', 'PET_SUVR', 'PET_COMPOSITE']\n",
    "    mri_cols = ['MRI_eTIV', 'MRI_Vol1', 'MRI_Vol2', 'MRI_Vol3', 'MRI_Vol4', 'MRI_Vol5', 'MRI_Vol6']\n",
    "    missing_indicators = ['HAS_CSF', 'HAS_PET', 'HAS_MRI']\n",
    "    \n",
    "    num_cols = [c for c in [\"AGE_AT_VISIT\", \"PTEDUCAT\"] + csf_cols + pet_cols + mri_cols + missing_indicators if c in df_fold.columns]\n",
    "    cat_cols = [c for c in [\"PTGENDER\", \"PTMARRY\"] if c in df_fold.columns]\n",
    "    \n",
    "    for c in csf_cols + pet_cols + mri_cols:\n",
    "        if c in df_fold.columns:\n",
    "            df_fold[c] = pd.to_numeric(df_fold[c], errors=\"coerce\").fillna(0.0)\n",
    "    \n",
    "    for c in [\"AGE_AT_VISIT\", \"PTEDUCAT\"]:\n",
    "        if c in df_fold.columns:\n",
    "            df_fold[c] = pd.to_numeric(df_fold[c], errors=\"coerce\").fillna(df_fold[c].median())\n",
    "    \n",
    "    parts = []\n",
    "    if num_cols:\n",
    "        scaler = StandardScaler()\n",
    "        X_num = pd.DataFrame(\n",
    "            scaler.fit_transform(df_fold[num_cols]),\n",
    "            columns=num_cols, index=df_fold.index\n",
    "        )\n",
    "        parts.append(X_num)\n",
    "    \n",
    "    if cat_cols:\n",
    "        X_cat = pd.get_dummies(df_fold[cat_cols], prefix=cat_cols, drop_first=False, dtype=float)\n",
    "        parts.append(X_cat)\n",
    "    \n",
    "    X = pd.concat(parts, axis=1).astype(float)\n",
    "    X_clean = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    \n",
    "    n_samples = X_clean.shape[0]\n",
    "    k = min(8, max(1, n_samples - 1))\n",
    "    n_neighbors = min(n_samples, k + 1)\n",
    "    \n",
    "    if n_samples >= 2:\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "        nbrs.fit(X_clean.values)\n",
    "        _, idx = nbrs.kneighbors(X_clean.values)\n",
    "        src_knn, dst_knn = [], []\n",
    "        for i in range(idx.shape[0]):\n",
    "            for j in idx[i, 1:]:\n",
    "                src_knn.append(i)\n",
    "                dst_knn.append(j)\n",
    "        edge_knn = torch.tensor([src_knn, dst_knn], dtype=torch.long)\n",
    "    else:\n",
    "        edge_knn = torch.empty((2, 0), dtype=torch.long)\n",
    "    \n",
    "    tmp = df_fold.reset_index()[[\"index\", \"RID\", date_col]].dropna(subset=[date_col]).sort_values([\"RID\", date_col])\n",
    "    src_tmp, dst_tmp = [], []\n",
    "    for rid, g in tmp.groupby(\"RID\"):\n",
    "        ids = g[\"index\"].tolist()\n",
    "        for a, b in zip(ids[:-1], ids[1:]):\n",
    "            src_tmp.append(a)\n",
    "            dst_tmp.append(b)\n",
    "    edge_tmp = torch.tensor([src_tmp, dst_tmp], dtype=torch.long) if src_tmp else torch.empty((2,0), dtype=torch.long)\n",
    "    \n",
    "    def undirected(e):\n",
    "        return torch.cat([e, e.flip(0)], dim=1) if e.numel() else e\n",
    "    \n",
    "    edges = []\n",
    "    if edge_knn.numel(): edges.append(undirected(edge_knn))\n",
    "    if edge_tmp.numel(): edges.append(undirected(edge_tmp))\n",
    "    edge_index = torch.cat(edges, dim=1) if edges else torch.empty((2,0), dtype=torch.long)\n",
    "    if edge_index.numel():\n",
    "        edge_index = torch.unique(edge_index, dim=1)\n",
    "    \n",
    "    node_rids = df_fold[\"RID\"].to_numpy()\n",
    "    use_for_label = df_fold[\"USE_FOR_LABEL\"].to_numpy()\n",
    "    \n",
    "    train_mask_np = (~np.isin(node_rids, test_rids)) & use_for_label\n",
    "    test_mask_np = np.isin(node_rids, test_rids) & use_for_label\n",
    "    \n",
    "    node_split = np.where(np.isin(node_rids, test_rids), \"test\", \"train\")\n",
    "    split_map = {\"train\":0, \"test\":1}\n",
    "    split_idx = np.vectorize(split_map.get)(node_split)\n",
    "    src_np = edge_index[0].cpu().numpy()\n",
    "    dst_np = edge_index[1].cpu().numpy()\n",
    "    keep_edges = split_idx[src_np] == split_idx[dst_np]\n",
    "    edge_index = edge_index[:, torch.tensor(keep_edges)]\n",
    "    \n",
    "    y_full = df_fold[\"YEARS_TO_ONSET\"].astype(float)\n",
    "    y_mu  = float(y_full[train_mask_np].mean()) if train_mask_np.any() else 0.0\n",
    "    y_std = float(y_full[train_mask_np].std(ddof=0)) if train_mask_np.any() else 1.0\n",
    "    if not np.isfinite(y_std) or y_std == 0.0:\n",
    "        y_std = 1.0\n",
    "    \n",
    "    y_scaled = (y_full - y_mu) / y_std\n",
    "    y_t = torch.tensor(y_scaled.fillna(0).values, dtype=torch.float32)\n",
    "    \n",
    "    x = torch.tensor(X_clean.values, dtype=torch.float32)\n",
    "    train_mask = torch.tensor(train_mask_np, dtype=torch.bool)\n",
    "    test_mask  = torch.tensor(test_mask_np,  dtype=torch.bool)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "                train_mask=train_mask, test_mask=test_mask)\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    model = GNNRegressor(in_ch=data.num_node_features, hid=64, dropout=0.3).to(device)\n",
    "    data = data.to(device)\n",
    "    \n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    for ep in range(n_epochs):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        \n",
    "        if data.train_mask.any():\n",
    "            loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                break\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_out = model(data.x, data.edge_index)\n",
    "        \n",
    "        if not test_mask.any():\n",
    "            return {\"test_mae\": np.nan, \"test_rmse\": np.nan, \"n_test\": 0}\n",
    "        \n",
    "        mae_scaled = torch.mean(torch.abs(final_out[test_mask] - data.y[test_mask])).item()\n",
    "        rmse_scaled = torch.sqrt(loss_fn(final_out[test_mask], data.y[test_mask])).item()\n",
    "        \n",
    "        test_mae = mae_scaled * y_std\n",
    "        test_rmse = rmse_scaled * y_std\n",
    "    \n",
    "    return {\n",
    "        \"test_mae\": float(test_mae),\n",
    "        \"test_rmse\": float(test_rmse),\n",
    "        \"n_train\": int(train_mask_np.sum()),\n",
    "        \"n_test\": int(test_mask_np.sum()),\n",
    "        \"test_rids\": list(map(int, test_rids))\n",
    "    }\n",
    "\n",
    "print(\"âœ… K-Fold function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "run_kfold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸš€ STARTING 10-FOLD CROSS-VALIDATION\n",
      "======================================================================\n",
      "Total pacientes etiquetados: 82\n",
      "Pacientes por fold (test): ~8\n",
      "Pacientes por fold (train): ~74\n",
      "Total folds: 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  10%|â–ˆ         | 1/10 [00:02<00:22,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1/10: MAE=0.070 years | Test patients=9 | Train=73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  20%|â–ˆâ–ˆ        | 2/10 [00:05<00:21,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 2/10: MAE=0.064 years | Test patients=9 | Train=73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:07<00:18,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 3/10: MAE=0.170 years | Test patients=8 | Train=74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:14,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 4/10: MAE=0.077 years | Test patients=8 | Train=74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:12<00:11,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 5/10: MAE=0.119 years | Test patients=8 | Train=74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:14<00:09,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 6/10: MAE=0.052 years | Test patients=8 | Train=74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:17<00:07,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 7/10: MAE=0.039 years | Test patients=8 | Train=74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:19<00:04,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 8/10: MAE=0.117 years | Test patients=8 | Train=74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:21<00:02,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 9/10: MAE=0.087 years | Test patients=8 | Train=74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 10/10: MAE=0.037 years | Test patients=8 | Train=74\n",
      "\n",
      "âœ… K-Fold Cross-Validation completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "K_FOLDS = 10\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ðŸš€ STARTING {K_FOLDS}-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total pacientes etiquetados: {len(rids_with_label)}\")\n",
    "print(f\"Pacientes por fold (test): ~{len(rids_with_label)//K_FOLDS}\")\n",
    "print(f\"Pacientes por fold (train): ~{len(rids_with_label) - len(rids_with_label)//K_FOLDS}\")\n",
    "print(f\"Total folds: {K_FOLDS}\\n\")\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "rids_shuffled = rids_with_label.copy()\n",
    "rng.shuffle(rids_shuffled)\n",
    "\n",
    "kf = KFold(n_splits=K_FOLDS, shuffle=False)  # Ya hicimos shuffle manual\n",
    "results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(tqdm(kf.split(rids_shuffled), total=K_FOLDS, desc=\"K-Fold Progress\")):\n",
    "    test_rids = rids_shuffled[test_idx]\n",
    "    \n",
    "    fold_result = train_kfold(df.copy(), test_rids, n_epochs=100)\n",
    "    fold_result['fold'] = fold_idx + 1\n",
    "    results.append(fold_result)\n",
    "    \n",
    "    print(f\"  Fold {fold_idx+1}/{K_FOLDS}: MAE={fold_result['test_mae']:.3f} years | \"\n",
    "          f\"Test patients={fold_result['n_test']} | Train={fold_result['n_train']}\")\n",
    "\n",
    "print(\"\\nâœ… K-Fold Cross-Validation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compute_metrics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“Š 10-FOLD CROSS-VALIDATION RESULTS\n",
      "======================================================================\n",
      "Valid folds: 10/10\n",
      "\n",
      "MAE (Mean Absolute Error):\n",
      "  Mean Â± 95% CI: 0.083 Â± 0.030 years\n",
      "  Median: 0.073 years\n",
      "  Std Dev: 0.042 years\n",
      "  Range: [0.037, 0.170] years\n",
      "\n",
      "RMSE (Root Mean Squared Error):\n",
      "  Mean Â± 95% CI: 0.132 Â± 0.065 years\n",
      "  Median: 0.100 years\n",
      "  Std Dev: 0.090 years\n",
      "======================================================================\n",
      "\n",
      "ðŸ“… En dÃ­as: MAE = 30 Â± 11 dÃ­as (95% CI)\n",
      "\n",
      "âœ… Resultados guardados:\n",
      "  - kfold_cv_summary.json\n",
      "  - kfold_cv_detailed_results.csv\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results_clean = df_results.dropna(subset=['test_mae', 'test_rmse'])\n",
    "\n",
    "mae_mean = df_results_clean['test_mae'].mean()\n",
    "mae_std = df_results_clean['test_mae'].std()\n",
    "mae_median = df_results_clean['test_mae'].median()\n",
    "mae_min = df_results_clean['test_mae'].min()\n",
    "mae_max = df_results_clean['test_mae'].max()\n",
    "\n",
    "rmse_mean = df_results_clean['test_rmse'].mean()\n",
    "rmse_std = df_results_clean['test_rmse'].std()\n",
    "rmse_median = df_results_clean['test_rmse'].median()\n",
    "\n",
    "from scipy import stats\n",
    "n_folds = len(df_results_clean)\n",
    "confidence = 0.95\n",
    "t_crit = stats.t.ppf((1 + confidence) / 2, n_folds - 1)\n",
    "mae_ci = t_crit * mae_std / np.sqrt(n_folds)\n",
    "rmse_ci = t_crit * rmse_std / np.sqrt(n_folds)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ðŸ“Š {K_FOLDS}-FOLD CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Valid folds: {n_folds}/{K_FOLDS}\")\n",
    "print(f\"\\nMAE (Mean Absolute Error):\")\n",
    "print(f\"  Mean Â± 95% CI: {mae_mean:.3f} Â± {mae_ci:.3f} years\")\n",
    "print(f\"  Median: {mae_median:.3f} years\")\n",
    "print(f\"  Std Dev: {mae_std:.3f} years\")\n",
    "print(f\"  Range: [{mae_min:.3f}, {mae_max:.3f}] years\")\n",
    "print(f\"\\nRMSE (Root Mean Squared Error):\")\n",
    "print(f\"  Mean Â± 95% CI: {rmse_mean:.3f} Â± {rmse_ci:.3f} years\")\n",
    "print(f\"  Median: {rmse_median:.3f} years\")\n",
    "print(f\"  Std Dev: {rmse_std:.3f} years\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "mae_days = mae_mean * 365\n",
    "mae_ci_days = mae_ci * 365\n",
    "print(f\"\\nðŸ“… En dÃ­as: MAE = {mae_days:.0f} Â± {mae_ci_days:.0f} dÃ­as (95% CI)\")\n",
    "\n",
    "kfold_summary = {\n",
    "    'model': f'All Biomarkers - {K_FOLDS}-Fold CV',\n",
    "    'n_folds': int(n_folds),\n",
    "    'n_patients': int(len(rids_with_label)),\n",
    "    'mae_mean': float(mae_mean),\n",
    "    'mae_ci': float(mae_ci),\n",
    "    'mae_median': float(mae_median),\n",
    "    'mae_std': float(mae_std),\n",
    "    'mae_min': float(mae_min),\n",
    "    'mae_max': float(mae_max),\n",
    "    'rmse_mean': float(rmse_mean),\n",
    "    'rmse_ci': float(rmse_ci),\n",
    "    'rmse_median': float(rmse_median),\n",
    "    'rmse_std': float(rmse_std)\n",
    "}\n",
    "\n",
    "with open('kfold_cv_summary.json', 'w') as f:\n",
    "    json.dump(kfold_summary, f, indent=2)\n",
    "\n",
    "df_results.to_csv('kfold_cv_detailed_results.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Resultados guardados:\")\n",
    "print(\"  - kfold_cv_summary.json\")\n",
    "print(\"  - kfold_cv_detailed_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "### Ventajas de 10-Fold CV sobre LOPO-CV\n",
    "\n",
    "1. **Test mÃ¡s robusto por fold:** 8 pacientes vs 1 paciente\n",
    "2. **MÃ¡s rÃ¡pido:** 10 entrenamientos vs 82 entrenamientos\n",
    "3. **Mejor balance:** Train/Test ratio mÃ¡s realista\n",
    "4. **Intervalos de confianza vÃ¡lidos:** IC 95% con 10 folds\n",
    "5. **EvaluaciÃ³n completa:** Todos los 82 pacientes son test exactamente 1 vez\n",
    "\n",
    "### ComparaciÃ³n\n",
    "\n",
    "| MÃ©todo | Test/Fold | Tiempo | Robustez |\n",
    "|--------|-----------|--------|----------|\n",
    "| Split Simple | 13 pac | 2 min | Baja |\n",
    "| **10-Fold CV** | **8 pac** | **5 min** | **Alta** |\n",
    "| LOPO-CV | 1 pac | 2h | Media |\n",
    "\n",
    "### Referencias\n",
    "\n",
    "- Kohavi (1995). \"A study of cross-validation and bootstrap for accuracy estimation and model selection.\"\n",
    "- Varoquaux et al. (2017). \"Assessing and tuning brain decoders: Cross-validation, caveats, and guidelines.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}