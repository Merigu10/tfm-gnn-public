{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# GNN con TODOS los Biomarcadores + Early Stopping + RegularizaciÃ³n\n",
    "Este notebook usa **TODOS los pacientes (6,210 visitas)** con regularizaciÃ³n mejorada:\n",
    "\n",
    "**Mejoras de regularizaciÃ³n:**\n",
    "- â¸ï¸ **Early Stopping**: Parada automÃ¡tica entre Ã©pocas 20-25 si no mejora validaciÃ³n\n",
    "- ðŸŽ² **Dropout aumentado**: 0.5 para evitar overfitting\n",
    "- âš–ï¸ **L2 Regularization**: Weight decay = 1e-3 (10x mÃ¡s fuerte)\n",
    "- ðŸ“Š **Monitoreo**: Tracking de overfitting y convergencia\n",
    "\n",
    "**Estrategia LEFT JOIN:**\n",
    "- Empezamos con TODAS las visitas demogrÃ¡ficas\n",
    "- AÃ±adimos CSF, PET, MRI cuando estÃ¡n disponibles\n",
    "- Cuando faltan: usamos 0 + aÃ±adimos indicadores binarios (HAS_CSF, HAS_PET, HAS_MRI)\n",
    "- El GNN aprende a usar biomarcadores cuando estÃ¡n disponibles\n",
    "\n",
    "**Features (~30):**\n",
    "- Demographics: AGE, EDUCATION, GENDER, MARITAL_STATUS\n",
    "- CSF: ABETA42, TAU, PTAU + ratios (cuando disponible)\n",
    "- PET: CENTILOIDS, SUMMARY_SUVR (cuando disponible)\n",
    "- MRI: 7 volÃºmenes cerebrales (cuando disponible)\n",
    "- Missing indicators: HAS_CSF, HAS_PET, HAS_MRI\n",
    "\n",
    "**Ventaja**: MÃ¡ximo dataset + regularizaciÃ³n Ã³ptima = Mejor generalizaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Demographics loaded: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_codes_to_labels(s: pd.Series, mapping: dict) -> pd.Series:\n",
    "    out = s.astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    out = out.map(mapping)\n",
    "    return out\n",
    "\n",
    "gender_map = {\"1\":\"male\",\"2\":\"female\",\"male\":\"male\",\"female\":\"female\",\"m\":\"male\",\"f\":\"female\"}\n",
    "marry_map  = {\"1\":\"married\",\"2\":\"widowed\",\"3\":\"divorced\",\"4\":\"never_married\",\"6\":\"domestic_partnership\"}\n",
    "\n",
    "def to_year(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s.where((s >= 1900) & (s <= 2100))\n",
    "    return s\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onset_cols = [c for c in [\"PTCOGBEG\",\"PTADBEG\",\"PTADDX\"] if c in df.columns]\n",
    "for c in onset_cols:\n",
    "    df[c] = to_year(df[c])\n",
    "\n",
    "def row_min_nonnull(row):\n",
    "    vals = [row[c] for c in onset_cols if pd.notna(row[c])]\n",
    "    return min(vals) if vals else np.nan\n",
    "\n",
    "df[\"YEAR_ONSET\"] = df.apply(row_min_nonnull, axis=1) if onset_cols else np.nan\n",
    "df[\"YEAR_ONSET\"] = to_year(df[\"YEAR_ONSET\"])\n",
    "\n",
    "for c in [\"PTDOBYY\",\"PTEDUCAT\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "if \"PTGENDER\" in df.columns:\n",
    "    df[\"PTGENDER\"] = norm_codes_to_labels(df[\"PTGENDER\"], gender_map)\n",
    "if \"PTMARRY\" in df.columns:\n",
    "    df[\"PTMARRY\"]  = norm_codes_to_labels(df[\"PTMARRY\"], marry_map)\n",
    "\n",
    "print(f\"\\nData processed. Shape: {df.shape}\")\n",
    "print(f\"Patients with YEAR_ONSET: {df['YEAR_ONSET'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge_csf",
   "metadata": {},
   "outputs": [],
   "source": [
    "biomarker_path = \"./data/adni/demographics/UPENNBIOMK_ROCHE_ELECSYS_11Oct2025.csv\"\n",
    "df_csf = pd.read_csv(biomarker_path)\n",
    "\n",
    "df_csf['VISCODE_NORMALIZED'] = df_csf['VISCODE2'].astype(str).str.strip()\n",
    "df_csf = df_csf.dropna(subset=['ABETA42', 'TAU', 'PTAU'])\n",
    "\n",
    "df_csf['TAU_ABETA42_RATIO'] = df_csf['TAU'] / (df_csf['ABETA42'] + 1e-6)\n",
    "df_csf['PTAU_ABETA42_RATIO'] = df_csf['PTAU'] / (df_csf['ABETA42'] + 1e-6)\n",
    "df_csf['PTAU_TAU_RATIO'] = df_csf['PTAU'] / (df_csf['TAU'] + 1e-6)\n",
    "\n",
    "df['VISCODE_NORMALIZED'] = df['VISCODE2'].astype(str).str.strip().replace({'sc': 'bl', 'f': 'bl', 'nan': ''})\n",
    "df = df.merge(\n",
    "    df_csf[['RID', 'VISCODE_NORMALIZED', 'ABETA42', 'TAU', 'PTAU', \n",
    "            'TAU_ABETA42_RATIO', 'PTAU_ABETA42_RATIO', 'PTAU_TAU_RATIO']],\n",
    "    on=['RID', 'VISCODE_NORMALIZED'],\n",
    "    how='left'  # LEFT JOIN - mantener todas las visitas\n",
    ")\n",
    "\n",
    "df['HAS_CSF'] = df['ABETA42'].notna().astype(float)\n",
    "\n",
    "print(f\"âœ… CSF merged (LEFT JOIN): {df['HAS_CSF'].sum():.0f}/{len(df)} visitas con CSF ({100*df['HAS_CSF'].mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge_pet",
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_path = \"./data/adni/demographics/All_Subjects_UCBERKELEY_AMY_6MM_11Oct2025.csv\"\n",
    "df_pet = pd.read_csv(pet_path, low_memory=False)\n",
    "\n",
    "df_pet['VISCODE_NORMALIZED'] = df_pet['VISCODE'].astype(str).str.strip().replace({'sc': 'bl', 'f': 'bl', 'nan': ''})\n",
    "df_pet = df_pet[['RID', 'VISCODE_NORMALIZED', 'CENTILOIDS', 'SUMMARY_SUVR', 'COMPOSITE_REF_SUVR']].copy()\n",
    "df_pet.columns = ['RID', 'VISCODE_NORMALIZED', 'PET_CENTILOIDS', 'PET_SUVR', 'PET_COMPOSITE']\n",
    "df_pet = df_pet.dropna(subset=['PET_CENTILOIDS', 'PET_SUVR'])\n",
    "\n",
    "df = df.merge(df_pet, on=['RID', 'VISCODE_NORMALIZED'], how='left')\n",
    "df['HAS_PET'] = df['PET_CENTILOIDS'].notna().astype(float)\n",
    "\n",
    "print(f\"âœ… PET merged (LEFT JOIN): {df['HAS_PET'].sum():.0f}/{len(df)} visitas con PET ({100*df['HAS_PET'].mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge_mri",
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_path = \"./data/adni/demographics/All_Subjects_UCSFFSX7_11Oct2025.csv\"\n",
    "df_mri = pd.read_csv(mri_path, low_memory=False)\n",
    "\n",
    "df_mri['VISCODE_NORMALIZED'] = df_mri['VISCODE2'].astype(str).str.strip().replace({'sc': 'bl', 'f': 'bl', 'nan': ''})\n",
    "df_mri = df_mri[['RID', 'VISCODE_NORMALIZED', 'ST101SV', 'ST11SV', 'ST12SV', \n",
    "                 'ST4SV', 'ST5SV', 'ST17SV', 'ST18SV']].copy()\n",
    "df_mri.columns = ['RID', 'VISCODE_NORMALIZED', 'MRI_eTIV', 'MRI_Vol1', 'MRI_Vol2', \n",
    "                  'MRI_Vol3', 'MRI_Vol4', 'MRI_Vol5', 'MRI_Vol6']\n",
    "df_mri = df_mri.dropna(subset=['MRI_eTIV', 'MRI_Vol1'])\n",
    "\n",
    "df = df.merge(df_mri, on=['RID', 'VISCODE_NORMALIZED'], how='left')\n",
    "df['HAS_MRI'] = df['MRI_eTIV'].notna().astype(float)\n",
    "\n",
    "df = df.drop(columns=['VISCODE_NORMALIZED'])\n",
    "\n",
    "print(f\"âœ… MRI merged (LEFT JOIN): {df['HAS_MRI'].sum():.0f}/{len(df)} visitas con MRI ({100*df['HAS_MRI'].mean():.1f}%)\")\n",
    "print(f\"\\nðŸ“Š Dataset final: {len(df)} visitas con todos los biomarcadores opcionales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_visits",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_col = \"EXAMDATE\" if \"EXAMDATE\" in df.columns else (\"VISDATE\" if \"VISDATE\" in df.columns else None)\n",
    "if not date_col:\n",
    "    raise ValueError(\"No se encuentra columna de fecha (VISDATE/EXAMDATE)\")\n",
    "\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "df[\"EXAM_YEAR\"] = to_year(df[date_col].dt.year)\n",
    "\n",
    "df[\"AGE_AT_VISIT\"] = np.where(\n",
    "    df[\"EXAM_YEAR\"].notna() & df[\"PTDOBYY\"].notna(),\n",
    "    df[\"EXAM_YEAR\"] - df[\"PTDOBYY\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df[\"YEARS_TO_ONSET\"] = np.where(\n",
    "    df[\"YEAR_ONSET\"].notna() & df[\"EXAM_YEAR\"].notna(),\n",
    "    df[\"YEAR_ONSET\"] - df[\"EXAM_YEAR\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df.loc[(df[\"YEARS_TO_ONSET\"] < 0) & df[\"YEAR_ONSET\"].notna(), \"YEARS_TO_ONSET\"] = np.nan\n",
    "df.loc[df[\"YEARS_TO_ONSET\"] > 50, \"YEARS_TO_ONSET\"] = np.nan\n",
    "\n",
    "df[\"HAS_LABEL\"] = df[\"YEARS_TO_ONSET\"].notna()\n",
    "\n",
    "print(f\"\\nVisits prepared: {len(df)}\")\n",
    "print(f\"Labeled visits (with YEARS_TO_ONSET): {df['HAS_LABEL'].sum()}\")\n",
    "print(f\"Using date column: {date_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csf_cols = ['ABETA42', 'TAU', 'PTAU', 'TAU_ABETA42_RATIO', 'PTAU_ABETA42_RATIO', 'PTAU_TAU_RATIO']\n",
    "pet_cols = ['PET_CENTILOIDS', 'PET_SUVR', 'PET_COMPOSITE']\n",
    "mri_cols = ['MRI_eTIV', 'MRI_Vol1', 'MRI_Vol2', 'MRI_Vol3', 'MRI_Vol4', 'MRI_Vol5', 'MRI_Vol6']\n",
    "missing_indicators = ['HAS_CSF', 'HAS_PET', 'HAS_MRI']\n",
    "\n",
    "num_cols = [c for c in [\"AGE_AT_VISIT\", \"PTEDUCAT\"] + csf_cols + pet_cols + mri_cols + missing_indicators if c in df.columns]\n",
    "\n",
    "cat_cols = [c for c in [\"PTGENDER\", \"PTMARRY\"] if c in df.columns]\n",
    "\n",
    "for c in csf_cols + pet_cols + mri_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "for c in [\"AGE_AT_VISIT\", \"PTEDUCAT\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(df[c].median())\n",
    "\n",
    "parts = []\n",
    "\n",
    "if num_cols:\n",
    "    scaler = StandardScaler()\n",
    "    X_num = pd.DataFrame(\n",
    "        scaler.fit_transform(df[num_cols]),\n",
    "        columns=num_cols,\n",
    "        index=df.index\n",
    "    )\n",
    "    parts.append(X_num)\n",
    "    print(f\"Numeric features ({len(num_cols)}):\")\n",
    "    print(f\"  Demographics: 2\")\n",
    "    print(f\"  CSF biomarkers: {len(csf_cols)}\")\n",
    "    print(f\"  PET imaging: {len(pet_cols)}\")\n",
    "    print(f\"  MRI volumetric: {len(mri_cols)}\")\n",
    "    print(f\"  Missing indicators: {len(missing_indicators)}\")\n",
    "\n",
    "if cat_cols:\n",
    "    X_cat = pd.get_dummies(df[cat_cols], prefix=cat_cols, drop_first=False, dtype=float)\n",
    "    parts.append(X_cat)\n",
    "    print(f\"Categorical (one-hot): {X_cat.shape[1]} columns\")\n",
    "\n",
    "if not parts:\n",
    "    raise ValueError(\"No features available\")\n",
    "\n",
    "X = pd.concat(parts, axis=1).astype(float)\n",
    "X_clean = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "print(f\"\\nâœ… Feature matrix (ALL BIOMARKERS with missing handling): {X_clean.shape}\")\n",
    "print(f\"Total features: {X_clean.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build_graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_samples = X_clean.shape[0]\n",
    "k = min(8, max(1, n_samples - 1))\n",
    "n_neighbors = min(n_samples, k + 1)\n",
    "\n",
    "if n_samples >= 2:\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "    nbrs.fit(X_clean.values)\n",
    "    _, idx = nbrs.kneighbors(X_clean.values)\n",
    "    src_knn, dst_knn = [], []\n",
    "    for i in range(idx.shape[0]):\n",
    "        for j in idx[i, 1:]:\n",
    "            src_knn.append(i)\n",
    "            dst_knn.append(j)\n",
    "    edge_knn = torch.tensor([src_knn, dst_knn], dtype=torch.long)\n",
    "else:\n",
    "    edge_knn = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "tmp = df.reset_index()[[\"index\", \"RID\", date_col]].dropna(subset=[date_col]).sort_values([\"RID\", date_col])\n",
    "src_tmp, dst_tmp = [], []\n",
    "for rid, g in tmp.groupby(\"RID\"):\n",
    "    ids = g[\"index\"].tolist()\n",
    "    for a, b in zip(ids[:-1], ids[1:]):\n",
    "        src_tmp.append(a)\n",
    "        dst_tmp.append(b)\n",
    "edge_tmp = torch.tensor([src_tmp, dst_tmp], dtype=torch.long) if src_tmp else torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "def undirected(e):\n",
    "    return torch.cat([e, e.flip(0)], dim=1) if e.numel() else e\n",
    "\n",
    "edges = []\n",
    "if edge_knn.numel(): edges.append(undirected(edge_knn))\n",
    "if edge_tmp.numel(): edges.append(undirected(edge_tmp))\n",
    "edge_index = torch.cat(edges, dim=1) if edges else torch.empty((2,0), dtype=torch.long)\n",
    "if edge_index.numel():\n",
    "    edge_index = torch.unique(edge_index, dim=1)\n",
    "\n",
    "print(f\"\\nGraph constructed:\")\n",
    "print(f\"  Nodes: {len(df)}\")\n",
    "print(f\"  kNN edges: {edge_knn.size(1)}\")\n",
    "print(f\"  Temporal edges: {edge_tmp.size(1)}\")\n",
    "print(f\"  Total edges (undirected, unique): {edge_index.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"USE_FOR_LABEL\"] = False\n",
    "if df[\"HAS_LABEL\"].any():\n",
    "    idx_last_pre = df.loc[df[\"HAS_LABEL\"]].groupby(\"RID\")[date_col].idxmax()\n",
    "    df.loc[idx_last_pre, \"USE_FOR_LABEL\"] = True\n",
    "\n",
    "rids_with_label = df.loc[df[\"USE_FOR_LABEL\"], \"RID\"].dropna().unique()\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(rids_with_label)\n",
    "n_lab_rids = len(rids_with_label)\n",
    "\n",
    "tr_n = max(1, int(0.7 * n_lab_rids))\n",
    "va_n = max(0, int(0.15 * n_lab_rids))\n",
    "if tr_n + va_n > max(0, n_lab_rids - 1):\n",
    "    va_n = max(0, n_lab_rids - 1 - tr_n)\n",
    "\n",
    "train_rids = set(rids_with_label[:tr_n])\n",
    "val_rids   = set(rids_with_label[tr_n:tr_n+va_n])\n",
    "test_rids  = set(rids_with_label[tr_n+va_n:])\n",
    "\n",
    "node_split = np.full(len(df), \"train\", dtype=object)\n",
    "node_rids = df[\"RID\"].to_numpy()\n",
    "node_split[np.isin(node_rids, list(val_rids))]  = \"val\"\n",
    "node_split[np.isin(node_rids, list(test_rids))] = \"test\"\n",
    "\n",
    "use_for_label = df[\"USE_FOR_LABEL\"].to_numpy()\n",
    "train_mask_np = (node_split == \"train\") & use_for_label\n",
    "val_mask_np   = (node_split == \"val\")   & use_for_label\n",
    "test_mask_np  = (node_split == \"test\")  & use_for_label\n",
    "\n",
    "split_map = {\"train\":0, \"val\":1, \"test\":2}\n",
    "split_idx = np.vectorize(split_map.get)(node_split)\n",
    "src_np = edge_index[0].cpu().numpy()\n",
    "dst_np = edge_index[1].cpu().numpy()\n",
    "keep_edges = split_idx[src_np] == split_idx[dst_np]\n",
    "edge_index = edge_index[:, torch.tensor(keep_edges)]\n",
    "\n",
    "print(f\"\\nSplits created:\")\n",
    "print(f\"  RIDs with labels: {n_lab_rids}\")\n",
    "print(f\"  Train RIDs: {len(train_rids)}\")\n",
    "print(f\"  Val RIDs: {len(val_rids)}\")\n",
    "print(f\"  Test RIDs: {len(test_rids)}\")\n",
    "print(f\"\\n  Labeled nodes:\")\n",
    "print(f\"    Train: {train_mask_np.sum()}\")\n",
    "print(f\"    Val: {val_mask_np.sum()}\")\n",
    "print(f\"    Test: {test_mask_np.sum()}\")\n",
    "print(f\"\\n  Edges (intra-split): {edge_index.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_tensors",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_full = df[\"YEARS_TO_ONSET\"].astype(float)\n",
    "y_mu  = float(y_full[train_mask_np].mean()) if train_mask_np.any() else 0.0\n",
    "y_std = float(y_full[train_mask_np].std(ddof=0)) if train_mask_np.any() else 1.0\n",
    "if not np.isfinite(y_std) or y_std == 0.0:\n",
    "    y_std = 1.0\n",
    "\n",
    "y_scaled = (y_full - y_mu) / y_std\n",
    "y_t = torch.tensor(y_scaled.fillna(0).values, dtype=torch.float32)\n",
    "\n",
    "x = torch.tensor(X_clean.values, dtype=torch.float32)\n",
    "train_mask = torch.tensor(train_mask_np, dtype=torch.bool)\n",
    "val_mask   = torch.tensor(val_mask_np,   dtype=torch.bool)\n",
    "test_mask  = torch.tensor(test_mask_np,  dtype=torch.bool)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "print(f\"\\nData object created:\")\n",
    "print(f\"  x.shape: {data.x.shape}\")\n",
    "print(f\"  edge_index.shape: {data.edge_index.shape}\")\n",
    "print(f\"  y.shape: {data.y.shape}\")\n",
    "print(f\"  Target scaling: mean={y_mu:.3f}, std={y_std:.3f}\")\n",
    "\n",
    "Y_MEAN_TRAIN, Y_STD_TRAIN = y_mu, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = GNNRegressor(in_ch=data.num_node_features, hid=64, dropout=0.5).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "print(f\"\\nðŸ§  Model created (REGULARIZED):\")\n",
    "print(f\"  Input features: {data.num_node_features}\")\n",
    "print(f\"  Hidden dim: 64\")\n",
    "print(f\"  Dropout: 0.5 (increased for better regularization)\")\n",
    "print(f\"  Architecture: GCNConv -> ReLU -> Dropout(0.5) -> GCNConv -> Output\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early_stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping entre Ã©pocas 20-25\"\"\"\n",
    "    def __init__(self, patience=5, min_epoch=20, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_epoch = min_epoch  # No activar antes de Ã©poca 20\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def __call__(self, epoch, val_loss):\n",
    "        if epoch < self.min_epoch:\n",
    "            if self.best_loss is None or val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                self.best_epoch = epoch\n",
    "            return False\n",
    "        \n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, min_epoch=20, min_delta=1e-4)\n",
    "\n",
    "print(\"âœ… Early Stopping configured:\")\n",
    "print(\"  Patience: 5 epochs\")\n",
    "print(\"  Min epoch: 20 (starts checking after epoch 20)\")\n",
    "print(\"  Min delta: 1e-4\")\n",
    "print(\"  Expected stop: Between epochs 20-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_mae': [],\n",
    "    'train_rmse': [],\n",
    "    'val_mae': [],\n",
    "    'val_rmse': [],\n",
    "    'val_loss': [],\n",
    "    'test_mae': [],\n",
    "    'test_rmse': [],\n",
    "    'overfitting_gap': []  # train_mae - val_mae\n",
    "}\n",
    "\n",
    "def eval_metrics(split=\"val\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        mask = data.train_mask if split==\"train\" else (data.val_mask if split==\"val\" else data.test_mask)\n",
    "        if not mask.any():\n",
    "            return float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "        mae_scaled = torch.mean(torch.abs(out[mask] - data.y[mask])).item()\n",
    "        mse = loss_fn(out[mask], data.y[mask]).item()\n",
    "        rmse_scaled = torch.sqrt(torch.tensor(mse)).item()\n",
    "        return mae_scaled * Y_STD_TRAIN, rmse_scaled * Y_STD_TRAIN, mse\n",
    "\n",
    "print(\"\\nðŸš€ Starting training with Early Stopping + L2 Regularization...\\n\")\n",
    "print(\"Regularization settings:\")\n",
    "print(f\"  Dropout: 0.5\")\n",
    "print(f\"  L2 (weight_decay): 1e-3\")\n",
    "print(f\"  Learning rate: 1e-2\")\n",
    "print(f\"  Early stopping: Active after epoch 20\\n\")\n",
    "\n",
    "best_model_state = None\n",
    "\n",
    "for ep in range(1, 101):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    \n",
    "    if data.train_mask.any():\n",
    "        loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            raise RuntimeError(\"Loss NaN/Inf\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_val = loss.detach().item()\n",
    "    else:\n",
    "        loss_val = float(\"nan\")\n",
    "    \n",
    "    tr_mae, tr_rmse, _ = eval_metrics(\"train\")\n",
    "    va_mae, va_rmse, va_loss = eval_metrics(\"val\")\n",
    "    te_mae, te_rmse, _ = eval_metrics(\"test\")\n",
    "    \n",
    "    overfitting_gap = tr_mae - va_mae if not np.isnan(tr_mae) and not np.isnan(va_mae) else np.nan\n",
    "    \n",
    "    history['epoch'].append(ep)\n",
    "    history['train_loss'].append(loss_val)\n",
    "    history['train_mae'].append(tr_mae)\n",
    "    history['train_rmse'].append(tr_rmse)\n",
    "    history['val_mae'].append(va_mae)\n",
    "    history['val_rmse'].append(va_rmse)\n",
    "    history['val_loss'].append(va_loss)\n",
    "    history['test_mae'].append(te_mae)\n",
    "    history['test_rmse'].append(te_rmse)\n",
    "    history['overfitting_gap'].append(overfitting_gap)\n",
    "    \n",
    "    if va_loss == early_stopping.best_loss:\n",
    "        best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    if ep % 5 == 0 or ep <= 25:\n",
    "        gap_str = f\"gap={overfitting_gap:+.3f}y\" if not np.isnan(overfitting_gap) else \"gap=N/A\"\n",
    "        print(\n",
    "            f\"ep {ep:03d} | train_loss {loss_val:.4f} \"\n",
    "            f\"| TR MAE {tr_mae:.3f}y RMSE {tr_rmse:.3f}y \"\n",
    "            f\"| VAL MAE {va_mae:.3f}y RMSE {va_rmse:.3f}y \"\n",
    "            f\"| TEST MAE {te_mae:.3f}y RMSE {te_rmse:.3f}y | {gap_str}\"\n",
    "        )\n",
    "    \n",
    "    if early_stopping(ep, va_loss):\n",
    "        print(f\"\\nâ¸ï¸  Early stopping triggered at epoch {ep}!\")\n",
    "        print(f\"   Best validation loss: {early_stopping.best_loss:.4f} at epoch {early_stopping.best_epoch}\")\n",
    "        break\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")\n",
    "print(f\"Final epoch: {ep}\")\n",
    "print(f\"Best validation epoch: {early_stopping.best_epoch}\")\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"âœ… Best model restored (epoch {early_stopping.best_epoch})\")\n",
    "\n",
    "import json\n",
    "with open('training_history_regularized.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(\"âœ… Training history saved to: training_history_regularized.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_out = model(data.x, data.edge_index)\n",
    "\n",
    "tr_mae, tr_rmse, _ = eval_metrics(\"train\")\n",
    "va_mae, va_rmse, _ = eval_metrics(\"val\")\n",
    "te_mae, te_rmse, _ = eval_metrics(\"test\")\n",
    "\n",
    "final_gap = tr_mae - va_mae\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š REGULARIZED MODEL RESULTS (Early Stop + Dropout + L2)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Dataset: {len(df)} visits (ALL demographics)\")\n",
    "print(f\"Features: {data.num_node_features} (demographics + CSF + PET + MRI + indicators)\")\n",
    "print(f\"\\nBiomarker availability:\")\n",
    "print(f\"  CSF: {df['HAS_CSF'].sum():.0f} visits ({100*df['HAS_CSF'].mean():.1f}%)\")\n",
    "print(f\"  PET: {df['HAS_PET'].sum():.0f} visits ({100*df['HAS_PET'].mean():.1f}%)\")\n",
    "print(f\"  MRI: {df['HAS_MRI'].sum():.0f} visits ({100*df['HAS_MRI'].mean():.1f}%)\")\n",
    "print(f\"\\nRegularization applied:\")\n",
    "print(f\"  â¸ï¸  Early stopping: Epoch {early_stopping.best_epoch} (stopped at {ep})\")\n",
    "print(f\"  ðŸŽ² Dropout: 0.5\")\n",
    "print(f\"  âš–ï¸  L2 weight decay: 1e-3\")\n",
    "print(\"\\nPerformance:\")\n",
    "print(f\"  TRAIN | MAE: {tr_mae:.3f} years | RMSE: {tr_rmse:.3f} years\")\n",
    "print(f\"  VAL   | MAE: {va_mae:.3f} years | RMSE: {va_rmse:.3f} years\")\n",
    "print(f\"  TEST  | MAE: {te_mae:.3f} years | RMSE: {te_rmse:.3f} years\")\n",
    "print(f\"\\n  ðŸ“‰ Overfitting gap (train-val MAE): {final_gap:+.3f} years\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "regularized_results = {\n",
    "    'model': 'All Biomarkers + Early Stopping + Dropout(0.5) + L2(1e-3)',\n",
    "    'features': data.num_node_features,\n",
    "    'train_mae': tr_mae,\n",
    "    'train_rmse': tr_rmse,\n",
    "    'val_mae': va_mae,\n",
    "    'val_rmse': va_rmse,\n",
    "    'test_mae': te_mae,\n",
    "    'test_rmse': te_rmse,\n",
    "    'overfitting_gap': final_gap,\n",
    "    'n_visits': len(df),\n",
    "    'csf_coverage': float(df['HAS_CSF'].mean()),\n",
    "    'pet_coverage': float(df['HAS_PET'].mean()),\n",
    "    'mri_coverage': float(df['HAS_MRI'].mean()),\n",
    "    'early_stop_epoch': early_stopping.best_epoch,\n",
    "    'final_epoch': ep,\n",
    "    'dropout': 0.5,\n",
    "    'weight_decay': 1e-3\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ’¡ Benefits of regularization:\")\n",
    "print(\"   âœ“ Prevents overfitting (smaller gap between train/val)\")\n",
    "print(\"   âœ“ Better generalization to test set\")\n",
    "print(\"   âœ“ Stops training at optimal point (epochs 20-25)\")\n",
    "print(\"   âœ“ Saves computation time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "with open('regularized_model_results.json', 'w') as f:\n",
    "    json.dump(regularized_results, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Results saved to: regularized_model_results.json\")\n",
    "print(\"\\nðŸŽ¯ Next steps:\")\n",
    "print(\"   1. Compare with baseline model (AllBiomarkers.ipynb)\")\n",
    "print(\"   2. Check training curves for overfitting reduction\")\n",
    "print(\"   3. Verify early stopping worked correctly (epoch 20-25)\")\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"   - Stopped at epoch: {ep} (best: {early_stopping.best_epoch})\")\n",
    "print(f\"   - Test MAE: {te_mae:.3f} years\")\n",
    "print(f\"   - Overfitting gap: {final_gap:+.3f} years\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}