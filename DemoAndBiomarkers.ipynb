{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# GNN con DemogrÃ¡ficos + Biomarcadores CSF\n",
    "Este notebook extiende DemographicOnly.ipynb aÃ±adiendo biomarcadores de lÃ­quido cefalorraquÃ­deo (CSF):\n",
    "- ABETA42 (Amiloide beta 42)\n",
    "- TAU (ProteÃ­na tau total)\n",
    "- PTAU (ProteÃ­na tau fosforilada)\n",
    "\n",
    "Objetivo: Predecir aÃ±os hasta el inicio de sÃ­ntomas de Alzheimer usando GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "load_demographics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics loaded: (6210, 84)\n",
      "Columns: ['PHASE', 'PTID', 'RID', 'VISCODE', 'VISCODE2', 'VISDATE', 'PTSOURCE', 'PTGENDER', 'PTDOB', 'PTDOBYY', 'PTHAND', 'PTMARRY', 'PTEDUCAT', 'PTWORKHS', 'PTWORK', 'PTNOTRT', 'PTRTYR', 'PTHOME', 'PTTLANG', 'PTPLANG', 'PTADBEG', 'PTCOGBEG', 'PTADDX', 'PTETHCAT', 'PTRACCAT', 'PTIDENT', 'PTORIENT', 'PTORIENTOT', 'PTENGSPK', 'PTNLANG', 'PTENGSPKAGE', 'PTCLANG', 'PTLANGSP', 'PTLANGWR', 'PTSPTIM', 'PTSPOTTIM', 'PTLANGPR1', 'PTLANGSP1', 'PTLANGRD1', 'PTLANGWR1', 'PTLANGUN1', 'PTLANGPR2', 'PTLANGSP2', 'PTLANGRD2', 'PTLANGWR2', 'PTLANGUN2', 'PTLANGPR3', 'PTLANGSP3', 'PTLANGRD3', 'PTLANGWR3', 'PTLANGUN3', 'PTLANGPR4', 'PTLANGSP4', 'PTLANGRD4', 'PTLANGWR4', 'PTLANGUN4', 'PTLANGPR5', 'PTLANGSP5', 'PTLANGRD5', 'PTLANGWR5', 'PTLANGUN5', 'PTLANGPR6', 'PTLANGSP6', 'PTLANGRD6', 'PTLANGWR6', 'PTLANGUN6', 'PTLANGTTL', 'PTETHCATH', 'PTASIAN', 'PTOPI', 'PTBORN', 'PTBIRPL', 'PTIMMAGE', 'PTIMMWHY', 'PTBIRPR', 'PTBIRGR', 'ID', 'SITEID', 'USERDATE', 'USERDATE2', 'DD_CRF_VERSION_LABEL', 'LANGUAGE_CODE', 'HAS_QC_ERROR', 'update_stamp']\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Demographics loaded: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "load_biomarkers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Biomarkers loaded: (3174, 13)\n",
      "Columns: ['PHASE', 'PTID', 'RID', 'VISCODE2', 'EXAMDATE', 'BATCH', 'RUNDATE', 'ABETA40', 'ABETA42', 'TAU', 'PTAU', 'COMMENT', 'update_stamp']\n",
      "\n",
      "Missing values in biomarkers:\n",
      "ABETA40    2240\n",
      "ABETA42       7\n",
      "TAU          15\n",
      "PTAU         27\n",
      "dtype: int64\n",
      "\n",
      "Biomarker stats:\n",
      "           ABETA42          TAU         PTAU\n",
      "count  3167.000000  3159.000000  3147.000000\n",
      "mean   1062.012409   286.722605    27.273298\n",
      "std     620.162858   128.400411    14.270766\n",
      "min     203.000000    80.080000     8.000000\n",
      "25%     592.650000   195.700000    17.175000\n",
      "50%     870.400000   256.900000    23.540000\n",
      "75%    1423.500000   344.150000    33.455000\n",
      "max    4779.000000  1018.000000   108.500000\n"
     ]
    }
   ],
   "source": [
    "biomarker_path = \"./data/adni/demographics/UPENNBIOMK_ROCHE_ELECSYS_11Oct2025.csv\"\n",
    "df_bio = pd.read_csv(biomarker_path)\n",
    "print(f\"\\nBiomarkers loaded: {df_bio.shape}\")\n",
    "print(f\"Columns: {list(df_bio.columns)}\")\n",
    "print(f\"\\nMissing values in biomarkers:\")\n",
    "print(df_bio[['ABETA40', 'ABETA42', 'TAU', 'PTAU']].isnull().sum())\n",
    "\n",
    "bio_cols_to_use = ['RID', 'VISCODE2', 'EXAMDATE', 'ABETA42', 'TAU', 'PTAU']\n",
    "df_bio = df_bio[bio_cols_to_use].copy()\n",
    "\n",
    "for col in ['ABETA42', 'TAU', 'PTAU']:\n",
    "    df_bio[col] = pd.to_numeric(df_bio[col], errors='coerce')\n",
    "\n",
    "print(f\"\\nBiomarker stats:\")\n",
    "print(df_bio[['ABETA42', 'TAU', 'PTAU']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "utilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_codes_to_labels(s: pd.Series, mapping: dict) -> pd.Series:\n",
    "    out = s.astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    out = out.map(mapping)\n",
    "    return out\n",
    "\n",
    "gender_map = {\"1\":\"male\",\"2\":\"female\",\"male\":\"male\",\"female\":\"female\",\"m\":\"male\",\"f\":\"female\"}\n",
    "marry_map  = {\"1\":\"married\",\"2\":\"widowed\",\"3\":\"divorced\",\"4\":\"never_married\",\"6\":\"domestic_partnership\"}\n",
    "\n",
    "def to_year(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s.where((s >= 1900) & (s <= 2100))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "process_demographics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Demographics processed. Shape: (6210, 85)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "onset_cols = [c for c in [\"PTCOGBEG\",\"PTADBEG\",\"PTADDX\"] if c in df.columns]\n",
    "for c in onset_cols:\n",
    "    df[c] = to_year(df[c])\n",
    "\n",
    "def row_min_nonnull(row):\n",
    "    vals = [row[c] for c in onset_cols if pd.notna(row[c])]\n",
    "    return min(vals) if vals else np.nan\n",
    "\n",
    "df[\"YEAR_ONSET\"] = df.apply(row_min_nonnull, axis=1) if onset_cols else np.nan\n",
    "df[\"YEAR_ONSET\"] = to_year(df[\"YEAR_ONSET\"])\n",
    "\n",
    "for c in [\"PTDOBYY\",\"PTEDUCAT\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "if \"PTGENDER\" in df.columns:\n",
    "    df[\"PTGENDER\"] = norm_codes_to_labels(df[\"PTGENDER\"], gender_map)\n",
    "if \"PTMARRY\" in df.columns:\n",
    "    df[\"PTMARRY\"]  = norm_codes_to_labels(df[\"PTMARRY\"], marry_map)\n",
    "\n",
    "print(f\"\\nDemographics processed. Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "merge_biomarkers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging using RID + VISCODE2\n",
      "\n",
      "VISCODE mapping applied:\n",
      "  PTDEMOG unique values (original): ['bl', 'f', 'm03', 'm06', 'm12', 'm126', 'm132', 'm138', 'm144', 'm150']\n",
      "  PTDEMOG unique values (normalized): ['', 'bl', 'm03', 'm06', 'm12', 'm126', 'm132', 'm138', 'm144', 'm150']\n",
      "  UPENNBIOMK unique values: ['bl', 'm102', 'm108', 'm114', 'm12', 'm120', 'm126', 'm132', 'm138', 'm144']\n",
      "\n",
      "After merge: (6210, 88)\n",
      "Rows with biomarker data: 1793\n",
      "\n",
      "Missing biomarkers after merge:\n",
      "ABETA42    4419\n",
      "TAU        4424\n",
      "PTAU       4428\n",
      "dtype: int64\n",
      "\n",
      "Merge quality check:\n",
      "  % of rows with at least one biomarker: 28.9%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "date_col = \"EXAMDATE\" if \"EXAMDATE\" in df.columns else (\"VISDATE\" if \"VISDATE\" in df.columns else None)\n",
    "if not date_col:\n",
    "    raise ValueError(\"No se encuentra columna de fecha (VISDATE/EXAMDATE)\")\n",
    "\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "df_bio['EXAMDATE'] = pd.to_datetime(df_bio['EXAMDATE'], errors='coerce')\n",
    "\n",
    "\n",
    "if 'VISCODE2' in df.columns:\n",
    "    viscode_col = 'VISCODE2'\n",
    "elif 'VISCODE' in df.columns:\n",
    "    viscode_col = 'VISCODE'\n",
    "else:\n",
    "    viscode_col = None\n",
    "\n",
    "if viscode_col:\n",
    "    print(f\"Merging using RID + {viscode_col}\")\n",
    "    \n",
    "    df['VISCODE_NORMALIZED'] = df[viscode_col].astype(str).str.strip()\n",
    "    df['VISCODE_NORMALIZED'] = df['VISCODE_NORMALIZED'].replace({'sc': 'bl', 'f': 'bl', 'nan': ''})\n",
    "    \n",
    "    df_bio['VISCODE_NORMALIZED'] = df_bio['VISCODE2'].astype(str).str.strip()\n",
    "    \n",
    "    print(f\"\\nVISCODE mapping applied:\")\n",
    "    ptdemog_unique = df[viscode_col].dropna().unique()\n",
    "    ptdemog_norm_unique = df['VISCODE_NORMALIZED'].unique()\n",
    "    upenn_unique = df_bio['VISCODE_NORMALIZED'].unique()\n",
    "    \n",
    "    print(f\"  PTDEMOG unique values (original): {sorted([str(x) for x in ptdemog_unique])[:10]}\")\n",
    "    print(f\"  PTDEMOG unique values (normalized): {sorted([str(x) for x in ptdemog_norm_unique])[:10]}\")\n",
    "    print(f\"  UPENNBIOMK unique values: {sorted([str(x) for x in upenn_unique])[:10]}\")\n",
    "    \n",
    "    df_merged = df.merge(\n",
    "        df_bio[['RID', 'VISCODE_NORMALIZED', 'ABETA42', 'TAU', 'PTAU']], \n",
    "        on=['RID', 'VISCODE_NORMALIZED'],\n",
    "        how='left',\n",
    "        suffixes=('', '_bio')\n",
    "    )\n",
    "    \n",
    "    df_merged = df_merged.drop(columns=['VISCODE_NORMALIZED'])\n",
    "    \n",
    "else:\n",
    "    print(\"VISCODE not found. Merging using RID + closest date match\")\n",
    "    df_merged = pd.merge_asof(\n",
    "        df.sort_values([date_col]),\n",
    "        df_bio[['RID', 'EXAMDATE', 'ABETA42', 'TAU', 'PTAU']].sort_values('EXAMDATE'),\n",
    "        left_on=date_col,\n",
    "        right_on='EXAMDATE',\n",
    "        by='RID',\n",
    "        direction='nearest',\n",
    "        tolerance=pd.Timedelta('180 days')  # mÃ¡ximo 6 meses de diferencia\n",
    "    )\n",
    "\n",
    "df = df_merged\n",
    "\n",
    "print(f\"\\nAfter merge: {df.shape}\")\n",
    "print(f\"Rows with biomarker data: {df[['ABETA42', 'TAU', 'PTAU']].notna().any(axis=1).sum()}\")\n",
    "print(f\"\\nMissing biomarkers after merge:\")\n",
    "print(df[['ABETA42', 'TAU', 'PTAU']].isnull().sum())\n",
    "\n",
    "print(f\"\\nMerge quality check:\")\n",
    "print(f\"  % of rows with at least one biomarker: {df[['ABETA42', 'TAU', 'PTAU']].notna().any(axis=1).sum() / len(df) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "feature_engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed.\n",
      "New ratio features: TAU_ABETA42_RATIO, PTAU_ABETA42_RATIO, PTAU_TAU_RATIO\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df['TAU_ABETA42_RATIO'] = df['TAU'] / (df['ABETA42'] + 1e-6)\n",
    "df['PTAU_ABETA42_RATIO'] = df['PTAU'] / (df['ABETA42'] + 1e-6)\n",
    "df['PTAU_TAU_RATIO'] = df['PTAU'] / (df['TAU'] + 1e-6)\n",
    "\n",
    "print(\"Feature engineering completed.\")\n",
    "print(f\"New ratio features: TAU_ABETA42_RATIO, PTAU_ABETA42_RATIO, PTAU_TAU_RATIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ea599f6zlo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDACIÃ“N DEL MERGE - Verificando relaciÃ³n paciente-biomarcador\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Muestra de 3 pacientes con biomarcadores completos:\n",
      "\n",
      "Paciente RID=3:\n",
      " RID       PTID VISCODE2  ABETA42   TAU  PTAU\n",
      "   3 011_S_0003       sc    741.5 239.7 22.83\n",
      "\n",
      "Paciente RID=4:\n",
      " RID       PTID VISCODE2  ABETA42   TAU  PTAU\n",
      "   4 022_S_0004       sc   1501.0 153.1 13.29\n",
      "\n",
      "Paciente RID=5:\n",
      " RID       PTID VISCODE2  ABETA42   TAU  PTAU\n",
      "   5 011_S_0005       sc    547.3 337.0 33.43\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ðŸ“ˆ ESTADÃSTICAS DEL MERGE:\n",
      "======================================================================\n",
      "Total de visitas: 6210\n",
      "Visitas con al menos un biomarcador: 1793 (28.9%)\n",
      "Visitas con todos los biomarcadores: 1780 (28.7%)\n",
      "\n",
      "Pacientes Ãºnicos (RID) con biomarcadores: 1633/4945 (33.0%)\n",
      "\n",
      "âœ… Merge completado. Revisa los ejemplos arriba para verificar la coherencia.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDACIÃ“N DEL MERGE - Verificando relaciÃ³n paciente-biomarcador\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample_rids = df[df[['ABETA42', 'TAU', 'PTAU']].notna().all(axis=1)]['RID'].unique()[:3]\n",
    "\n",
    "print(f\"\\nðŸ“Š Muestra de {len(sample_rids)} pacientes con biomarcadores completos:\\n\")\n",
    "\n",
    "for rid in sample_rids:\n",
    "    patient_data = df[df['RID'] == rid][['RID', 'PTID', viscode_col if viscode_col else 'VISDATE', \n",
    "                                         'ABETA42', 'TAU', 'PTAU']].head(3)\n",
    "    print(f\"Paciente RID={rid}:\")\n",
    "    print(patient_data.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“ˆ ESTADÃSTICAS DEL MERGE:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_rows = len(df)\n",
    "rows_with_biomarkers = df[['ABETA42', 'TAU', 'PTAU']].notna().any(axis=1).sum()\n",
    "rows_complete_biomarkers = df[['ABETA42', 'TAU', 'PTAU']].notna().all(axis=1).sum()\n",
    "\n",
    "print(f\"Total de visitas: {total_rows}\")\n",
    "print(f\"Visitas con al menos un biomarcador: {rows_with_biomarkers} ({rows_with_biomarkers/total_rows*100:.1f}%)\")\n",
    "print(f\"Visitas con todos los biomarcadores: {rows_complete_biomarkers} ({rows_complete_biomarkers/total_rows*100:.1f}%)\")\n",
    "\n",
    "patients_with_bio = df[df[['ABETA42', 'TAU', 'PTAU']].notna().any(axis=1)]['RID'].nunique()\n",
    "total_patients = df['RID'].nunique()\n",
    "print(f\"\\nPacientes Ãºnicos (RID) con biomarcadores: {patients_with_bio}/{total_patients} ({patients_with_bio/total_patients*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ… Merge completado. Revisa los ejemplos arriba para verificar la coherencia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "prepare_visits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visits prepared: 6210\n",
      "Labeled visits (with YEARS_TO_ONSET): 82\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df[\"EXAM_YEAR\"] = to_year(df[date_col].dt.year)\n",
    "\n",
    "df[\"AGE_AT_VISIT\"] = np.where(\n",
    "    df[\"EXAM_YEAR\"].notna() & df[\"PTDOBYY\"].notna(),\n",
    "    df[\"EXAM_YEAR\"] - df[\"PTDOBYY\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df[\"YEARS_TO_ONSET\"] = np.where(\n",
    "    df[\"YEAR_ONSET\"].notna() & df[\"EXAM_YEAR\"].notna(),\n",
    "    df[\"YEAR_ONSET\"] - df[\"EXAM_YEAR\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df.loc[(df[\"YEARS_TO_ONSET\"] < 0) & df[\"YEAR_ONSET\"].notna(), \"YEARS_TO_ONSET\"] = np.nan\n",
    "df.loc[df[\"YEARS_TO_ONSET\"] > 50, \"YEARS_TO_ONSET\"] = np.nan\n",
    "\n",
    "df[\"HAS_LABEL\"] = df[\"YEARS_TO_ONSET\"].notna()\n",
    "\n",
    "print(f\"\\nVisits prepared: {len(df)}\")\n",
    "print(f\"Labeled visits (with YEARS_TO_ONSET): {df['HAS_LABEL'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "impute_biomarkers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before imputation:\n",
      "ABETA42               4419\n",
      "TAU                   4424\n",
      "PTAU                  4428\n",
      "TAU_ABETA42_RATIO     4426\n",
      "PTAU_ABETA42_RATIO    4430\n",
      "PTAU_TAU_RATIO        4428\n",
      "dtype: int64\n",
      "ABETA42: filled remaining NaN with median 942.500\n",
      "TAU: filled remaining NaN with median 244.800\n",
      "PTAU: filled remaining NaN with median 21.990\n",
      "TAU_ABETA42_RATIO: filled remaining NaN with median 0.226\n",
      "PTAU_ABETA42_RATIO: filled remaining NaN with median 0.021\n",
      "PTAU_TAU_RATIO: filled remaining NaN with median 0.091\n",
      "\n",
      "After imputation:\n",
      "ABETA42               0\n",
      "TAU                   0\n",
      "PTAU                  0\n",
      "TAU_ABETA42_RATIO     0\n",
      "PTAU_ABETA42_RATIO    0\n",
      "PTAU_TAU_RATIO        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "biomarker_cols = ['ABETA42', 'TAU', 'PTAU', 'TAU_ABETA42_RATIO', 'PTAU_ABETA42_RATIO', 'PTAU_TAU_RATIO']\n",
    "\n",
    "df = df.sort_values(['RID', date_col])\n",
    "\n",
    "print(f\"\\nBefore imputation:\")\n",
    "print(df[biomarker_cols].isnull().sum())\n",
    "\n",
    "for col in biomarker_cols:\n",
    "    df[col] = df.groupby('RID')[col].ffill()\n",
    "    df[col] = df.groupby('RID')[col].bfill()\n",
    "\n",
    "for col in biomarker_cols:\n",
    "    if df[col].isnull().any():\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "        print(f\"{col}: filled remaining NaN with median {median_val:.3f}\")\n",
    "\n",
    "print(f\"\\nAfter imputation:\")\n",
    "print(df[biomarker_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "build_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features (8): ['AGE_AT_VISIT', 'PTEDUCAT', 'ABETA42', 'TAU', 'PTAU', 'TAU_ABETA42_RATIO', 'PTAU_ABETA42_RATIO', 'PTAU_TAU_RATIO']\n",
      "Categorical features (2): ['PTGENDER', 'PTMARRY'] -> 7 one-hot columns\n",
      "\n",
      "Final feature matrix shape: (6210, 15)\n",
      "Total features: 15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_cols = [c for c in [\"AGE_AT_VISIT\", \"PTEDUCAT\"] + biomarker_cols if c in df.columns]\n",
    "\n",
    "cat_cols = [c for c in [\"PTGENDER\", \"PTMARRY\"] if c in df.columns]\n",
    "\n",
    "for c in [\"AGE_AT_VISIT\", \"PTEDUCAT\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(df[c].median())\n",
    "\n",
    "parts = []\n",
    "\n",
    "if num_cols:\n",
    "    scaler = StandardScaler()\n",
    "    X_num = pd.DataFrame(\n",
    "        scaler.fit_transform(df[num_cols]),\n",
    "        columns=num_cols,\n",
    "        index=df.index\n",
    "    )\n",
    "    parts.append(X_num)\n",
    "    print(f\"Numeric features ({len(num_cols)}): {num_cols}\")\n",
    "\n",
    "if cat_cols:\n",
    "    X_cat = pd.get_dummies(df[cat_cols], prefix=cat_cols, drop_first=False, dtype=float)\n",
    "    parts.append(X_cat)\n",
    "    print(f\"Categorical features ({len(cat_cols)}): {cat_cols} -> {X_cat.shape[1]} one-hot columns\")\n",
    "\n",
    "if not parts:\n",
    "    raise ValueError(\"No features available\")\n",
    "\n",
    "X = pd.concat(parts, axis=1).astype(float)\n",
    "X_clean = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {X_clean.shape}\")\n",
    "print(f\"Total features: {X_clean.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "build_graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph constructed:\n",
      "  Nodes: 6210\n",
      "  kNN edges: 49680\n",
      "  Temporal edges: 1264\n",
      "  Total edges (undirected, unique): 70699\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_samples = X_clean.shape[0]\n",
    "k = min(8, max(1, n_samples - 1))\n",
    "n_neighbors = min(n_samples, k + 1)\n",
    "\n",
    "if n_samples >= 2:\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "    nbrs.fit(X_clean.values)\n",
    "    _, idx = nbrs.kneighbors(X_clean.values)\n",
    "    src_knn, dst_knn = [], []\n",
    "    for i in range(idx.shape[0]):\n",
    "        for j in idx[i, 1:]:\n",
    "            src_knn.append(i)\n",
    "            dst_knn.append(j)\n",
    "    edge_knn = torch.tensor([src_knn, dst_knn], dtype=torch.long)\n",
    "else:\n",
    "    edge_knn = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "tmp = df.reset_index()[[\"index\", \"RID\", date_col]].dropna(subset=[date_col]).sort_values([\"RID\", date_col])\n",
    "src_tmp, dst_tmp = [], []\n",
    "for rid, g in tmp.groupby(\"RID\"):\n",
    "    ids = g[\"index\"].tolist()\n",
    "    for a, b in zip(ids[:-1], ids[1:]):\n",
    "        src_tmp.append(a)\n",
    "        dst_tmp.append(b)\n",
    "edge_tmp = torch.tensor([src_tmp, dst_tmp], dtype=torch.long) if src_tmp else torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "def undirected(e):\n",
    "    return torch.cat([e, e.flip(0)], dim=1) if e.numel() else e\n",
    "\n",
    "edges = []\n",
    "if edge_knn.numel(): edges.append(undirected(edge_knn))\n",
    "if edge_tmp.numel(): edges.append(undirected(edge_tmp))\n",
    "edge_index = torch.cat(edges, dim=1) if edges else torch.empty((2,0), dtype=torch.long)\n",
    "if edge_index.numel():\n",
    "    edge_index = torch.unique(edge_index, dim=1)\n",
    "\n",
    "print(f\"\\nGraph constructed:\")\n",
    "print(f\"  Nodes: {len(df)}\")\n",
    "print(f\"  kNN edges: {edge_knn.size(1)}\")\n",
    "print(f\"  Temporal edges: {edge_tmp.size(1)}\")\n",
    "print(f\"  Total edges (undirected, unique): {edge_index.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "splits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splits created:\n",
      "  RIDs with labels: 82\n",
      "  Train RIDs: 57\n",
      "  Val RIDs: 12\n",
      "  Test RIDs: 13\n",
      "\n",
      "  Labeled nodes for training:\n",
      "    Train: 57\n",
      "    Val: 12\n",
      "    Test: 13\n",
      "\n",
      "  Edges (intra-split only): 70041\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df[\"USE_FOR_LABEL\"] = False\n",
    "if df[\"HAS_LABEL\"].any():\n",
    "    idx_last_pre = df.loc[df[\"HAS_LABEL\"]].groupby(\"RID\")[date_col].idxmax()\n",
    "    df.loc[idx_last_pre, \"USE_FOR_LABEL\"] = True\n",
    "\n",
    "rids_with_label = df.loc[df[\"USE_FOR_LABEL\"], \"RID\"].dropna().unique()\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(rids_with_label)\n",
    "n_lab_rids = len(rids_with_label)\n",
    "\n",
    "tr_n = max(1, int(0.7 * n_lab_rids))\n",
    "va_n = max(0, int(0.15 * n_lab_rids))\n",
    "if tr_n + va_n > max(0, n_lab_rids - 1):\n",
    "    va_n = max(0, n_lab_rids - 1 - tr_n)\n",
    "\n",
    "train_rids = set(rids_with_label[:tr_n])\n",
    "val_rids   = set(rids_with_label[tr_n:tr_n+va_n])\n",
    "test_rids  = set(rids_with_label[tr_n+va_n:])\n",
    "\n",
    "node_split = np.full(len(df), \"train\", dtype=object)\n",
    "node_rids = df[\"RID\"].to_numpy()\n",
    "node_split[np.isin(node_rids, list(val_rids))]  = \"val\"\n",
    "node_split[np.isin(node_rids, list(test_rids))] = \"test\"\n",
    "\n",
    "use_for_label = df[\"USE_FOR_LABEL\"].to_numpy()\n",
    "train_mask_np = (node_split == \"train\") & use_for_label\n",
    "val_mask_np   = (node_split == \"val\")   & use_for_label\n",
    "test_mask_np  = (node_split == \"test\")  & use_for_label\n",
    "\n",
    "split_map = {\"train\":0, \"val\":1, \"test\":2}\n",
    "split_idx = np.vectorize(split_map.get)(node_split)\n",
    "src_np = edge_index[0].cpu().numpy()\n",
    "dst_np = edge_index[1].cpu().numpy()\n",
    "keep_edges = split_idx[src_np] == split_idx[dst_np]\n",
    "edge_index = edge_index[:, torch.tensor(keep_edges)]\n",
    "\n",
    "print(f\"\\nSplits created:\")\n",
    "print(f\"  RIDs with labels: {n_lab_rids}\")\n",
    "print(f\"  Train RIDs: {len(train_rids)}\")\n",
    "print(f\"  Val RIDs: {len(val_rids)}\")\n",
    "print(f\"  Test RIDs: {len(test_rids)}\")\n",
    "print(f\"\\n  Labeled nodes for training:\")\n",
    "print(f\"    Train: {train_mask_np.sum()}\")\n",
    "print(f\"    Val: {val_mask_np.sum()}\")\n",
    "print(f\"    Test: {test_mask_np.sum()}\")\n",
    "print(f\"\\n  Edges (intra-split only): {edge_index.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "prepare_tensors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data object created:\n",
      "  x.shape: torch.Size([6210, 15])\n",
      "  edge_index.shape: torch.Size([2, 70041])\n",
      "  y.shape: torch.Size([6210])\n",
      "  Target scaling: mean=0.018, std=0.131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_full = df[\"YEARS_TO_ONSET\"].astype(float)\n",
    "y_mu  = float(y_full[train_mask_np].mean()) if train_mask_np.any() else 0.0\n",
    "y_std = float(y_full[train_mask_np].std(ddof=0)) if train_mask_np.any() else 1.0\n",
    "if not np.isfinite(y_std) or y_std == 0.0:\n",
    "    y_std = 1.0\n",
    "\n",
    "y_scaled = (y_full - y_mu) / y_std\n",
    "y_t = torch.tensor(y_scaled.fillna(0).values, dtype=torch.float32)\n",
    "\n",
    "x = torch.tensor(X_clean.values, dtype=torch.float32)\n",
    "train_mask = torch.tensor(train_mask_np, dtype=torch.bool)\n",
    "val_mask   = torch.tensor(val_mask_np,   dtype=torch.bool)\n",
    "test_mask  = torch.tensor(test_mask_np,  dtype=torch.bool)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "print(f\"\\nData object created:\")\n",
    "print(f\"  x.shape: {data.x.shape}\")\n",
    "print(f\"  edge_index.shape: {data.edge_index.shape}\")\n",
    "print(f\"  y.shape: {data.y.shape}\")\n",
    "print(f\"  Target scaling: mean={y_mu:.3f}, std={y_std:.3f}\")\n",
    "\n",
    "Y_MEAN_TRAIN, Y_STD_TRAIN = y_mu, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model created:\n",
      "  Input features: 15\n",
      "  Hidden dim: 64\n",
      "  Architecture: GCNConv -> ReLU -> Dropout -> GCNConv -> Output\n",
      "\n",
      "Model parameters: 1089\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = GNNRegressor(in_ch=data.num_node_features, hid=64, dropout=0.3).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Input features: {data.num_node_features}\")\n",
    "print(f\"  Hidden dim: 64\")\n",
    "print(f\"  Architecture: GCNConv -> ReLU -> Dropout -> GCNConv -> Output\")\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "ep 010 | train_loss(MSE) 0.8799 | TR MAE 0.048y RMSE 0.123y | VAL MAE 0.037y RMSE 0.058y | TEST MAE 0.046y RMSE 0.061y\n",
      "ep 020 | train_loss(MSE) 0.7422 | TR MAE 0.050y RMSE 0.113y | VAL MAE 0.052y RMSE 0.090y | TEST MAE 0.067y RMSE 0.087y\n",
      "ep 030 | train_loss(MSE) 0.6695 | TR MAE 0.049y RMSE 0.107y | VAL MAE 0.060y RMSE 0.106y | TEST MAE 0.073y RMSE 0.099y\n",
      "ep 040 | train_loss(MSE) 0.6303 | TR MAE 0.043y RMSE 0.101y | VAL MAE 0.063y RMSE 0.114y | TEST MAE 0.068y RMSE 0.101y\n",
      "ep 050 | train_loss(MSE) 0.5297 | TR MAE 0.042y RMSE 0.093y | VAL MAE 0.067y RMSE 0.134y | TEST MAE 0.076y RMSE 0.118y\n",
      "ep 060 | train_loss(MSE) 0.4291 | TR MAE 0.042y RMSE 0.086y | VAL MAE 0.079y RMSE 0.160y | TEST MAE 0.084y RMSE 0.134y\n",
      "ep 070 | train_loss(MSE) 0.3527 | TR MAE 0.038y RMSE 0.077y | VAL MAE 0.082y RMSE 0.177y | TEST MAE 0.085y RMSE 0.141y\n",
      "ep 080 | train_loss(MSE) 0.3524 | TR MAE 0.037y RMSE 0.070y | VAL MAE 0.103y RMSE 0.214y | TEST MAE 0.094y RMSE 0.162y\n",
      "ep 090 | train_loss(MSE) 0.2929 | TR MAE 0.031y RMSE 0.065y | VAL MAE 0.108y RMSE 0.222y | TEST MAE 0.087y RMSE 0.157y\n",
      "ep 100 | train_loss(MSE) 0.2727 | TR MAE 0.032y RMSE 0.060y | VAL MAE 0.124y RMSE 0.265y | TEST MAE 0.103y RMSE 0.181y\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def eval_metrics(split=\"val\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        mask = data.train_mask if split==\"train\" else (data.val_mask if split==\"val\" else data.test_mask)\n",
    "        if not mask.any():\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "        mae_scaled = torch.mean(torch.abs(out[mask] - data.y[mask])).item()\n",
    "        rmse_scaled = torch.sqrt(loss_fn(out[mask], data.y[mask])).item()\n",
    "        return mae_scaled * Y_STD_TRAIN, rmse_scaled * Y_STD_TRAIN  # en aÃ±os\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "for ep in range(1, 101):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    \n",
    "    if data.train_mask.any():\n",
    "        loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            raise RuntimeError(\"Loss NaN/Inf; check data\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_val = loss.detach().item()\n",
    "    else:\n",
    "        loss_val = float(\"nan\")\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "        tr_mae, tr_rmse = eval_metrics(\"train\")\n",
    "        va_mae, va_rmse = eval_metrics(\"val\")\n",
    "        te_mae, te_rmse = eval_metrics(\"test\")\n",
    "        print(\n",
    "            f\"ep {ep:03d} | train_loss(MSE) {loss_val:.4f} \"\n",
    "            f\"| TR MAE {tr_mae:.3f}y RMSE {tr_rmse:.3f}y \"\n",
    "            f\"| VAL MAE {va_mae:.3f}y RMSE {va_rmse:.3f}y \"\n",
    "            f\"| TEST MAE {te_mae:.3f}y RMSE {te_rmse:.3f}y\"\n",
    "        )\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "final_eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULTS (with Demographics + CSF Biomarkers)\n",
      "============================================================\n",
      "TRAIN | MAE: 0.032 years | RMSE: 0.060 years\n",
      "VAL   | MAE: 0.124 years | RMSE: 0.265 years\n",
      "TEST  | MAE: 0.103 years | RMSE: 0.181 years\n",
      "============================================================\n",
      "\n",
      "NOTE: Compare these results with DemographicOnly.ipynb to see\n",
      "the improvement gained from adding CSF biomarkers!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_out = model(data.x, data.edge_index)\n",
    "\n",
    "tr_mae, tr_rmse = eval_metrics(\"train\")\n",
    "va_mae, va_rmse = eval_metrics(\"val\")\n",
    "te_mae, te_rmse = eval_metrics(\"test\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS (with Demographics + CSF Biomarkers)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"TRAIN | MAE: {tr_mae:.3f} years | RMSE: {tr_rmse:.3f} years\")\n",
    "print(f\"VAL   | MAE: {va_mae:.3f} years | RMSE: {va_rmse:.3f} years\")\n",
    "print(f\"TEST  | MAE: {te_mae:.3f} years | RMSE: {te_rmse:.3f} years\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNOTE: Compare these results with DemographicOnly.ipynb to see\")\n",
    "print(\"the improvement gained from adding CSF biomarkers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "save_embeddings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embeddings saved to: visit_embeddings_with_biomarkers.csv\n",
      "Shape: (6210, 66)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    h = F.relu(model.c1(data.x, data.edge_index))\n",
    "    embeddings = h.cpu().numpy()\n",
    "\n",
    "emb_df = pd.DataFrame(embeddings, index=df.index)\n",
    "emb_df.insert(0, \"RID\", df[\"RID\"].values)\n",
    "emb_df.insert(1, \"VISDATE\", pd.to_datetime(df[date_col]).dt.date.astype(\"string\"))\n",
    "emb_df.to_csv(\"visit_embeddings_with_biomarkers.csv\", index=False)\n",
    "\n",
    "print(\"\\nEmbeddings saved to: visit_embeddings_with_biomarkers.csv\")\n",
    "print(f\"Shape: {emb_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588dcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "icji41faov",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Results saved to: enhanced_biomarkers_results.json\n",
      "\n",
      "You can now run CompareModels.ipynb to compare with baseline!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "enhanced_results = {\n",
    "    'model': 'Demographics + CSF Biomarkers',\n",
    "    'features': data.num_node_features,\n",
    "    'train_mae': tr_mae,\n",
    "    'train_rmse': tr_rmse,\n",
    "    'val_mae': va_mae,\n",
    "    'val_rmse': va_rmse,\n",
    "    'test_mae': te_mae,\n",
    "    'test_rmse': te_rmse\n",
    "}\n",
    "\n",
    "with open('enhanced_biomarkers_results.json', 'w') as f:\n",
    "    json.dump(enhanced_results, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Results saved to: enhanced_biomarkers_results.json\")\n",
    "print(\"\\nYou can now run CompareModels.ipynb to compare with baseline!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}