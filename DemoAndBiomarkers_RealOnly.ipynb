{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# GNN con Demográficos + Biomarcadores CSF (Solo datos reales)\n",
    "Este notebook es una **versión mejorada** que usa únicamente visitas con biomarcadores **reales**.\n",
    "\n",
    "**Diferencia clave con DemoAndBiomarkers.ipynb:**\n",
    "- ❌ **Versión anterior**: Imputaba biomarcadores faltantes (~71% imputados)\n",
    "- ✅ **Esta versión**: Solo usa visitas con biomarcadores medidos (~29% de datos, pero reales)\n",
    "\n",
    "**Features:**\n",
    "- Demographics: AGE_AT_VISIT, PTEDUCAT, PTGENDER, PTMARRY\n",
    "- Biomarkers: ABETA42, TAU, PTAU\n",
    "- Ratios: TAU/ABETA42, PTAU/ABETA42, PTAU/TAU\n",
    "\n",
    "**Objetivo**: Predecir años hasta inicio de Alzheimer con datos de alta calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_demographics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics loaded: (6210, 84)\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Demographics loaded: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_biomarkers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biomarkers loaded: (3174, 13)\n",
      "\n",
      "Missing values:\n",
      "ABETA42     7\n",
      "TAU        15\n",
      "PTAU       27\n",
      "dtype: int64\n",
      "\n",
      "✅ After filtering complete biomarkers: 3143 visits with real data\n"
     ]
    }
   ],
   "source": [
    "biomarker_path = \"./data/adni/demographics/UPENNBIOMK_ROCHE_ELECSYS_11Oct2025.csv\"\n",
    "df_bio = pd.read_csv(biomarker_path)\n",
    "\n",
    "print(f\"Biomarkers loaded: {df_bio.shape}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df_bio[['ABETA42', 'TAU', 'PTAU']].isnull().sum())\n",
    "\n",
    "bio_cols_to_use = ['RID', 'VISCODE2', 'EXAMDATE', 'ABETA42', 'TAU', 'PTAU']\n",
    "df_bio = df_bio[bio_cols_to_use].copy()\n",
    "\n",
    "for col in ['ABETA42', 'TAU', 'PTAU']:\n",
    "    df_bio[col] = pd.to_numeric(df_bio[col], errors='coerce')\n",
    "\n",
    "df_bio = df_bio.dropna(subset=['ABETA42', 'TAU', 'PTAU'])\n",
    "print(f\"\\n✅ After filtering complete biomarkers: {df_bio.shape[0]} visits with real data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "utilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_codes_to_labels(s: pd.Series, mapping: dict) -> pd.Series:\n",
    "    out = s.astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    out = out.map(mapping)\n",
    "    return out\n",
    "\n",
    "gender_map = {\"1\":\"male\",\"2\":\"female\",\"male\":\"male\",\"female\":\"female\",\"m\":\"male\",\"f\":\"female\"}\n",
    "marry_map  = {\"1\":\"married\",\"2\":\"widowed\",\"3\":\"divorced\",\"4\":\"never_married\",\"6\":\"domestic_partnership\"}\n",
    "\n",
    "def to_year(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s.where((s >= 1900) & (s <= 2100))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "process_demographics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics processed: (6210, 85)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "onset_cols = [c for c in [\"PTCOGBEG\",\"PTADBEG\",\"PTADDX\"] if c in df.columns]\n",
    "for c in onset_cols:\n",
    "    df[c] = to_year(df[c])\n",
    "\n",
    "def row_min_nonnull(row):\n",
    "    vals = [row[c] for c in onset_cols if pd.notna(row[c])]\n",
    "    return min(vals) if vals else np.nan\n",
    "\n",
    "df[\"YEAR_ONSET\"] = df.apply(row_min_nonnull, axis=1) if onset_cols else np.nan\n",
    "df[\"YEAR_ONSET\"] = to_year(df[\"YEAR_ONSET\"])\n",
    "\n",
    "for c in [\"PTDOBYY\",\"PTEDUCAT\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "if \"PTGENDER\" in df.columns:\n",
    "    df[\"PTGENDER\"] = norm_codes_to_labels(df[\"PTGENDER\"], gender_map)\n",
    "if \"PTMARRY\" in df.columns:\n",
    "    df[\"PTMARRY\"]  = norm_codes_to_labels(df[\"PTMARRY\"], marry_map)\n",
    "\n",
    "print(f\"Demographics processed: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "merge_biomarkers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging using RID + VISCODE2\n",
      "\n",
      "✅ After merge (INNER JOIN - only real biomarkers): (1780, 88)\n",
      "Rows with complete biomarkers: 1780\n",
      "Unique patients with biomarkers: 1622\n"
     ]
    }
   ],
   "source": [
    "\n",
    "date_col = \"EXAMDATE\" if \"EXAMDATE\" in df.columns else (\"VISDATE\" if \"VISDATE\" in df.columns else None)\n",
    "if not date_col:\n",
    "    raise ValueError(\"No date column found\")\n",
    "\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "df_bio['EXAMDATE'] = pd.to_datetime(df_bio['EXAMDATE'], errors='coerce')\n",
    "\n",
    "viscode_col = 'VISCODE2' if 'VISCODE2' in df.columns else ('VISCODE' if 'VISCODE' in df.columns else None)\n",
    "\n",
    "if viscode_col:\n",
    "    print(f\"Merging using RID + {viscode_col}\")\n",
    "    \n",
    "    df['VISCODE_NORMALIZED'] = df[viscode_col].astype(str).str.strip().replace({'sc': 'bl', 'f': 'bl', 'nan': ''})\n",
    "    df_bio['VISCODE_NORMALIZED'] = df_bio['VISCODE2'].astype(str).str.strip()\n",
    "    \n",
    "    df_merged = df.merge(\n",
    "        df_bio[['RID', 'VISCODE_NORMALIZED', 'ABETA42', 'TAU', 'PTAU']], \n",
    "        on=['RID', 'VISCODE_NORMALIZED'],\n",
    "        how='inner',  # INNER JOIN: solo visitas con biomarcadores\n",
    "        suffixes=('', '_bio')\n",
    "    )\n",
    "    \n",
    "    df_merged = df_merged.drop(columns=['VISCODE_NORMALIZED'])\n",
    "else:\n",
    "    print(\"Using date-based merge\")\n",
    "    df_merged = pd.merge_asof(\n",
    "        df.sort_values([date_col]),\n",
    "        df_bio[['RID', 'EXAMDATE', 'ABETA42', 'TAU', 'PTAU']].sort_values('EXAMDATE'),\n",
    "        left_on=date_col,\n",
    "        right_on='EXAMDATE',\n",
    "        by='RID',\n",
    "        direction='nearest',\n",
    "        tolerance=pd.Timedelta('30 days')  # Match estricto: máximo 30 días\n",
    "    )\n",
    "    df_merged = df_merged.dropna(subset=['ABETA42', 'TAU', 'PTAU'])\n",
    "\n",
    "df = df_merged\n",
    "\n",
    "print(f\"\\n✅ After merge (INNER JOIN - only real biomarkers): {df.shape}\")\n",
    "print(f\"Rows with complete biomarkers: {df[['ABETA42', 'TAU', 'PTAU']].notna().all(axis=1).sum()}\")\n",
    "print(f\"Unique patients with biomarkers: {df['RID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feature_engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed.\n",
      "Created ratios: TAU_ABETA42_RATIO, PTAU_ABETA42_RATIO, PTAU_TAU_RATIO\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['TAU_ABETA42_RATIO'] = df['TAU'] / (df['ABETA42'] + 1e-6)\n",
    "df['PTAU_ABETA42_RATIO'] = df['PTAU'] / (df['ABETA42'] + 1e-6)\n",
    "df['PTAU_TAU_RATIO'] = df['PTAU'] / (df['TAU'] + 1e-6)\n",
    "\n",
    "print(\"Feature engineering completed.\")\n",
    "print(f\"Created ratios: TAU_ABETA42_RATIO, PTAU_ABETA42_RATIO, PTAU_TAU_RATIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prepare_visits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visits prepared: 1780\n",
      "Labeled visits: 23\n",
      "Unique patients: 1622\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df[\"EXAM_YEAR\"] = to_year(df[date_col].dt.year)\n",
    "\n",
    "df[\"AGE_AT_VISIT\"] = np.where(\n",
    "    df[\"EXAM_YEAR\"].notna() & df[\"PTDOBYY\"].notna(),\n",
    "    df[\"EXAM_YEAR\"] - df[\"PTDOBYY\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df[\"YEARS_TO_ONSET\"] = np.where(\n",
    "    df[\"YEAR_ONSET\"].notna() & df[\"EXAM_YEAR\"].notna(),\n",
    "    df[\"YEAR_ONSET\"] - df[\"EXAM_YEAR\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df.loc[(df[\"YEARS_TO_ONSET\"] < 0) & df[\"YEAR_ONSET\"].notna(), \"YEARS_TO_ONSET\"] = np.nan\n",
    "df.loc[df[\"YEARS_TO_ONSET\"] > 50, \"YEARS_TO_ONSET\"] = np.nan\n",
    "\n",
    "df[\"HAS_LABEL\"] = df[\"YEARS_TO_ONSET\"].notna()\n",
    "\n",
    "print(f\"\\nVisits prepared: {len(df)}\")\n",
    "print(f\"Labeled visits: {df['HAS_LABEL'].sum()}\")\n",
    "print(f\"Unique patients: {df['RID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "build_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features (8): ['AGE_AT_VISIT', 'PTEDUCAT', 'ABETA42', 'TAU', 'PTAU', 'TAU_ABETA42_RATIO', 'PTAU_ABETA42_RATIO', 'PTAU_TAU_RATIO']\n",
      "Categorical features (2): ['PTGENDER', 'PTMARRY'] -> 6 columns\n",
      "\n",
      "✅ Feature matrix (REAL biomarkers only): (1780, 14)\n",
      "Total features: 14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "biomarker_cols = ['ABETA42', 'TAU', 'PTAU', 'TAU_ABETA42_RATIO', 'PTAU_ABETA42_RATIO', 'PTAU_TAU_RATIO']\n",
    "num_cols = [c for c in [\"AGE_AT_VISIT\", \"PTEDUCAT\"] + biomarker_cols if c in df.columns]\n",
    "cat_cols = [c for c in [\"PTGENDER\", \"PTMARRY\"] if c in df.columns]\n",
    "\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(df[c].median())\n",
    "\n",
    "parts = []\n",
    "\n",
    "if num_cols:\n",
    "    scaler = StandardScaler()\n",
    "    X_num = pd.DataFrame(\n",
    "        scaler.fit_transform(df[num_cols]),\n",
    "        columns=num_cols,\n",
    "        index=df.index\n",
    "    )\n",
    "    parts.append(X_num)\n",
    "    print(f\"Numeric features ({len(num_cols)}): {num_cols}\")\n",
    "\n",
    "if cat_cols:\n",
    "    X_cat = pd.get_dummies(df[cat_cols], prefix=cat_cols, drop_first=False, dtype=float)\n",
    "    parts.append(X_cat)\n",
    "    print(f\"Categorical features ({len(cat_cols)}): {cat_cols} -> {X_cat.shape[1]} columns\")\n",
    "\n",
    "X = pd.concat(parts, axis=1).astype(float)\n",
    "X_clean = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "print(f\"\\n✅ Feature matrix (REAL biomarkers only): {X_clean.shape}\")\n",
    "print(f\"Total features: {X_clean.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "build_graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph: 1780 nodes, 19808 edges\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_samples = X_clean.shape[0]\n",
    "k = min(8, max(1, n_samples - 1))\n",
    "n_neighbors = min(n_samples, k + 1)\n",
    "\n",
    "if n_samples >= 2:\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "    nbrs.fit(X_clean.values)\n",
    "    _, idx = nbrs.kneighbors(X_clean.values)\n",
    "    src_knn, dst_knn = [], []\n",
    "    for i in range(idx.shape[0]):\n",
    "        for j in idx[i, 1:]:\n",
    "            src_knn.append(i)\n",
    "            dst_knn.append(j)\n",
    "    edge_knn = torch.tensor([src_knn, dst_knn], dtype=torch.long)\n",
    "else:\n",
    "    edge_knn = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "tmp = df.reset_index()[[\"index\", \"RID\", date_col]].dropna(subset=[date_col]).sort_values([\"RID\", date_col])\n",
    "src_tmp, dst_tmp = [], []\n",
    "for rid, g in tmp.groupby(\"RID\"):\n",
    "    ids = g[\"index\"].tolist()\n",
    "    for a, b in zip(ids[:-1], ids[1:]):\n",
    "        src_tmp.append(a)\n",
    "        dst_tmp.append(b)\n",
    "edge_tmp = torch.tensor([src_tmp, dst_tmp], dtype=torch.long) if src_tmp else torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "def undirected(e):\n",
    "    return torch.cat([e, e.flip(0)], dim=1) if e.numel() else e\n",
    "\n",
    "edges = []\n",
    "if edge_knn.numel(): edges.append(undirected(edge_knn))\n",
    "if edge_tmp.numel(): edges.append(undirected(edge_tmp))\n",
    "edge_index = torch.cat(edges, dim=1) if edges else torch.empty((2,0), dtype=torch.long)\n",
    "if edge_index.numel():\n",
    "    edge_index = torch.unique(edge_index, dim=1)\n",
    "\n",
    "print(f\"\\nGraph: {len(df)} nodes, {edge_index.size(1)} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "splits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splits: train=16, val=3, test=4 RIDs\n",
      "Labels: train=16, val=3, test=4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df[\"USE_FOR_LABEL\"] = False\n",
    "if df[\"HAS_LABEL\"].any():\n",
    "    idx_last_pre = df.loc[df[\"HAS_LABEL\"]].groupby(\"RID\")[date_col].idxmax()\n",
    "    df.loc[idx_last_pre, \"USE_FOR_LABEL\"] = True\n",
    "\n",
    "rids_with_label = df.loc[df[\"USE_FOR_LABEL\"], \"RID\"].dropna().unique()\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(rids_with_label)\n",
    "n_lab_rids = len(rids_with_label)\n",
    "\n",
    "tr_n = max(1, int(0.7 * n_lab_rids))\n",
    "va_n = max(0, int(0.15 * n_lab_rids))\n",
    "if tr_n + va_n > max(0, n_lab_rids - 1):\n",
    "    va_n = max(0, n_lab_rids - 1 - tr_n)\n",
    "\n",
    "train_rids = set(rids_with_label[:tr_n])\n",
    "val_rids   = set(rids_with_label[tr_n:tr_n+va_n])\n",
    "test_rids  = set(rids_with_label[tr_n+va_n:])\n",
    "\n",
    "node_split = np.full(len(df), \"train\", dtype=object)\n",
    "node_rids = df[\"RID\"].to_numpy()\n",
    "node_split[np.isin(node_rids, list(val_rids))]  = \"val\"\n",
    "node_split[np.isin(node_rids, list(test_rids))] = \"test\"\n",
    "\n",
    "use_for_label = df[\"USE_FOR_LABEL\"].to_numpy()\n",
    "train_mask_np = (node_split == \"train\") & use_for_label\n",
    "val_mask_np   = (node_split == \"val\")   & use_for_label\n",
    "test_mask_np  = (node_split == \"test\")  & use_for_label\n",
    "\n",
    "split_map = {\"train\":0, \"val\":1, \"test\":2}\n",
    "split_idx = np.vectorize(split_map.get)(node_split)\n",
    "src_np = edge_index[0].cpu().numpy()\n",
    "dst_np = edge_index[1].cpu().numpy()\n",
    "keep_edges = split_idx[src_np] == split_idx[dst_np]\n",
    "edge_index = edge_index[:, torch.tensor(keep_edges)]\n",
    "\n",
    "print(f\"\\nSplits: train={len(train_rids)}, val={len(val_rids)}, test={len(test_rids)} RIDs\")\n",
    "print(f\"Labels: train={train_mask_np.sum()}, val={val_mask_np.sum()}, test={test_mask_np.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prepare_tensors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data: xtorch.Size([1780, 14]), edgestorch.Size([2, 19642])\n",
      "Target scaling: mean=0.000, std=1.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_full = df[\"YEARS_TO_ONSET\"].astype(float)\n",
    "y_mu  = float(y_full[train_mask_np].mean()) if train_mask_np.any() else 0.0\n",
    "y_std = float(y_full[train_mask_np].std(ddof=0)) if train_mask_np.any() else 1.0\n",
    "if not np.isfinite(y_std) or y_std == 0.0:\n",
    "    y_std = 1.0\n",
    "\n",
    "y_scaled = (y_full - y_mu) / y_std\n",
    "y_t = torch.tensor(y_scaled.fillna(0).values, dtype=torch.float32)\n",
    "\n",
    "x = torch.tensor(X_clean.values, dtype=torch.float32)\n",
    "train_mask = torch.tensor(train_mask_np, dtype=torch.bool)\n",
    "val_mask   = torch.tensor(val_mask_np,   dtype=torch.bool)\n",
    "test_mask  = torch.tensor(test_mask_np,  dtype=torch.bool)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "print(f\"\\nData: x{data.x.shape}, edges{data.edge_index.shape}\")\n",
    "print(f\"Target scaling: mean={y_mu:.3f}, std={y_std:.3f}\")\n",
    "\n",
    "Y_MEAN_TRAIN, Y_STD_TRAIN = y_mu, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: 14 features -> 64 hidden -> 1 output\n",
      "Parameters: 1025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = GNNRegressor(in_ch=data.num_node_features, hid=64, dropout=0.3).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "print(f\"\\nModel: {data.num_node_features} features -> 64 hidden -> 1 output\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "\n",
      "ep 010 | loss 0.0137 | TR MAE 0.070y RMSE 0.080y | VAL MAE 0.110y RMSE 0.183y | TEST MAE 0.092y RMSE 0.106y\n",
      "ep 020 | loss 0.0092 | TR MAE 0.065y RMSE 0.088y | VAL MAE 0.149y RMSE 0.202y | TEST MAE 0.088y RMSE 0.110y\n",
      "ep 030 | loss 0.0205 | TR MAE 0.106y RMSE 0.140y | VAL MAE 0.226y RMSE 0.316y | TEST MAE 0.169y RMSE 0.199y\n",
      "ep 040 | loss 0.0140 | TR MAE 0.080y RMSE 0.088y | VAL MAE 0.143y RMSE 0.210y | TEST MAE 0.134y RMSE 0.156y\n",
      "ep 050 | loss 0.0115 | TR MAE 0.067y RMSE 0.094y | VAL MAE 0.063y RMSE 0.066y | TEST MAE 0.107y RMSE 0.114y\n",
      "ep 060 | loss 0.0109 | TR MAE 0.044y RMSE 0.054y | VAL MAE 0.094y RMSE 0.126y | TEST MAE 0.039y RMSE 0.041y\n",
      "ep 070 | loss 0.0014 | TR MAE 0.029y RMSE 0.035y | VAL MAE 0.111y RMSE 0.173y | TEST MAE 0.076y RMSE 0.090y\n",
      "ep 080 | loss 0.0244 | TR MAE 0.050y RMSE 0.067y | VAL MAE 0.047y RMSE 0.079y | TEST MAE 0.063y RMSE 0.099y\n",
      "ep 090 | loss 0.0108 | TR MAE 0.033y RMSE 0.043y | VAL MAE 0.099y RMSE 0.146y | TEST MAE 0.076y RMSE 0.090y\n",
      "ep 100 | loss 0.0021 | TR MAE 0.039y RMSE 0.062y | VAL MAE 0.103y RMSE 0.171y | TEST MAE 0.088y RMSE 0.113y\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def eval_metrics(split=\"val\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        mask = data.train_mask if split==\"train\" else (data.val_mask if split==\"val\" else data.test_mask)\n",
    "        if not mask.any():\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "        mae_scaled = torch.mean(torch.abs(out[mask] - data.y[mask])).item()\n",
    "        rmse_scaled = torch.sqrt(loss_fn(out[mask], data.y[mask])).item()\n",
    "        return mae_scaled * Y_STD_TRAIN, rmse_scaled * Y_STD_TRAIN\n",
    "\n",
    "print(\"\\nTraining...\\n\")\n",
    "\n",
    "for ep in range(1, 101):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    \n",
    "    if data.train_mask.any():\n",
    "        loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            raise RuntimeError(\"Loss NaN/Inf\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_val = loss.detach().item()\n",
    "    else:\n",
    "        loss_val = float(\"nan\")\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "        tr_mae, tr_rmse = eval_metrics(\"train\")\n",
    "        va_mae, va_rmse = eval_metrics(\"val\")\n",
    "        te_mae, te_rmse = eval_metrics(\"test\")\n",
    "        print(\n",
    "            f\"ep {ep:03d} | loss {loss_val:.4f} \"\n",
    "            f\"| TR MAE {tr_mae:.3f}y RMSE {tr_rmse:.3f}y \"\n",
    "            f\"| VAL MAE {va_mae:.3f}y RMSE {va_rmse:.3f}y \"\n",
    "            f\"| TEST MAE {te_mae:.3f}y RMSE {te_rmse:.3f}y\"\n",
    "        )\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "final_eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL RESULTS (Real Biomarkers Only)\n",
      "======================================================================\n",
      "Dataset: 1780 visits from 1622 patients\n",
      "Features: 14 (demographics + real CSF biomarkers)\n",
      "\n",
      "Performance:\n",
      "  TRAIN | MAE: 0.039 years | RMSE: 0.062 years\n",
      "  VAL   | MAE: 0.103 years | RMSE: 0.171 years\n",
      "  TEST  | MAE: 0.088 years | RMSE: 0.113 years\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_out = model(data.x, data.edge_index)\n",
    "\n",
    "tr_mae, tr_rmse = eval_metrics(\"train\")\n",
    "va_mae, va_rmse = eval_metrics(\"val\")\n",
    "te_mae, te_rmse = eval_metrics(\"test\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS (Real Biomarkers Only)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Dataset: {len(df)} visits from {df['RID'].nunique()} patients\")\n",
    "print(f\"Features: {data.num_node_features} (demographics + real CSF biomarkers)\")\n",
    "print(\"\\nPerformance:\")\n",
    "print(f\"  TRAIN | MAE: {tr_mae:.3f} years | RMSE: {tr_rmse:.3f} years\")\n",
    "print(f\"  VAL   | MAE: {va_mae:.3f} years | RMSE: {va_rmse:.3f} years\")\n",
    "print(f\"  TEST  | MAE: {te_mae:.3f} years | RMSE: {te_rmse:.3f} years\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "save_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: real_biomarkers_results.json\n",
      "\n",
      "Compare with baseline using CompareModels.ipynb!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'model': 'Demographics + CSF Biomarkers (Real Only)',\n",
    "    'features': data.num_node_features,\n",
    "    'train_mae': tr_mae,\n",
    "    'train_rmse': tr_rmse,\n",
    "    'val_mae': va_mae,\n",
    "    'val_rmse': va_rmse,\n",
    "    'test_mae': te_mae,\n",
    "    'test_rmse': te_rmse,\n",
    "    'n_visits': len(df),\n",
    "    'n_patients': int(df['RID'].nunique())\n",
    "}\n",
    "\n",
    "with open('real_biomarkers_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nResults saved to: real_biomarkers_results.json\")\n",
    "print(\"\\nCompare with baseline using CompareModels.ipynb!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}