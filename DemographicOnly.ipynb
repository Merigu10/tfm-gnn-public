{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n",
      "Exe: c:\\Users\\guzmam19\\VSCode\\tfm-gnn\\.venv\\Scripts\\python.exe\n",
      "Torch: 2.8.0+cpu | CUDA? False | CUDA runtime: None\n",
      "PyG: 2.6.1\n"
     ]
    }
   ],
   "source": [
    "import sys, torch, torch_geometric\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Exe:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA?\", torch.cuda.is_available(), \"| CUDA runtime:\", torch.version.cuda)\n",
    "print(\"PyG:\", torch_geometric.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa47681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 | Loss 0.8703 | ValAcc 0.716 | TestAcc 0.712\n",
      "Epoch 020 | Loss 0.2379 | ValAcc 0.754 | TestAcc 0.790\n",
      "Epoch 030 | Loss 0.0914 | ValAcc 0.758 | TestAcc 0.787\n",
      "Epoch 040 | Loss 0.0478 | ValAcc 0.762 | TestAcc 0.789\n",
      "Epoch 050 | Loss 0.0415 | ValAcc 0.762 | TestAcc 0.785\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "dataset = Planetoid(root=\"data/Planetoid\", name=\"Cora\")\n",
    "data = dataset[0]   # un grafo con .x (features), .edge_index (aristas), .y (labels)\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cpu\")        # \"cuda\" si luego usas GPU\n",
    "model = GCN(dataset.num_node_features, 16, dataset.num_classes).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        acc = (pred[mask] == data.y[mask]).sum() / mask.sum()\n",
    "        accs.append(acc.item())\n",
    "    return accs\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss {loss:.4f} | ValAcc {val_acc:.3f} | TestAcc {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb6810b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4945, 84)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "df['VISDATE'] = pd.to_datetime(df['VISDATE'], errors='coerce')\n",
    "\n",
    "df_sorted = df.sort_values(by=['RID','VISDATE'])\n",
    "\n",
    "df_unique = df_sorted.drop_duplicates(subset='RID', keep='first')\n",
    "\n",
    "df_unique.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "802b8978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (3977, 8) y: (3977,)\n",
      "Ejemplo columnas: ['PTDOBYY', 'PTEDUCAT', 'PTGENDER_2.0', 'PTMARRY_2.0', 'PTMARRY_3.0', 'PTMARRY_4.0', 'PTMARRY_5.0', 'PTMARRY_6.0']\n",
      "Rango y (año diagnóstico): 1980.0 → 9999.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "cols = [\"RID\", \"PTDOBYY\", \"PTEDUCAT\", \"PTGENDER\", \"PTMARRY\", \"PTADDX\"]\n",
    "for c in cols:\n",
    "    if c not in df.columns:\n",
    "        print(\"⚠️ Falta columna:\", c)\n",
    "df = df[[c for c in cols if c in df.columns]].copy()\n",
    "\n",
    "target_col = \"PTADDX\" if \"PTADDX\" in df.columns else \"PTADBEG\"\n",
    "df = df[~df[target_col].isna()].copy()\n",
    "\n",
    "X = df[[\"PTDOBYY\", \"PTEDUCAT\", \"PTGENDER\", \"PTMARRY\"]].copy()\n",
    "\n",
    "for num_col in [\"PTDOBYY\", \"PTEDUCAT\"]:\n",
    "    X[num_col] = pd.to_numeric(X[num_col], errors=\"coerce\")\n",
    "X[\"PTEDUCAT\"] = X[\"PTEDUCAT\"].fillna(X[\"PTEDUCAT\"].median())\n",
    "X[\"PTDOBYY\"] = X[\"PTDOBYY\"].fillna(X[\"PTDOBYY\"].median())\n",
    "\n",
    "X = pd.get_dummies(X, columns=[\"PTGENDER\", \"PTMARRY\"], drop_first=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[[\"PTDOBYY\",\"PTEDUCAT\"]] = scaler.fit_transform(X[[\"PTDOBYY\",\"PTEDUCAT\"]])\n",
    "\n",
    "y = pd.to_numeric(df[target_col], errors=\"coerce\").astype(float)\n",
    "mask_valid = ~y.isna()\n",
    "X = X.loc[mask_valid].reset_index(drop=True)\n",
    "y = y.loc[mask_valid].reset_index(drop=True)\n",
    "\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)\n",
    "print(\"Ejemplo columnas:\", list(X.columns)[:10])\n",
    "print(\"Rango y (año diagnóstico):\", y.min(), \"→\", y.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc8cde87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTDOBYY         float64\n",
      "PTEDUCAT        float64\n",
      "PTGENDER_2.0       bool\n",
      "PTMARRY_2.0        bool\n",
      "PTMARRY_3.0        bool\n",
      "PTMARRY_4.0        bool\n",
      "PTMARRY_5.0        bool\n",
      "PTMARRY_6.0        bool\n",
      "dtype: object\n",
      "float64\n",
      "0 missing after coercion\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# ahora convertir a tensor\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m y_t = torch.tensor(y.values, dtype=torch.float)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Tensores\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "y = pd.to_numeric(y, errors='coerce').fillna(0)\n",
    "\n",
    "print(X.dtypes)\n",
    "print(y.dtypes)\n",
    "print(X.isna().sum().sum(), \"missing after coercion\")\n",
    "\n",
    "import torch\n",
    "x = torch.tensor(X.values, dtype=torch.float)\n",
    "y_t = torch.tensor(y.values, dtype=torch.float)\n",
    "\n",
    "\n",
    "x = torch.tensor(X.values, dtype=torch.float)\n",
    "y_t = torch.tensor(y.values, dtype=torch.float)\n",
    "\n",
    "k = 8\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1, metric=\"euclidean\").fit(X.values)\n",
    "_, idx = nbrs.kneighbors(X.values)\n",
    "src, dst = [], []\n",
    "for i in range(idx.shape[0]):\n",
    "    for j in idx[i, 1:]:\n",
    "        src.append(i); dst.append(j)\n",
    "edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "\n",
    "n = len(y_t)\n",
    "perm = torch.randperm(n)\n",
    "tr_n, va_n = int(0.7*n), int(0.15*n)\n",
    "train_mask = torch.zeros(n, dtype=torch.bool); train_mask[perm[:tr_n]] = True\n",
    "val_mask   = torch.zeros(n, dtype=torch.bool); val_mask[perm[tr_n:tr_n+va_n]] = True\n",
    "test_mask  = torch.zeros(n, dtype=torch.bool); test_mask[perm[tr_n+va_n:]] = True\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=32, p=0.3):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.p = p\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.p, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = GNNRegressor(data.num_node_features).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for ep in range(1, 61):\n",
    "    model.train(); opt.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward(); opt.step()\n",
    "    if ep % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            v = loss_fn(out[data.val_mask], data.y[data.val_mask]).item()\n",
    "            t = loss_fn(out[data.test_mask], data.y[data.test_mask]).item()\n",
    "        print(f\"ep {ep:03d} | train {loss.item():.3f} | val {v:.3f} | test {t:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80d3fdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> X: (3977, 8)  y: (3977,)\n",
      "Tipos de X:\n",
      "PTDOBYY         float64\n",
      "PTEDUCAT        float64\n",
      "PTGENDER_2.0    float64\n",
      "PTMARRY_2.0     float64\n",
      "PTMARRY_3.0     float64\n",
      "PTMARRY_4.0     float64\n",
      "PTMARRY_5.0     float64\n",
      "PTMARRY_6.0     float64\n",
      "dtype: object\n",
      "Data(x=[3977, 8], edge_index=[2, 63632], y=[3977], train_mask=[3977], val_mask=[3977], test_mask=[3977])\n",
      "ep 010 | train 87087480.000 | val 88189488.000 | test 86944544.000\n",
      "ep 020 | train 87023864.000 | val 88120576.000 | test 86873456.000\n",
      "ep 030 | train 86913832.000 | val 88003952.000 | test 86752720.000\n",
      "ep 040 | train 86746304.000 | val 87825568.000 | test 86567800.000\n",
      "ep 050 | train 86506264.000 | val 87575352.000 | test 86308248.000\n",
      "ep 060 | train 86191952.000 | val 87245608.000 | test 85966152.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "base_cols = [\"RID\", \"PTDOBYY\", \"PTEDUCAT\", \"PTGENDER\", \"PTMARRY\"]\n",
    "target_col = \"PTADDX\" if \"PTADDX\" in df.columns else (\"PTADBEG\" if \"PTADBEG\" in df.columns else None)\n",
    "\n",
    "missing = [c for c in base_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas en el CSV: {missing}\")\n",
    "\n",
    "if target_col is None:\n",
    "    raise ValueError(\"No encuentro PTADDX ni PTADBEG en el CSV para usar como target.\")\n",
    "\n",
    "df = df[base_cols + [target_col]].copy()\n",
    "\n",
    "df[target_col] = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "df = df.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "\n",
    "for c in [\"PTDOBYY\", \"PTEDUCAT\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df[\"PTDOBYY\"] = df[\"PTDOBYY\"].fillna(df[\"PTDOBYY\"].median())\n",
    "df[\"PTEDUCAT\"] = df[\"PTEDUCAT\"].fillna(df[\"PTEDUCAT\"].median())\n",
    "\n",
    "X = df[[\"PTDOBYY\", \"PTEDUCAT\", \"PTGENDER\", \"PTMARRY\"]].copy()\n",
    "X = pd.get_dummies(X, columns=[\"PTGENDER\", \"PTMARRY\"], drop_first=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[[\"PTDOBYY\", \"PTEDUCAT\"]] = scaler.fit_transform(X[[\"PTDOBYY\", \"PTEDUCAT\"]])\n",
    "\n",
    "X = X.astype(float)\n",
    "\n",
    "y = df[target_col].astype(float)\n",
    "\n",
    "print(\"Shapes -> X:\", X.shape, \" y:\", y.shape)\n",
    "print(\"Tipos de X:\")\n",
    "print(X.dtypes)\n",
    "\n",
    "k = 8  # ajústalo si quieres más/menos aristas por nodo\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1, metric=\"euclidean\")\n",
    "nbrs.fit(X.values)\n",
    "_, idx = nbrs.kneighbors(X.values)\n",
    "\n",
    "src, dst = [], []\n",
    "for i in range(idx.shape[0]):\n",
    "    for j in idx[i, 1:]:  # saltamos self\n",
    "        src.append(i); dst.append(j)\n",
    "edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "\n",
    "x = torch.tensor(X.values, dtype=torch.float)\n",
    "y_t = torch.tensor(y.values, dtype=torch.float)\n",
    "\n",
    "n = len(y_t)\n",
    "perm = torch.randperm(n)\n",
    "tr_n, va_n = int(0.7*n), int(0.15*n)\n",
    "\n",
    "train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "val_mask   = torch.zeros(n, dtype=torch.bool)\n",
    "test_mask  = torch.zeros(n, dtype=torch.bool)\n",
    "\n",
    "train_mask[perm[:tr_n]] = True\n",
    "val_mask[perm[tr_n:tr_n+va_n]] = True\n",
    "test_mask[perm[tr_n+va_n:]] = True\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "print(data)\n",
    "\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cpu\")  # si luego usas GPU: torch.device(\"cuda\")\n",
    "model = GNNRegressor(in_ch=data.num_node_features, hid=64, dropout=0.3).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def evaluate(split=\"val\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        if split == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif split == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            mask = data.train_mask\n",
    "        loss = loss_fn(out[mask], data.y[mask]).item()\n",
    "    return loss\n",
    "\n",
    "for ep in range(1, 61):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        val_loss = evaluate(\"val\")\n",
    "        test_loss = evaluate(\"test\")\n",
    "        print(f\"ep {ep:03d} | train {loss.item():.3f} | val {val_loss:.3f} | test {test_loss:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2caaaf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTGENDER (normalizado):\n",
      "PTGENDER\n",
      "female    2127\n",
      "male      1850\n",
      "Name: count, dtype: int64\n",
      "\n",
      "PTMARRY  (normalizado):\n",
      "PTMARRY\n",
      "married                 2742\n",
      "divorced                 496\n",
      "widowed                  431\n",
      "never_married            259\n",
      "domestic_partnership      30\n",
      "NaN                       19\n",
      "Name: count, dtype: int64\n",
      "Shapes -> X: (3977, 9)  y: (3977,)\n",
      "Tipos de X:\n",
      "PTDOBYY                         float64\n",
      "PTEDUCAT                        float64\n",
      "PTGENDER_female                 float64\n",
      "PTGENDER_male                   float64\n",
      "PTMARRY_divorced                float64\n",
      "PTMARRY_domestic_partnership    float64\n",
      "PTMARRY_married                 float64\n",
      "PTMARRY_never_married           float64\n",
      "PTMARRY_widowed                 float64\n",
      "dtype: object\n",
      "Data(x=[3977, 9], edge_index=[2, 63632], y=[3977], train_mask=[3977], val_mask=[3977], test_mask=[3977])\n",
      "ep 010 | train 87065392.000 | val 88166616.000 | test 86922920.000\n",
      "ep 020 | train 86969400.000 | val 88066136.000 | test 86822432.000\n",
      "ep 030 | train 86811112.000 | val 87903200.000 | test 86659360.000\n",
      "ep 040 | train 86572616.000 | val 87661392.000 | test 86417456.000\n",
      "ep 050 | train 86242184.000 | val 87327992.000 | test 86083880.000\n",
      "ep 060 | train 85807408.000 | val 86893144.000 | test 85648440.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"  # <-- ajusta la ruta si procede\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "base_cols = [\"RID\", \"PTDOBYY\", \"PTEDUCAT\", \"PTGENDER\", \"PTMARRY\"]\n",
    "target_col = \"PTADDX\" if \"PTADDX\" in df.columns else (\"PTADBEG\" if \"PTADBEG\" in df.columns else None)\n",
    "\n",
    "missing = [c for c in base_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas en el CSV: {missing}\")\n",
    "\n",
    "if target_col is None:\n",
    "    raise ValueError(\"No encuentro PTADDX ni PTADBEG en el CSV para usar como target.\")\n",
    "\n",
    "df = df[base_cols + [target_col]].copy()\n",
    "\n",
    "df[target_col] = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "df = df.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def norm_codes_to_labels(s: pd.Series, mapping: dict) -> pd.Series:\n",
    "    out = s.astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    out = out.map(mapping)\n",
    "    return out\n",
    "\n",
    "gender_map = {\n",
    "    \"1\": \"male\",\n",
    "    \"2\": \"female\",\n",
    "    \"male\": \"male\",\n",
    "    \"female\": \"female\",\n",
    "    \"m\": \"male\",\n",
    "    \"f\": \"female\"\n",
    "}\n",
    "\n",
    "marry_map = {\n",
    "    \"1\": \"married\",\n",
    "    \"2\": \"widowed\",\n",
    "    \"3\": \"divorced\",\n",
    "    \"4\": \"never_married\",\n",
    "    \"6\": \"domestic_partnership\"\n",
    "}\n",
    "\n",
    "if \"PTGENDER\" in df.columns:\n",
    "    df[\"PTGENDER\"] = norm_codes_to_labels(df[\"PTGENDER\"], gender_map)\n",
    "\n",
    "if \"PTMARRY\" in df.columns:\n",
    "    df[\"PTMARRY\"] = norm_codes_to_labels(df[\"PTMARRY\"], marry_map)\n",
    "\n",
    "print(\"PTGENDER (normalizado):\")\n",
    "print(df[\"PTGENDER\"].value_counts(dropna=False))\n",
    "print(\"\\nPTMARRY  (normalizado):\")\n",
    "print(df[\"PTMARRY\"].value_counts(dropna=False))\n",
    "\n",
    "for c in [\"PTDOBYY\", \"PTEDUCAT\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df[\"PTDOBYY\"] = df[\"PTDOBYY\"].fillna(df[\"PTDOBYY\"].median())\n",
    "df[\"PTEDUCAT\"] = df[\"PTEDUCAT\"].fillna(df[\"PTEDUCAT\"].median())\n",
    "\n",
    "num_cols = [\"PTDOBYY\", \"PTEDUCAT\"]\n",
    "cat_cols = [\"PTGENDER\", \"PTMARRY\"]\n",
    "\n",
    "X_parts = []\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_num = pd.DataFrame(\n",
    "    scaler.fit_transform(df[num_cols]),\n",
    "    columns=num_cols,\n",
    "    index=df.index\n",
    ")\n",
    "X_parts.append(X_num)\n",
    "\n",
    "X_cat = pd.get_dummies(\n",
    "    df[cat_cols],\n",
    "    prefix=cat_cols,\n",
    "    drop_first=False,\n",
    "    dtype=float\n",
    ")\n",
    "X_parts.append(X_cat)\n",
    "\n",
    "X = pd.concat(X_parts, axis=1).astype(float)\n",
    "\n",
    "y = df[target_col].astype(float)\n",
    "\n",
    "print(\"Shapes -> X:\", X.shape, \" y:\", y.shape)\n",
    "print(\"Tipos de X:\")\n",
    "print(X.dtypes)\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "k = 8\n",
    "k = min(k, max(1, n_samples - 1))\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1, metric=\"euclidean\")\n",
    "nbrs.fit(X.values)\n",
    "_, idx = nbrs.kneighbors(X.values)\n",
    "\n",
    "src, dst = [], []\n",
    "for i in range(idx.shape[0]):\n",
    "    for j in idx[i, 1:]:  # saltamos self\n",
    "        src.append(i); dst.append(j)\n",
    "edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "\n",
    "x = torch.tensor(X.values, dtype=torch.float)\n",
    "y_t = torch.tensor(y.values, dtype=torch.float)\n",
    "\n",
    "n = len(y_t)\n",
    "perm = torch.randperm(n)\n",
    "tr_n, va_n = int(0.7*n), int(0.15*n)\n",
    "\n",
    "train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "val_mask   = torch.zeros(n, dtype=torch.bool)\n",
    "test_mask  = torch.zeros(n, dtype=torch.bool)\n",
    "\n",
    "train_mask[perm[:tr_n]] = True\n",
    "val_mask[perm[tr_n:tr_n+va_n]] = True\n",
    "test_mask[perm[tr_n+va_n:]] = True\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "print(data)\n",
    "\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cpu\")  # si luego usas GPU: torch.device(\"cuda\")\n",
    "model = GNNRegressor(in_ch=data.num_node_features, hid=64, dropout=0.3).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def evaluate(split=\"val\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        if split == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif split == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            mask = data.train_mask\n",
    "        loss = loss_fn(out[mask], data.y[mask]).item()\n",
    "    return loss\n",
    "\n",
    "for ep in range(1, 61):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        val_loss = evaluate(\"val\")\n",
    "        test_loss = evaluate(\"test\")\n",
    "        print(f\"ep {ep:03d} | train {loss.item():.3f} | val {val_loss:.3f} | test {test_loss:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b445e9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pacientes con YEAR_ONSET válido: 4876 / 4945\n",
      "PTGENDER (normalizado):\n",
      "PTGENDER\n",
      "female    2479\n",
      "male      2397\n",
      "Name: count, dtype: int64\n",
      "\n",
      "PTMARRY  (normalizado):\n",
      "PTMARRY\n",
      "married                 3467\n",
      "divorced                 555\n",
      "widowed                  498\n",
      "never_married            298\n",
      "NaN                       38\n",
      "domestic_partnership      20\n",
      "Name: count, dtype: int64\n",
      "Shapes -> X: (4876, 9)  y: (4876,)\n",
      "Tipos de X:\n",
      "PTDOBYY                         float64\n",
      "PTEDUCAT                        float64\n",
      "PTGENDER_female                 float64\n",
      "PTGENDER_male                   float64\n",
      "PTMARRY_divorced                float64\n",
      "PTMARRY_domestic_partnership    float64\n",
      "PTMARRY_married                 float64\n",
      "PTMARRY_never_married           float64\n",
      "PTMARRY_widowed                 float64\n",
      "dtype: object\n",
      "Data(x=[4876, 9], edge_index=[2, 78016], y=[4876], train_mask=[4876], val_mask=[4876], test_mask=[4876])\n",
      "ep 010 | train 30461614.000 | val 31072988.000 | test 32362452.000\n",
      "ep 020 | train 30424528.000 | val 31032738.000 | test 32321900.000\n",
      "ep 030 | train 30360176.000 | val 30963888.000 | test 32252528.000\n",
      "ep 040 | train 30259704.000 | val 30858154.000 | test 32145956.000\n",
      "ep 050 | train 30116584.000 | val 30706850.000 | test 31993518.000\n",
      "ep 060 | train 29924280.000 | val 30503120.000 | test 31788228.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"  # <-- ajusta la ruta si procede\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "base_cols = [\"RID\", \"PTDOBYY\", \"PTEDUCAT\", \"PTGENDER\", \"PTMARRY\"]\n",
    "onset_cols = [c for c in [\"PTCOGBEG\", \"PTADBEG\", \"PTADDX\"] if c in df.columns]\n",
    "\n",
    "missing = [c for c in base_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas en el CSV: {missing}\")\n",
    "\n",
    "if not onset_cols:\n",
    "    raise ValueError(\"No encuentro PTCOGBEG/PTADBEG/PTADDX en el CSV.\")\n",
    "\n",
    "df = df[base_cols + onset_cols].copy()\n",
    "\n",
    "for c in onset_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "def row_min_nonnull(row):\n",
    "    vals = [row[c] for c in onset_cols if pd.notna(row[c])]\n",
    "    return min(vals) if vals else np.nan\n",
    "\n",
    "df[\"YEAR_ONSET\"] = df.apply(row_min_nonnull, axis=1)\n",
    "\n",
    "def first_notnull(s):\n",
    "    return next((v for v in s if pd.notna(v)), np.nan)\n",
    "\n",
    "agg_dict = {\n",
    "    \"YEAR_ONSET\": \"min\",\n",
    "    \"PTDOBYY\": \"median\",\n",
    "    \"PTEDUCAT\": \"median\",\n",
    "    \"PTGENDER\": first_notnull,\n",
    "    \"PTMARRY\": first_notnull,\n",
    "}\n",
    "df = df.groupby(\"RID\", as_index=False).agg(agg_dict)\n",
    "\n",
    "df[\"YEAR_ONSET\"] = pd.to_numeric(df[\"YEAR_ONSET\"], errors=\"coerce\")\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[\"YEAR_ONSET\"]).reset_index(drop=True)\n",
    "print(f\"Pacientes con YEAR_ONSET válido: {len(df)} / {before}\")\n",
    "\n",
    "\n",
    "def norm_codes_to_labels(s: pd.Series, mapping: dict) -> pd.Series:\n",
    "    out = s.astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    out = out.map(mapping)\n",
    "    return out\n",
    "\n",
    "gender_map = {\n",
    "    \"1\": \"male\",\n",
    "    \"2\": \"female\",\n",
    "    \"male\": \"male\",\n",
    "    \"female\": \"female\",\n",
    "    \"m\": \"male\",\n",
    "    \"f\": \"female\"\n",
    "}\n",
    "\n",
    "marry_map = {\n",
    "    \"1\": \"married\",\n",
    "    \"2\": \"widowed\",\n",
    "    \"3\": \"divorced\",\n",
    "    \"4\": \"never_married\",\n",
    "    \"6\": \"domestic_partnership\"\n",
    "}\n",
    "\n",
    "if \"PTGENDER\" in df.columns:\n",
    "    df[\"PTGENDER\"] = norm_codes_to_labels(df[\"PTGENDER\"], gender_map)\n",
    "\n",
    "if \"PTMARRY\" in df.columns:\n",
    "    df[\"PTMARRY\"] = norm_codes_to_labels(df[\"PTMARRY\"], marry_map)\n",
    "\n",
    "print(\"PTGENDER (normalizado):\")\n",
    "print(df[\"PTGENDER\"].value_counts(dropna=False))\n",
    "print(\"\\nPTMARRY  (normalizado):\")\n",
    "print(df[\"PTMARRY\"].value_counts(dropna=False))\n",
    "\n",
    "for c in [\"PTDOBYY\", \"PTEDUCAT\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df[\"PTDOBYY\"] = df[\"PTDOBYY\"].fillna(df[\"PTDOBYY\"].median())\n",
    "df[\"PTEDUCAT\"] = df[\"PTEDUCAT\"].fillna(df[\"PTEDUCAT\"].median())\n",
    "\n",
    "num_cols = [\"PTDOBYY\", \"PTEDUCAT\"]\n",
    "cat_cols = [\"PTGENDER\", \"PTMARRY\"]\n",
    "\n",
    "X_parts = []\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_num = pd.DataFrame(\n",
    "    scaler.fit_transform(df[num_cols]),\n",
    "    columns=num_cols,\n",
    "    index=df.index\n",
    ")\n",
    "X_parts.append(X_num)\n",
    "\n",
    "X_cat = pd.get_dummies(\n",
    "    df[cat_cols],\n",
    "    prefix=cat_cols,\n",
    "    drop_first=False,\n",
    "    dtype=float\n",
    ")\n",
    "X_parts.append(X_cat)\n",
    "\n",
    "X = pd.concat(X_parts, axis=1).astype(float)\n",
    "\n",
    "y = df[\"YEAR_ONSET\"].astype(float)\n",
    "\n",
    "print(\"Shapes -> X:\", X.shape, \" y:\", y.shape)\n",
    "print(\"Tipos de X:\")\n",
    "print(X.dtypes)\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "k = 8\n",
    "k = min(k, max(1, n_samples - 1))\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1, metric=\"euclidean\")\n",
    "nbrs.fit(X.values)\n",
    "_, idx = nbrs.kneighbors(X.values)\n",
    "\n",
    "src, dst = [], []\n",
    "for i in range(idx.shape[0]):\n",
    "    for j in idx[i, 1:]:  # saltamos self\n",
    "        src.append(i); dst.append(j)\n",
    "edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "\n",
    "x = torch.tensor(X.values, dtype=torch.float)\n",
    "y_t = torch.tensor(y.values, dtype=torch.float)\n",
    "\n",
    "n = len(y_t)\n",
    "perm = torch.randperm(n)\n",
    "tr_n, va_n = int(0.7*n), int(0.15*n)\n",
    "\n",
    "train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "val_mask   = torch.zeros(n, dtype=torch.bool)\n",
    "test_mask  = torch.zeros(n, dtype=torch.bool)\n",
    "\n",
    "train_mask[perm[:tr_n]] = True\n",
    "val_mask[perm[tr_n:tr_n+va_n]] = True\n",
    "test_mask[perm[tr_n+va_n:]] = True\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "print(data)\n",
    "\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cpu\")  # si luego usas GPU: torch.device(\"cuda\")\n",
    "model = GNNRegressor(in_ch=data.num_node_features, hid=64, dropout=0.3).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def evaluate(split=\"val\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        if split == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif split == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            mask = data.train_mask\n",
    "        loss = loss_fn(out[mask], data.y[mask]).item()\n",
    "    return loss\n",
    "\n",
    "for ep in range(1, 61):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        val_loss = evaluate(\"val\")\n",
    "        test_loss = evaluate(\"test\")\n",
    "        print(f\"ep {ep:03d} | train {loss.item():.3f} | val {val_loss:.3f} | test {test_loss:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02d20911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Modo VISITAS (usando VISDATE como fecha de visita)\n",
      "Nodos: 6210 | etiquetados: 1942 | edges: 72424\n",
      "Data(x=[6210, 9], edge_index=[2, 72424], y=[6210], train_mask=[6210], val_mask=[6210], test_mask=[6210])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guzmam19\\AppData\\Local\\Temp\\ipykernel_7744\\1342829536.py:299: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:836.)\n",
      "  print(f\"ep {ep:03d} | train {float(loss):.3f} | val {evaluate('val'):.3f} | test {evaluate('test'):.3f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 010 | train 60674200.000 | val 61875528.000 | test 61226168.000\n",
      "ep 020 | train 60587500.000 | val 61782024.000 | test 61136012.000\n",
      "ep 030 | train 60441728.000 | val 61627844.000 | test 60987496.000\n",
      "ep 040 | train 60221352.000 | val 61396260.000 | test 60765316.000\n",
      "ep 050 | train 59914252.000 | val 61075724.000 | test 60458104.000\n",
      "ep 060 | train 59512608.000 | val 60654476.000 | test 60054388.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "def norm_codes_to_labels(s: pd.Series, mapping: dict) -> pd.Series:\n",
    "    out = s.astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    out = out.map(mapping)\n",
    "    return out\n",
    "\n",
    "gender_map = {\"1\":\"male\",\"2\":\"female\",\"male\":\"male\",\"female\":\"female\",\"m\":\"male\",\"f\":\"female\"}\n",
    "marry_map  = {\"1\":\"married\",\"2\":\"widowed\",\"3\":\"divorced\",\"4\":\"never_married\",\"6\":\"domestic_partnership\"}\n",
    "\n",
    "onset_cols = [c for c in [\"PTCOGBEG\",\"PTADBEG\",\"PTADDX\"] if c in df.columns]\n",
    "for c in onset_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "def row_min_nonnull(row):\n",
    "    vals = [row[c] for c in onset_cols if pd.notna(row[c])]\n",
    "    return min(vals) if vals else np.nan\n",
    "\n",
    "df[\"YEAR_ONSET\"] = df.apply(row_min_nonnull, axis=1) if onset_cols else np.nan\n",
    "df[\"YEAR_ONSET\"] = pd.to_numeric(df[\"YEAR_ONSET\"], errors=\"coerce\")\n",
    "\n",
    "for c in [\"PTDOBYY\",\"PTEDUCAT\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "if \"PTGENDER\" in df.columns:\n",
    "    df[\"PTGENDER\"] = norm_codes_to_labels(df[\"PTGENDER\"], gender_map)\n",
    "if \"PTMARRY\" in df.columns:\n",
    "    df[\"PTMARRY\"]  = norm_codes_to_labels(df[\"PTMARRY\"], marry_map)\n",
    "\n",
    "date_col = \"EXAMDATE\" if \"EXAMDATE\" in df.columns else (\"VISDATE\" if \"VISDATE\" in df.columns else None)\n",
    "has_visits = (\"RID\" in df.columns) and (date_col is not None)\n",
    "\n",
    "if has_visits:\n",
    "    print(f\">> Modo VISITAS (usando {date_col} como fecha de visita)\")\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    df[\"EXAM_YEAR\"] = df[date_col].dt.year\n",
    "\n",
    "    df[\"AGE_AT_VISIT\"] = np.where(\n",
    "        df[\"EXAM_YEAR\"].notna() & df[\"PTDOBYY\"].notna(),\n",
    "        df[\"EXAM_YEAR\"] - df[\"PTDOBYY\"],\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    df[\"YEARS_TO_ONSET\"] = np.where(\n",
    "        df[\"YEAR_ONSET\"].notna() & df[\"EXAM_YEAR\"].notna(),\n",
    "        df[\"YEAR_ONSET\"] - df[\"EXAM_YEAR\"],\n",
    "        np.nan\n",
    "    )\n",
    "    df[\"HAS_LABEL\"] = df[\"YEARS_TO_ONSET\"].notna() & (df[\"YEARS_TO_ONSET\"] >= 0)\n",
    "\n",
    "    num_cols = [c for c in [\"AGE_AT_VISIT\",\"PTEDUCAT\"] if c in df.columns]\n",
    "    cat_cols = [c for c in [\"PTGENDER\",\"PTMARRY\"] if c in df.columns]\n",
    "\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(df[c].median())\n",
    "\n",
    "    parts = []\n",
    "    if num_cols:\n",
    "        scaler = StandardScaler()\n",
    "        X_num = pd.DataFrame(\n",
    "            scaler.fit_transform(df[num_cols]),\n",
    "            columns=num_cols, index=df.index\n",
    "        )\n",
    "        parts.append(X_num)\n",
    "    if cat_cols:\n",
    "        X_cat = pd.get_dummies(df[cat_cols], prefix=cat_cols, drop_first=False, dtype=float)\n",
    "        parts.append(X_cat)\n",
    "    if not parts:\n",
    "        raise ValueError(\"No hay columnas de features seleccionadas en modo visitas.\")\n",
    "\n",
    "    X = pd.concat(parts, axis=1).astype(float)\n",
    "\n",
    "    y_full = df[\"YEARS_TO_ONSET\"].astype(float)\n",
    "    y_t = torch.tensor(y_full.fillna(0).values, dtype=torch.float32)\n",
    "    label_mask = torch.tensor(df[\"HAS_LABEL\"].fillna(False).values, dtype=torch.bool)\n",
    "\n",
    "    X_clean = X.replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "    n_samples = X_clean.shape[0]\n",
    "    k = min(8, max(1, n_samples - 1))\n",
    "    n_neighbors = min(n_samples, k + 1)\n",
    "\n",
    "    if n_samples >= 2:\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "        nbrs.fit(X_clean.values)\n",
    "        _, idx = nbrs.kneighbors(X_clean.values)\n",
    "        src_knn, dst_knn = [], []\n",
    "        for i in range(idx.shape[0]):\n",
    "            for j in idx[i, 1:]:\n",
    "                src_knn.append(i); dst_knn.append(j)\n",
    "        edge_knn = torch.tensor([src_knn, dst_knn], dtype=torch.long)\n",
    "    else:\n",
    "        edge_knn = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    if \"RID\" not in df.columns:\n",
    "        raise ValueError(\"PTDEMOG no tiene RID; necesario para aristas temporales.\")\n",
    "    tmp = df.reset_index()[[\"index\",\"RID\", date_col]].dropna(subset=[date_col])\n",
    "    tmp = tmp.sort_values([\"RID\", date_col])\n",
    "    src_tmp, dst_tmp = [], []\n",
    "    for rid, g in tmp.groupby(\"RID\"):\n",
    "        ids = g[\"index\"].tolist()\n",
    "        for a, b in zip(ids[:-1], ids[1:]):\n",
    "            src_tmp.append(a); dst_tmp.append(b)\n",
    "    edge_tmp = torch.tensor([src_tmp, dst_tmp], dtype=torch.long) if src_tmp else torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "    def undirected(e): \n",
    "        return torch.cat([e, e.flip(0)], dim=1) if e.numel() else e\n",
    "\n",
    "    edges = []\n",
    "    if edge_knn.numel(): edges.append(undirected(edge_knn))\n",
    "    if edge_tmp.numel(): edges.append(undirected(edge_tmp))\n",
    "    edge_index = torch.cat(edges, dim=1) if edges else torch.empty((2,0), dtype=torch.long)\n",
    "    if edge_index.numel():\n",
    "        edge_index = torch.unique(edge_index, dim=1)\n",
    "\n",
    "else:\n",
    "    print(\">> Modo PACIENTE (fallback: no hay EXAMDATE/VISDATE)\")\n",
    "    if \"RID\" not in df.columns:\n",
    "        raise ValueError(\"PTDEMOG no tiene RID.\")\n",
    "    keep_cols = [c for c in [\"RID\",\"PTDOBYY\",\"PTEDUCAT\",\"PTGENDER\",\"PTMARRY\",\"YEAR_ONSET\"] if c in df.columns]\n",
    "    df = df[keep_cols].copy()\n",
    "\n",
    "    def first_notnull(s):\n",
    "        return next((v for v in s if pd.notna(v)), np.nan)\n",
    "    agg = {\n",
    "        \"PTDOBYY\":\"median\",\n",
    "        \"PTEDUCAT\":\"median\",\n",
    "        \"PTGENDER\":first_notnull,\n",
    "        \"PTMARRY\":first_notnull,\n",
    "        \"YEAR_ONSET\":\"min\"\n",
    "    }\n",
    "    df = df.groupby(\"RID\", as_index=False).agg({k:v for k,v in agg.items() if k in df.columns})\n",
    "\n",
    "    num_cols = [c for c in [\"PTDOBYY\",\"PTEDUCAT\"] if c in df.columns]\n",
    "    cat_cols = [c for c in [\"PTGENDER\",\"PTMARRY\"] if c in df.columns]\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(df[c].median())\n",
    "\n",
    "    parts = []\n",
    "    if num_cols:\n",
    "        scaler = StandardScaler()\n",
    "        X_num = pd.DataFrame(\n",
    "            scaler.fit_transform(df[num_cols]),\n",
    "            columns=num_cols, index=df.index\n",
    "        )\n",
    "        parts.append(X_num)\n",
    "    if cat_cols:\n",
    "        X_cat = pd.get_dummies(df[cat_cols], prefix=cat_cols, drop_first=False, dtype=float)\n",
    "        parts.append(X_cat)\n",
    "    if not parts:\n",
    "        raise ValueError(\"No hay columnas de features en modo paciente.\")\n",
    "    X = pd.concat(parts, axis=1).astype(float)\n",
    "\n",
    "    y_full = df[\"YEAR_ONSET\"].astype(float) if \"YEAR_ONSET\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    y_t = torch.tensor(y_full.fillna(0).values, dtype=torch.float32)\n",
    "    label_mask = torch.tensor(y_full.notna().values, dtype=torch.bool)\n",
    "\n",
    "    X_clean = X.replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "    n_samples = X_clean.shape[0]\n",
    "    k = min(8, max(1, n_samples - 1))\n",
    "    n_neighbors = min(n_samples, k + 1)\n",
    "    if n_samples >= 2:\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "        nbrs.fit(X_clean.values)\n",
    "        _, idx = nbrs.kneighbors(X_clean.values)\n",
    "        src, dst = [], []\n",
    "        for i in range(idx.shape[0]):\n",
    "            for j in idx[i, 1:]:\n",
    "                src.append(i); dst.append(j)\n",
    "        edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "        edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "x = torch.tensor(X_clean.values, dtype=torch.float32)\n",
    "n = x.size(0)\n",
    "\n",
    "labeled_idx = torch.nonzero(label_mask, as_tuple=False).view(-1)\n",
    "n_lab = labeled_idx.numel()\n",
    "print(f\"Nodos: {n} | etiquetados: {n_lab} | edges: {edge_index.size(1)}\")\n",
    "\n",
    "if n_lab > 0:\n",
    "    perm = labeled_idx[torch.randperm(n_lab)]\n",
    "    tr_n = max(1, int(0.7*n_lab)) if n_lab >= 3 else max(1, n_lab)\n",
    "    va_n = max(0, int(0.15*n_lab)) if n_lab >= 7 else 0\n",
    "    if tr_n + va_n > max(0, n_lab-1):\n",
    "        va_n = max(0, n_lab - 1 - tr_n)\n",
    "\n",
    "    train_idx = perm[:tr_n]\n",
    "    val_idx   = perm[tr_n:tr_n+va_n]\n",
    "    test_idx  = perm[tr_n+va_n:]\n",
    "else:\n",
    "    train_idx = val_idx = test_idx = torch.tensor([], dtype=torch.long)\n",
    "\n",
    "train_mask = torch.zeros(n, dtype=torch.bool); train_mask[train_idx] = True\n",
    "val_mask   = torch.zeros(n, dtype=torch.bool); val_mask[val_idx]     = True\n",
    "test_mask  = torch.zeros(n, dtype=torch.bool); test_mask[test_idx]   = True\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "print(data)\n",
    "\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = GNNRegressor(in_ch=data.num_node_features, hid=64, dropout=0.3).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def evaluate(split=\"val\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        mask = data.train_mask if split==\"train\" else (data.val_mask if split==\"val\" else data.test_mask)\n",
    "        if not mask.any():\n",
    "            return float(\"nan\")\n",
    "        return loss_fn(out[mask], data.y[mask]).item()\n",
    "\n",
    "for ep in range(1, 61):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    if data.train_mask.any():\n",
    "        loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            raise RuntimeError(\"Loss NaN/Inf; revisa X/y/fechas.\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    else:\n",
    "        loss = torch.tensor(float(\"nan\"))\n",
    "    if ep % 10 == 0:\n",
    "        print(f\"ep {ep:03d} | train {float(loss):.3f} | val {evaluate('val'):.3f} | test {evaluate('test'):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f207013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Modo VISITAS (usando VISDATE como fecha de visita)\n",
      "Nodos: 6210 | etiquetados: 82 | edges: 72424\n",
      "Data(x=[6210, 9], edge_index=[2, 72424], y=[6210], train_mask=[6210], val_mask=[6210], test_mask=[6210])\n",
      "ep 010 | train_loss(MSE_scaled) 0.0090 | TR MAE 0.006y RMSE 0.007y | VAL MAE 0.092y RMSE 0.289y | TEST MAE 0.009y RMSE 0.013y\n",
      "ep 020 | train_loss(MSE_scaled) 0.0024 | TR MAE 0.002y RMSE 0.003y | VAL MAE 0.087y RMSE 0.289y | TEST MAE 0.004y RMSE 0.005y\n",
      "ep 030 | train_loss(MSE_scaled) 0.0022 | TR MAE 0.002y RMSE 0.003y | VAL MAE 0.086y RMSE 0.289y | TEST MAE 0.003y RMSE 0.004y\n",
      "ep 040 | train_loss(MSE_scaled) 0.0012 | TR MAE 0.002y RMSE 0.003y | VAL MAE 0.087y RMSE 0.289y | TEST MAE 0.005y RMSE 0.007y\n",
      "ep 050 | train_loss(MSE_scaled) 0.0014 | TR MAE 0.001y RMSE 0.002y | VAL MAE 0.085y RMSE 0.289y | TEST MAE 0.002y RMSE 0.003y\n",
      "ep 060 | train_loss(MSE_scaled) 0.0009 | TR MAE 0.001y RMSE 0.002y | VAL MAE 0.085y RMSE 0.288y | TEST MAE 0.003y RMSE 0.004y\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "def norm_codes_to_labels(s: pd.Series, mapping: dict) -> pd.Series:\n",
    "    out = s.astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    out = out.map(mapping)\n",
    "    return out\n",
    "\n",
    "gender_map = {\"1\":\"male\",\"2\":\"female\",\"male\":\"male\",\"female\":\"female\",\"m\":\"male\",\"f\":\"female\"}\n",
    "marry_map  = {\"1\":\"married\",\"2\":\"widowed\",\"3\":\"divorced\",\"4\":\"never_married\",\"6\":\"domestic_partnership\"}\n",
    "\n",
    "def to_year(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s.where((s >= 1900) & (s <= 2100))\n",
    "    return s\n",
    "\n",
    "onset_cols = [c for c in [\"PTCOGBEG\",\"PTADBEG\",\"PTADDX\"] if c in df.columns]\n",
    "for c in onset_cols:\n",
    "    df[c] = to_year(df[c])\n",
    "\n",
    "def row_min_nonnull(row):\n",
    "    vals = [row[c] for c in onset_cols if pd.notna(row[c])]\n",
    "    return min(vals) if vals else np.nan\n",
    "\n",
    "df[\"YEAR_ONSET\"] = row_min_nonnull(df) if False else (\n",
    "    df.apply(row_min_nonnull, axis=1) if onset_cols else np.nan\n",
    ")\n",
    "df[\"YEAR_ONSET\"] = to_year(df[\"YEAR_ONSET\"])\n",
    "\n",
    "for c in [\"PTDOBYY\",\"PTEDUCAT\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "if \"PTGENDER\" in df.columns:\n",
    "    df[\"PTGENDER\"] = norm_codes_to_labels(df[\"PTGENDER\"], gender_map)\n",
    "if \"PTMARRY\" in df.columns:\n",
    "    df[\"PTMARRY\"]  = norm_codes_to_labels(df[\"PTMARRY\"], marry_map)\n",
    "\n",
    "date_col = \"EXAMDATE\" if \"EXAMDATE\" in df.columns else (\"VISDATE\" if \"VISDATE\" in df.columns else None)\n",
    "has_visits = (\"RID\" in df.columns) and (date_col is not None)\n",
    "\n",
    "if has_visits:\n",
    "    print(f\">> Modo VISITAS (usando {date_col} como fecha de visita)\")\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    df[\"EXAM_YEAR\"] = df[date_col].dt.year\n",
    "    df[\"EXAM_YEAR\"] = to_year(df[\"EXAM_YEAR\"])\n",
    "\n",
    "    df[\"AGE_AT_VISIT\"] = np.where(\n",
    "        df[\"EXAM_YEAR\"].notna() & df[\"PTDOBYY\"].notna(),\n",
    "        df[\"EXAM_YEAR\"] - df[\"PTDOBYY\"],\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    df[\"YEARS_TO_ONSET\"] = np.where(\n",
    "        df[\"YEAR_ONSET\"].notna() & df[\"EXAM_YEAR\"].notna(),\n",
    "        df[\"YEAR_ONSET\"] - df[\"EXAM_YEAR\"],\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    df.loc[(df[\"YEARS_TO_ONSET\"] < 0) & (df[\"YEAR_ONSET\"].notna()), \"YEARS_TO_ONSET\"] = np.nan\n",
    "    df.loc[df[\"YEARS_TO_ONSET\"] > 50, \"YEARS_TO_ONSET\"] = np.nan\n",
    "\n",
    "    df[\"HAS_LABEL\"] = df[\"YEARS_TO_ONSET\"].notna()\n",
    "\n",
    "    num_cols = [c for c in [\"AGE_AT_VISIT\",\"PTEDUCAT\"] if c in df.columns]\n",
    "    cat_cols = [c for c in [\"PTGENDER\",\"PTMARRY\"] if c in df.columns]\n",
    "\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(df[c].median())\n",
    "\n",
    "    parts = []\n",
    "    if num_cols:\n",
    "        scaler = StandardScaler()\n",
    "        X_num = pd.DataFrame(\n",
    "            scaler.fit_transform(df[num_cols]),\n",
    "            columns=num_cols, index=df.index\n",
    "        )\n",
    "        parts.append(X_num)\n",
    "    if cat_cols:\n",
    "        X_cat = pd.get_dummies(df[cat_cols], prefix=cat_cols, drop_first=False, dtype=float)\n",
    "        parts.append(X_cat)\n",
    "    if not parts:\n",
    "        raise ValueError(\"No hay columnas de features seleccionadas en modo visitas.\")\n",
    "\n",
    "    X = pd.concat(parts, axis=1).astype(float)\n",
    "\n",
    "    y_full = df[\"YEARS_TO_ONSET\"].astype(float)\n",
    "    label_mask_np = df[\"HAS_LABEL\"].fillna(False).to_numpy()\n",
    "    label_mask = torch.tensor(label_mask_np, dtype=torch.bool)\n",
    "\n",
    "    y_mu = float(y_full[label_mask_np].mean()) if label_mask_np.any() else 0.0\n",
    "    y_std = float(y_full[label_mask_np].std(ddof=0)) if label_mask_np.any() else 1.0\n",
    "    if y_std == 0 or np.isnan(y_std):\n",
    "        y_std = 1.0\n",
    "\n",
    "    y_scaled = (y_full - y_mu) / y_std\n",
    "    y_t = torch.tensor(y_scaled.fillna(0).values, dtype=torch.float32)\n",
    "\n",
    "    X_clean = X.replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "    n_samples = X_clean.shape[0]\n",
    "    k = min(8, max(1, n_samples - 1))\n",
    "    n_neighbors = min(n_samples, k + 1)\n",
    "\n",
    "    if n_samples >= 2:\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "        nbrs.fit(X_clean.values)\n",
    "        _, idx = nbrs.kneighbors(X_clean.values)\n",
    "        src_knn, dst_knn = [], []\n",
    "        for i in range(idx.shape[0]):\n",
    "            for j in idx[i, 1:]:\n",
    "                src_knn.append(i); dst_knn.append(j)\n",
    "        edge_knn = torch.tensor([src_knn, dst_knn], dtype=torch.long)\n",
    "    else:\n",
    "        edge_knn = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    if \"RID\" not in df.columns:\n",
    "        raise ValueError(\"PTDEMOG no tiene RID; necesario para aristas temporales.\")\n",
    "    tmp = df.reset_index()[[\"index\",\"RID\", date_col]].dropna(subset=[date_col])\n",
    "    tmp = tmp.sort_values([\"RID\", date_col])\n",
    "    src_tmp, dst_tmp = [], []\n",
    "    for rid, g in tmp.groupby(\"RID\"):\n",
    "        ids = g[\"index\"].tolist()\n",
    "        for a, b in zip(ids[:-1], ids[1:]):\n",
    "            src_tmp.append(a); dst_tmp.append(b)\n",
    "    edge_tmp = torch.tensor([src_tmp, dst_tmp], dtype=torch.long) if src_tmp else torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "    def undirected(e): \n",
    "        return torch.cat([e, e.flip(0)], dim=1) if e.numel() else e\n",
    "\n",
    "    edges = []\n",
    "    if edge_knn.numel(): edges.append(undirected(edge_knn))\n",
    "    if edge_tmp.numel(): edges.append(undirected(edge_tmp))\n",
    "    edge_index = torch.cat(edges, dim=1) if edges else torch.empty((2,0), dtype=torch.long)\n",
    "    if edge_index.numel():\n",
    "        edge_index = torch.unique(edge_index, dim=1)\n",
    "\n",
    "else:\n",
    "    print(\">> Modo PACIENTE (fallback: no hay EXAMDATE/VISDATE)\")\n",
    "    if \"RID\" not in df.columns:\n",
    "        raise ValueError(\"PTDEMOG no tiene RID.\")\n",
    "    keep_cols = [c for c in [\"RID\",\"PTDOBYY\",\"PTEDUCAT\",\"PTGENDER\",\"PTMARRY\",\"YEAR_ONSET\"] if c in df.columns]\n",
    "    df = df[keep_cols].copy()\n",
    "\n",
    "    def first_notnull(s):\n",
    "        return next((v for v in s if pd.notna(v)), np.nan)\n",
    "    agg = {\"PTDOBYY\":\"median\",\"PTEDUCAT\":\"median\",\"PTGENDER\":first_notnull,\"PTMARRY\":first_notnull,\"YEAR_ONSET\":\"min\"}\n",
    "    df = df.groupby(\"RID\", as_index=False).agg({k:v for k,v in agg.items() if k in df.columns})\n",
    "\n",
    "    num_cols = [c for c in [\"PTDOBYY\",\"PTEDUCAT\"] if c in df.columns]\n",
    "    cat_cols = [c for c in [\"PTGENDER\",\"PTMARRY\"] if c in df.columns]\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(df[c].median())\n",
    "\n",
    "    parts = []\n",
    "    if num_cols:\n",
    "        scaler = StandardScaler()\n",
    "        X_num = pd.DataFrame(scaler.fit_transform(df[num_cols]), columns=num_cols, index=df.index); parts.append(X_num)\n",
    "    if cat_cols:\n",
    "        X_cat = pd.get_dummies(df[cat_cols], prefix=cat_cols, drop_first=False, dtype=float); parts.append(X_cat)\n",
    "    if not parts:\n",
    "        raise ValueError(\"No hay features en modo paciente.\")\n",
    "    X = pd.concat(parts, axis=1).astype(float)\n",
    "\n",
    "    y_full = to_year(df[\"YEAR_ONSET\"]).astype(float)\n",
    "    label_mask_np = y_full.notna().to_numpy()\n",
    "    label_mask = torch.tensor(label_mask_np, dtype=torch.bool)\n",
    "\n",
    "    y_mu = float(y_full[label_mask_np].mean()) if label_mask_np.any() else 0.0\n",
    "    y_std = float(y_full[label_mask_np].std(ddof=0)) if label_mask_np.any() else 1.0\n",
    "    if y_std == 0 or np.isnan(y_std): y_std = 1.0\n",
    "\n",
    "    y_scaled = (y_full - y_mu) / y_std\n",
    "    y_t = torch.tensor(y_scaled.fillna(0).values, dtype=torch.float32)\n",
    "\n",
    "    X_clean = X.replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "    n_samples = X_clean.shape[0]\n",
    "    k = min(8, max(1, n_samples - 1))\n",
    "    n_neighbors = min(n_samples, k + 1)\n",
    "    if n_samples >= 2:\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "        nbrs.fit(X_clean.values)\n",
    "        _, idx = nbrs.kneighbors(X_clean.values)\n",
    "        src, dst = [], []\n",
    "        for i in range(idx.shape[0]):\n",
    "            for j in idx[i, 1:]:\n",
    "                src.append(i); dst.append(j)\n",
    "        edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "        edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "x = torch.tensor(X_clean.values, dtype=torch.float32)\n",
    "n = x.size(0)\n",
    "\n",
    "labeled_idx = torch.nonzero(label_mask, as_tuple=False).view(-1)\n",
    "n_lab = labeled_idx.numel()\n",
    "print(f\"Nodos: {n} | etiquetados: {n_lab} | edges: {edge_index.size(1)}\")\n",
    "\n",
    "if n_lab > 0:\n",
    "    perm = labeled_idx[torch.randperm(n_lab)]\n",
    "    tr_n = max(1, int(0.7*n_lab)) if n_lab >= 3 else max(1, n_lab)\n",
    "    va_n = max(0, int(0.15*n_lab)) if n_lab >= 7 else 0\n",
    "    if tr_n + va_n > max(0, n_lab-1):\n",
    "        va_n = max(0, n_lab - 1 - tr_n)\n",
    "\n",
    "    train_idx = perm[:tr_n]\n",
    "    val_idx   = perm[tr_n:tr_n+va_n]\n",
    "    test_idx  = perm[tr_n+va_n:]\n",
    "else:\n",
    "    train_idx = val_idx = test_idx = torch.tensor([], dtype=torch.long)\n",
    "\n",
    "train_mask = torch.zeros(n, dtype=torch.bool); train_mask[train_idx] = True\n",
    "val_mask   = torch.zeros(n, dtype=torch.bool); val_mask[val_idx]     = True\n",
    "test_mask  = torch.zeros(n, dtype=torch.bool); test_mask[test_idx]   = True\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "print(data)\n",
    "\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = GNNRegressor(in_ch=data.num_node_features, hid=64, dropout=0.3).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def eval_metrics(split=\"val\", y_mu=0.0, y_std=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        mask = data.train_mask if split==\"train\" else (data.val_mask if split==\"val\" else data.test_mask)\n",
    "        if not mask.any():\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "        mae_scaled = torch.mean(torch.abs(out[mask] - data.y[mask])).item()\n",
    "        rmse_scaled = torch.sqrt(loss_fn(out[mask], data.y[mask])).item()\n",
    "        mae_years = mae_scaled * y_std\n",
    "        rmse_years = rmse_scaled * y_std\n",
    "        return mae_years, rmse_years\n",
    "\n",
    "for ep in range(1, 61):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    if data.train_mask.any():\n",
    "        loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            raise RuntimeError(\"Loss NaN/Inf; revisa X/y/fechas.\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_val = loss.detach().item()\n",
    "    else:\n",
    "        loss_val = float(\"nan\")\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        tr_mae, tr_rmse = eval_metrics(\"train\", y_mu, y_std)\n",
    "        va_mae, va_rmse = eval_metrics(\"val\", y_mu, y_std)\n",
    "        te_mae, te_rmse = eval_metrics(\"test\", y_mu, y_std)\n",
    "        print(\n",
    "            f\"ep {ep:03d} | train_loss(MSE_scaled) {loss_val:.4f} \"\n",
    "            f\"| TR MAE {tr_mae:.3f}y RMSE {tr_rmse:.3f}y \"\n",
    "            f\"| VAL MAE {va_mae:.3f}y RMSE {va_rmse:.3f}y \"\n",
    "            f\"| TEST MAE {te_mae:.3f}y RMSE {te_rmse:.3f}y\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "748c4ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Modo VISITAS (usando VISDATE como fecha de visita)\n",
      "Nodos: 6210 | etiquetados (pre-evento): 82 | edges totales: 72424\n",
      "RIDs con etiqueta: 82 | train_rids=57, val_rids=12, test_rids=13\n",
      "Labels -> train:57 val:12 test:13\n",
      "Edges intra-split: 71756\n",
      "ep 010 | train_loss(MSE_scaled) 0.8334 | TR MAE 0.046y RMSE 0.120y | VAL MAE 0.038y RMSE 0.056y | TEST MAE 0.033y RMSE 0.061y\n",
      "ep 020 | train_loss(MSE_scaled) 0.7920 | TR MAE 0.044y RMSE 0.115y | VAL MAE 0.036y RMSE 0.049y | TEST MAE 0.042y RMSE 0.072y\n",
      "ep 030 | train_loss(MSE_scaled) 0.7105 | TR MAE 0.040y RMSE 0.110y | VAL MAE 0.047y RMSE 0.068y | TEST MAE 0.043y RMSE 0.089y\n",
      "ep 040 | train_loss(MSE_scaled) 0.6600 | TR MAE 0.041y RMSE 0.105y | VAL MAE 0.049y RMSE 0.086y | TEST MAE 0.049y RMSE 0.114y\n",
      "ep 050 | train_loss(MSE_scaled) 0.5895 | TR MAE 0.037y RMSE 0.099y | VAL MAE 0.069y RMSE 0.110y | TEST MAE 0.056y RMSE 0.135y\n",
      "ep 060 | train_loss(MSE_scaled) 0.5556 | TR MAE 0.036y RMSE 0.095y | VAL MAE 0.068y RMSE 0.119y | TEST MAE 0.060y RMSE 0.155y\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "def norm_codes_to_labels(s: pd.Series, mapping: dict) -> pd.Series:\n",
    "    out = s.astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    out = out.map(mapping)\n",
    "    return out\n",
    "\n",
    "gender_map = {\"1\":\"male\",\"2\":\"female\",\"male\":\"male\",\"female\":\"female\",\"m\":\"male\",\"f\":\"female\"}\n",
    "marry_map  = {\"1\":\"married\",\"2\":\"widowed\",\"3\":\"divorced\",\"4\":\"never_married\",\"6\":\"domestic_partnership\"}\n",
    "\n",
    "def to_year(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s.where((s >= 1900) & (s <= 2100))\n",
    "    return s\n",
    "\n",
    "onset_cols = [c for c in [\"PTCOGBEG\",\"PTADBEG\",\"PTADDX\"] if c in df.columns]\n",
    "for c in onset_cols:\n",
    "    df[c] = to_year(df[c])\n",
    "\n",
    "def row_min_nonnull(row):\n",
    "    vals = [row[c] for c in onset_cols if pd.notna(row[c])]\n",
    "    return min(vals) if vals else np.nan\n",
    "\n",
    "df[\"YEAR_ONSET\"] = df.apply(row_min_nonnull, axis=1) if onset_cols else np.nan\n",
    "df[\"YEAR_ONSET\"] = to_year(df[\"YEAR_ONSET\"])\n",
    "\n",
    "for c in [\"PTDOBYY\",\"PTEDUCAT\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "if \"PTGENDER\" in df.columns:\n",
    "    df[\"PTGENDER\"] = norm_codes_to_labels(df[\"PTGENDER\"], gender_map)\n",
    "if \"PTMARRY\" in df.columns:\n",
    "    df[\"PTMARRY\"]  = norm_codes_to_labels(df[\"PTMARRY\"], marry_map)\n",
    "\n",
    "date_col = \"EXAMDATE\" if \"EXAMDATE\" in df.columns else (\"VISDATE\" if \"VISDATE\" in df.columns else None)\n",
    "has_visits = (\"RID\" in df.columns) and (date_col is not None)\n",
    "\n",
    "if not has_visits:\n",
    "    raise ValueError(\"No encuentro columna de fecha de visita (VISDATE/EXAMDATE) o RID en PTDEMOG.\")\n",
    "\n",
    "print(f\">> Modo VISITAS (usando {date_col} como fecha de visita)\")\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "df[\"EXAM_YEAR\"] = to_year(df[date_col].dt.year)\n",
    "\n",
    "df[\"AGE_AT_VISIT\"] = np.where(\n",
    "    df[\"EXAM_YEAR\"].notna() & df[\"PTDOBYY\"].notna(),\n",
    "    df[\"EXAM_YEAR\"] - df[\"PTDOBYY\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df[\"YEARS_TO_ONSET\"] = np.where(\n",
    "    df[\"YEAR_ONSET\"].notna() & df[\"EXAM_YEAR\"].notna(),\n",
    "    df[\"YEAR_ONSET\"] - df[\"EXAM_YEAR\"],\n",
    "    np.nan\n",
    ")\n",
    "df.loc[(df[\"YEARS_TO_ONSET\"] < 0) & df[\"YEAR_ONSET\"].notna(), \"YEARS_TO_ONSET\"] = np.nan\n",
    "df.loc[df[\"YEARS_TO_ONSET\"] > 50, \"YEARS_TO_ONSET\"] = np.nan\n",
    "\n",
    "df[\"HAS_LABEL\"] = df[\"YEARS_TO_ONSET\"].notna()\n",
    "\n",
    "num_cols = [c for c in [\"AGE_AT_VISIT\",\"PTEDUCAT\"] if c in df.columns]\n",
    "cat_cols = [c for c in [\"PTGENDER\",\"PTMARRY\"] if c in df.columns]\n",
    "\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(df[c].median())\n",
    "\n",
    "parts = []\n",
    "if num_cols:\n",
    "    scaler = StandardScaler()\n",
    "    X_num = pd.DataFrame(\n",
    "        scaler.fit_transform(df[num_cols]),\n",
    "        columns=num_cols, index=df.index\n",
    "    )\n",
    "    parts.append(X_num)\n",
    "if cat_cols:\n",
    "    X_cat = pd.get_dummies(df[cat_cols], prefix=cat_cols, drop_first=False, dtype=float)\n",
    "    parts.append(X_cat)\n",
    "if not parts:\n",
    "    raise ValueError(\"No hay columnas de features seleccionadas en modo visitas.\")\n",
    "\n",
    "X = pd.concat(parts, axis=1).astype(float)\n",
    "X_clean = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "n_samples = X_clean.shape[0]\n",
    "k = min(8, max(1, n_samples - 1))\n",
    "n_neighbors = min(n_samples, k + 1)\n",
    "\n",
    "if n_samples >= 2:\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "    nbrs.fit(X_clean.values)\n",
    "    _, idx = nbrs.kneighbors(X_clean.values)\n",
    "    src_knn, dst_knn = [], []\n",
    "    for i in range(idx.shape[0]):\n",
    "        for j in idx[i, 1:]:\n",
    "            src_knn.append(i); dst_knn.append(j)\n",
    "    edge_knn = torch.tensor([src_knn, dst_knn], dtype=torch.long)\n",
    "else:\n",
    "    edge_knn = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "tmp = df.reset_index()[[\"index\",\"RID\", date_col]].dropna(subset=[date_col]).sort_values([\"RID\", date_col])\n",
    "src_tmp, dst_tmp = [], []\n",
    "for rid, g in tmp.groupby(\"RID\"):\n",
    "    ids = g[\"index\"].tolist()\n",
    "    for a, b in zip(ids[:-1], ids[1:]):\n",
    "        src_tmp.append(a); dst_tmp.append(b)\n",
    "edge_tmp = torch.tensor([src_tmp, dst_tmp], dtype=torch.long) if src_tmp else torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "def undirected(e): \n",
    "    return torch.cat([e, e.flip(0)], dim=1) if e.numel() else e\n",
    "\n",
    "edges = []\n",
    "if edge_knn.numel(): edges.append(undirected(edge_knn))\n",
    "if edge_tmp.numel(): edges.append(undirected(edge_tmp))\n",
    "edge_index = torch.cat(edges, dim=1) if edges else torch.empty((2,0), dtype=torch.long)\n",
    "if edge_index.numel():\n",
    "    edge_index = torch.unique(edge_index, dim=1)\n",
    "\n",
    "print(f\"Nodos: {len(df)} | etiquetados (pre-evento): {int(df['HAS_LABEL'].sum())} | edges totales: {edge_index.size(1)}\")\n",
    "\n",
    "df[\"USE_FOR_LABEL\"] = False\n",
    "if df[\"HAS_LABEL\"].any():\n",
    "    idx_last_pre = df.loc[df[\"HAS_LABEL\"]].groupby(\"RID\")[date_col].idxmax()\n",
    "    df.loc[idx_last_pre, \"USE_FOR_LABEL\"] = True\n",
    "\n",
    "rids_with_label = df.loc[df[\"USE_FOR_LABEL\"], \"RID\"].dropna().unique()\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(rids_with_label)\n",
    "n_lab_rids = len(rids_with_label)\n",
    "tr_n = max(1, int(0.7 * n_lab_rids))\n",
    "va_n = max(0, int(0.15 * n_lab_rids))\n",
    "if tr_n + va_n > max(0, n_lab_rids - 1):\n",
    "    va_n = max(0, n_lab_rids - 1 - tr_n)\n",
    "\n",
    "train_rids = set(rids_with_label[:tr_n])\n",
    "val_rids   = set(rids_with_label[tr_n:tr_n+va_n])\n",
    "test_rids  = set(rids_with_label[tr_n+va_n:])\n",
    "\n",
    "node_split = np.full(len(df), \"train\", dtype=object)\n",
    "node_rids = df[\"RID\"].to_numpy()\n",
    "node_split[np.isin(node_rids, list(val_rids))]  = \"val\"\n",
    "node_split[np.isin(node_rids, list(test_rids))] = \"test\"\n",
    "\n",
    "use_for_label = df[\"USE_FOR_LABEL\"].to_numpy()\n",
    "train_mask_np = (node_split == \"train\") & use_for_label\n",
    "val_mask_np   = (node_split == \"val\")   & use_for_label\n",
    "test_mask_np  = (node_split == \"test\")  & use_for_label\n",
    "\n",
    "split_map = {\"train\":0, \"val\":1, \"test\":2}\n",
    "split_idx = np.vectorize(split_map.get)(node_split)\n",
    "src_np = edge_index[0].cpu().numpy()\n",
    "dst_np = edge_index[1].cpu().numpy()\n",
    "keep_edges = split_idx[src_np] == split_idx[dst_np]\n",
    "edge_index = edge_index[:, torch.tensor(keep_edges)]\n",
    "\n",
    "y_full = df[\"YEARS_TO_ONSET\"].astype(float)  # NaNs fuera de USE_FOR_LABEL no importan\n",
    "y_mu  = float(y_full[train_mask_np].mean()) if train_mask_np.any() else 0.0\n",
    "y_std = float(y_full[train_mask_np].std(ddof=0)) if train_mask_np.any() else 1.0\n",
    "if not np.isfinite(y_std) or y_std == 0.0:\n",
    "    y_std = 1.0\n",
    "\n",
    "y_scaled = (y_full - y_mu) / y_std\n",
    "y_t = torch.tensor(y_scaled.fillna(0).values, dtype=torch.float32)\n",
    "\n",
    "x = torch.tensor(X_clean.values, dtype=torch.float32)\n",
    "train_mask = torch.tensor(train_mask_np, dtype=torch.bool)\n",
    "val_mask   = torch.tensor(val_mask_np,   dtype=torch.bool)\n",
    "test_mask  = torch.tensor(test_mask_np,  dtype=torch.bool)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "print(f\"RIDs con etiqueta: {n_lab_rids} | \"\n",
    "      f\"train_rids={len(train_rids)}, val_rids={len(val_rids)}, test_rids={len(test_rids)}\")\n",
    "print(f\"Labels -> train:{train_mask.sum().item()} val:{val_mask.sum().item()} test:{test_mask.sum().item()}\")\n",
    "print(f\"Edges intra-split: {edge_index.size(1)}\")\n",
    "\n",
    "Y_MEAN_TRAIN, Y_STD_TRAIN = y_mu, y_std\n",
    "\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = GNNRegressor(in_ch=data.num_node_features, hid=64, dropout=0.3).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def eval_metrics(split=\"val\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        mask = data.train_mask if split==\"train\" else (data.val_mask if split==\"val\" else data.test_mask)\n",
    "        if not mask.any():\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "        mae_scaled = torch.mean(torch.abs(out[mask] - data.y[mask])).item()\n",
    "        rmse_scaled = torch.sqrt(loss_fn(out[mask], data.y[mask])).item()\n",
    "        return mae_scaled * Y_STD_TRAIN, rmse_scaled * Y_STD_TRAIN  # en AÑOS\n",
    "\n",
    "for ep in range(1, 61):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    if data.train_mask.any():\n",
    "        loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            raise RuntimeError(\"Loss NaN/Inf; revisa X/y/fechas.\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_val = loss.detach().item()\n",
    "    else:\n",
    "        loss_val = float(\"nan\")\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        tr_mae, tr_rmse = eval_metrics(\"train\")\n",
    "        va_mae, va_rmse = eval_metrics(\"val\")\n",
    "        te_mae, te_rmse = eval_metrics(\"test\")\n",
    "        print(\n",
    "            f\"ep {ep:03d} | train_loss(MSE_scaled) {loss_val:.4f} \"\n",
    "            f\"| TR MAE {tr_mae:.3f}y RMSE {tr_rmse:.3f}y \"\n",
    "            f\"| VAL MAE {va_mae:.3f}y RMSE {va_rmse:.3f}y \"\n",
    "            f\"| TEST MAE {te_mae:.3f}y RMSE {te_rmse:.3f}y\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545073e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e95e12db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Modo VISITAS (usando VISDATE como fecha de visita)\n",
      "Nodos: 6210 | etiquetados (pre-evento): 23 | edges totales: 72424\n",
      "RIDs con etiqueta: 23 | train_rids=16, val_rids=3, test_rids=4\n",
      "Labels -> train:16 val:3 test:4\n",
      "Edges intra-split: 72206\n",
      "ep 010 | train_loss(MSE_scaled) 1.0048 | TR MAE 0.115y RMSE 0.133y | VAL MAE 0.073y RMSE 0.081y | TEST MAE 0.188y RMSE 0.264y\n",
      "ep 020 | train_loss(MSE_scaled) 0.8418 | TR MAE 0.104y RMSE 0.120y | VAL MAE 0.071y RMSE 0.073y | TEST MAE 0.172y RMSE 0.245y\n",
      "ep 030 | train_loss(MSE_scaled) 0.6665 | TR MAE 0.089y RMSE 0.111y | VAL MAE 0.061y RMSE 0.073y | TEST MAE 0.161y RMSE 0.227y\n",
      "[BEST] TR MAE 0.092y RMSE 0.113y | VAL MAE 0.063y RMSE 0.072y | TEST MAE 0.164y RMSE 0.231y\n",
      "[Baseline VAL]  mean(TRAIN) MAE=0.077y | zero MAE=0.159y\n",
      "[Baseline TEST]  mean(TRAIN) MAE=0.190y | zero MAE=0.313y\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "def norm_codes_to_labels(s: pd.Series, mapping: dict) -> pd.Series:\n",
    "    out = s.astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    out = out.map(mapping)\n",
    "    return out\n",
    "\n",
    "gender_map = {\"1\":\"male\",\"2\":\"female\",\"male\":\"male\",\"female\":\"female\",\"m\":\"male\",\"f\":\"female\"}\n",
    "marry_map  = {\"1\":\"married\",\"2\":\"widowed\",\"3\":\"divorced\",\"4\":\"never_married\",\"6\":\"domestic_partnership\"}\n",
    "\n",
    "def to_year(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s.where((s >= 1900) & (s <= 2100))\n",
    "    return s\n",
    "\n",
    "onset_cols = [c for c in [\"PTCOGBEG\",\"PTADBEG\",\"PTADDX\"] if c in df.columns]\n",
    "for c in onset_cols:\n",
    "    df[c] = to_year(df[c])\n",
    "\n",
    "def row_min_nonnull(row):\n",
    "    vals = [row[c] for c in onset_cols if pd.notna(row[c])]\n",
    "    return min(vals) if vals else np.nan\n",
    "\n",
    "df[\"YEAR_ONSET\"] = df.apply(row_min_nonnull, axis=1) if onset_cols else np.nan\n",
    "df[\"YEAR_ONSET\"] = to_year(df[\"YEAR_ONSET\"])\n",
    "\n",
    "for c in [\"PTDOBYY\",\"PTEDUCAT\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "if \"PTGENDER\" in df.columns:\n",
    "    df[\"PTGENDER\"] = norm_codes_to_labels(df[\"PTGENDER\"], gender_map)\n",
    "if \"PTMARRY\" in df.columns:\n",
    "    df[\"PTMARRY\"]  = norm_codes_to_labels(df[\"PTMARRY\"], marry_map)\n",
    "\n",
    "date_col = \"EXAMDATE\" if \"EXAMDATE\" in df.columns else (\"VISDATE\" if \"VISDATE\" in df.columns else None)\n",
    "has_visits = (\"RID\" in df.columns) and (date_col is not None)\n",
    "if not has_visits:\n",
    "    raise ValueError(\"No encuentro columna de fecha de visita (VISDATE/EXAMDATE) o RID en PTDEMOG.\")\n",
    "\n",
    "print(f\">> Modo VISITAS (usando {date_col} como fecha de visita)\")\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "df[\"EXAM_YEAR\"] = to_year(df[date_col].dt.year)\n",
    "\n",
    "df[\"AGE_AT_VISIT\"] = np.where(\n",
    "    df[\"EXAM_YEAR\"].notna() & df[\"PTDOBYY\"].notna(),\n",
    "    df[\"EXAM_YEAR\"] - df[\"PTDOBYY\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df[\"ONSET_DATE\"] = pd.NaT\n",
    "mask_on = df[\"YEAR_ONSET\"].notna()\n",
    "df.loc[mask_on, \"ONSET_DATE\"] = pd.to_datetime(\n",
    "    df.loc[mask_on, \"YEAR_ONSET\"].astype(int).astype(str) + \"-07-01\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "df[\"YEARS_TO_ONSET\"] = np.where(\n",
    "    mask_on & df[date_col].notna(),\n",
    "    (df[\"ONSET_DATE\"] - df[date_col]).dt.days / 365.25,\n",
    "    np.nan\n",
    ")\n",
    "df.loc[df[\"YEARS_TO_ONSET\"] < 0, \"YEARS_TO_ONSET\"] = np.nan\n",
    "df.loc[df[\"YEARS_TO_ONSET\"] > 15, \"YEARS_TO_ONSET\"] = np.nan\n",
    "df.drop(columns=[\"ONSET_DATE\"], inplace=True)\n",
    "\n",
    "df[\"HAS_LABEL\"] = df[\"YEARS_TO_ONSET\"].notna()\n",
    "\n",
    "num_cols = [c for c in [\"AGE_AT_VISIT\",\"PTEDUCAT\"] if c in df.columns]\n",
    "cat_cols = [c for c in [\"PTGENDER\",\"PTMARRY\"] if c in df.columns]\n",
    "\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(df[c].median())\n",
    "\n",
    "parts = []\n",
    "if num_cols:\n",
    "    scaler = StandardScaler()\n",
    "    X_num = pd.DataFrame(\n",
    "        scaler.fit_transform(df[num_cols]),\n",
    "        columns=num_cols, index=df.index\n",
    "    )\n",
    "    parts.append(X_num)\n",
    "if cat_cols:\n",
    "    X_cat = pd.get_dummies(df[cat_cols], prefix=cat_cols, drop_first=False, dtype=float)\n",
    "    parts.append(X_cat)\n",
    "if not parts:\n",
    "    raise ValueError(\"No hay columnas de features seleccionadas en modo visitas.\")\n",
    "\n",
    "X = pd.concat(parts, axis=1).astype(float)\n",
    "X_clean = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "n_samples = X_clean.shape[0]\n",
    "k = min(8, max(1, n_samples - 1))\n",
    "n_neighbors = min(n_samples, k + 1)\n",
    "\n",
    "if n_samples >= 2:\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "    nbrs.fit(X_clean.values)\n",
    "    _, idx = nbrs.kneighbors(X_clean.values)\n",
    "    src_knn, dst_knn = [], []\n",
    "    for i in range(idx.shape[0]):\n",
    "        for j in idx[i, 1:]:\n",
    "            src_knn.append(i); dst_knn.append(j)\n",
    "    edge_knn = torch.tensor([src_knn, dst_knn], dtype=torch.long)\n",
    "else:\n",
    "    edge_knn = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "tmp = df.reset_index()[[\"index\",\"RID\", date_col]].dropna(subset=[date_col]).sort_values([\"RID\", date_col])\n",
    "src_tmp, dst_tmp = [], []\n",
    "for rid, g in tmp.groupby(\"RID\"):\n",
    "    ids = g[\"index\"].tolist()\n",
    "    for a, b in zip(ids[:-1], ids[1:]):\n",
    "        src_tmp.append(a); dst_tmp.append(b)\n",
    "edge_tmp = torch.tensor([src_tmp, dst_tmp], dtype=torch.long) if src_tmp else torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "def undirected(e): \n",
    "    return torch.cat([e, e.flip(0)], dim=1) if e.numel() else e\n",
    "\n",
    "edges = []\n",
    "if edge_knn.numel(): edges.append(undirected(edge_knn))\n",
    "if edge_tmp.numel(): edges.append(undirected(edge_tmp))\n",
    "edge_index = torch.cat(edges, dim=1) if edges else torch.empty((2,0), dtype=torch.long)\n",
    "if edge_index.numel():\n",
    "    edge_index = torch.unique(edge_index, dim=1)\n",
    "\n",
    "print(f\"Nodos: {len(df)} | etiquetados (pre-evento): {int(df['HAS_LABEL'].sum())} | edges totales: {edge_index.size(1)}\")\n",
    "\n",
    "df[\"USE_FOR_LABEL\"] = False\n",
    "if df[\"HAS_LABEL\"].any():\n",
    "    idx_last_pre = df.loc[df[\"HAS_LABEL\"]].groupby(\"RID\")[date_col].idxmax()\n",
    "    df.loc[idx_last_pre, \"USE_FOR_LABEL\"] = True\n",
    "\n",
    "rids_with_label = df.loc[df[\"USE_FOR_LABEL\"], \"RID\"].dropna().unique()\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(rids_with_label)\n",
    "n_lab_rids = len(rids_with_label)\n",
    "tr_n = max(1, int(0.7 * n_lab_rids))\n",
    "va_n = max(0, int(0.15 * n_lab_rids))\n",
    "if tr_n + va_n > max(0, n_lab_rids - 1):\n",
    "    va_n = max(0, n_lab_rids - 1 - tr_n)\n",
    "\n",
    "train_rids = set(rids_with_label[:tr_n])\n",
    "val_rids   = set(rids_with_label[tr_n:tr_n+va_n])\n",
    "test_rids  = set(rids_with_label[tr_n+va_n:])\n",
    "\n",
    "node_split = np.full(len(df), \"train\", dtype=object)\n",
    "node_rids = df[\"RID\"].to_numpy()\n",
    "node_split[np.isin(node_rids, list(val_rids))]  = \"val\"\n",
    "node_split[np.isin(node_rids, list(test_rids))] = \"test\"\n",
    "\n",
    "use_for_label = df[\"USE_FOR_LABEL\"].to_numpy()\n",
    "train_mask_np = (node_split == \"train\") & use_for_label\n",
    "val_mask_np   = (node_split == \"val\")   & use_for_label\n",
    "test_mask_np  = (node_split == \"test\")  & use_for_label\n",
    "\n",
    "split_map = {\"train\":0, \"val\":1, \"test\":2}\n",
    "split_idx = np.vectorize(split_map.get)(node_split)\n",
    "src_np = edge_index[0].cpu().numpy()\n",
    "dst_np = edge_index[1].cpu().numpy()\n",
    "keep_edges = split_idx[src_np] == split_idx[dst_np]\n",
    "edge_index = edge_index[:, torch.tensor(keep_edges)]\n",
    "\n",
    "y_full = df[\"YEARS_TO_ONSET\"].astype(float)  # años (fraccional)\n",
    "y_mu  = float(y_full[train_mask_np].mean()) if train_mask_np.any() else 0.0\n",
    "y_std = float(y_full[train_mask_np].std(ddof=0)) if train_mask_np.any() else 1.0\n",
    "if not np.isfinite(y_std) or y_std == 0.0:\n",
    "    y_std = 1.0\n",
    "\n",
    "y_scaled = (y_full - y_mu) / y_std\n",
    "y_t = torch.tensor(y_scaled.fillna(0).values, dtype=torch.float32)\n",
    "\n",
    "x = torch.tensor(X_clean.values, dtype=torch.float32)\n",
    "train_mask = torch.tensor(train_mask_np, dtype=torch.bool)\n",
    "val_mask   = torch.tensor(val_mask_np,   dtype=torch.bool)\n",
    "test_mask  = torch.tensor(test_mask_np,  dtype=torch.bool)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "print(f\"RIDs con etiqueta: {n_lab_rids} | \"\n",
    "      f\"train_rids={len(train_rids)}, val_rids={len(val_rids)}, test_rids={len(test_rids)}\")\n",
    "print(f\"Labels -> train:{train_mask.sum().item()} val:{val_mask.sum().item()} test:{test_mask.sum().item()}\")\n",
    "print(f\"Edges intra-split: {edge_index.size(1)}\")\n",
    "Y_MEAN_TRAIN, Y_STD_TRAIN = y_mu, y_std\n",
    "\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=32, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = GNNRegressor(in_ch=data.num_node_features, hid=32, dropout=0.5).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=5e-3, weight_decay=2e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def eval_metrics(split=\"val\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        mask = data.train_mask if split==\"train\" else (data.val_mask if split==\"val\" else data.test_mask)\n",
    "        if not mask.any():\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "        mae_scaled = torch.mean(torch.abs(out[mask] - data.y[mask])).item()\n",
    "        rmse_scaled = torch.sqrt(loss_fn(out[mask], data.y[mask])).item()\n",
    "        return mae_scaled * Y_STD_TRAIN, rmse_scaled * Y_STD_TRAIN  # años\n",
    "\n",
    "best_val_rmse = float(\"inf\"); bad = 0; patience = 10\n",
    "best_state = None\n",
    "\n",
    "for ep in range(1, 121):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    if data.train_mask.any():\n",
    "        loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "        if torch.isnan(loss) or torch.isinf(loss): raise RuntimeError(\"Loss NaN/Inf; revisa X/y/fechas.\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_val = loss.detach().item()\n",
    "    else:\n",
    "        loss_val = float(\"nan\")\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        tr_mae, tr_rmse = eval_metrics(\"train\")\n",
    "        va_mae, va_rmse = eval_metrics(\"val\")\n",
    "        te_mae, te_rmse = eval_metrics(\"test\")\n",
    "        print(f\"ep {ep:03d} | train_loss(MSE_scaled) {loss_val:.4f} \"\n",
    "              f\"| TR MAE {tr_mae:.3f}y RMSE {tr_rmse:.3f}y \"\n",
    "              f\"| VAL MAE {va_mae:.3f}y RMSE {va_rmse:.3f}y \"\n",
    "              f\"| TEST MAE {te_mae:.3f}y RMSE {te_rmse:.3f}y\")\n",
    "\n",
    "    _, val_rmse = eval_metrics(\"val\")\n",
    "    if np.isfinite(val_rmse) and (val_rmse + 1e-4 < best_val_rmse):\n",
    "        best_val_rmse = val_rmse\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        bad = 0\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            break\n",
    "\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state)\n",
    "tr_mae, tr_rmse = eval_metrics(\"train\")\n",
    "va_mae, va_rmse = eval_metrics(\"val\")\n",
    "te_mae, te_rmse = eval_metrics(\"test\")\n",
    "print(f\"[BEST] TR MAE {tr_mae:.3f}y RMSE {tr_rmse:.3f}y | VAL MAE {va_mae:.3f}y RMSE {va_rmse:.3f}y | TEST MAE {te_mae:.3f}y RMSE {te_rmse:.3f}y\")\n",
    "\n",
    "y_years = y_full  # ya en años\n",
    "def baseline_mae(y, mask, const):\n",
    "    arr = y[mask].to_numpy()\n",
    "    if arr.size == 0: return float(\"nan\")\n",
    "    return float(np.mean(np.abs(arr - const)))\n",
    "\n",
    "train_mask_np = train_mask.cpu().numpy()\n",
    "val_mask_np   = val_mask.cpu().numpy()\n",
    "test_mask_np  = test_mask.cpu().numpy()\n",
    "\n",
    "for split, m in [(\"VAL\", val_mask_np), (\"TEST\", test_mask_np)]:\n",
    "    mae_mean = baseline_mae(y_years, m, Y_MEAN_TRAIN)\n",
    "    mae_zero = baseline_mae(y_years, m, 0.0)\n",
    "    print(f\"[Baseline {split}]  mean(TRAIN) MAE={mae_mean:.3f}y | zero MAE={mae_zero:.3f}y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b847cf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SSL] Positives (temporal): 2528\n",
      "[SSL] Pos train:1784 val:350 test:394\n",
      "ep 010 | loss 0.5473 | TR AUC 0.894 AP 0.876 | VAL AUC 0.902 AP 0.886 | TEST AUC 0.901 AP 0.884\n",
      "ep 020 | loss 0.5279 | TR AUC 0.902 AP 0.886 | VAL AUC 0.899 AP 0.888 | TEST AUC 0.897 AP 0.881\n",
      "ep 030 | loss 0.5023 | TR AUC 0.911 AP 0.888 | VAL AUC 0.893 AP 0.884 | TEST AUC 0.918 AP 0.907\n",
      "ep 040 | loss 0.4895 | TR AUC 0.918 AP 0.904 | VAL AUC 0.890 AP 0.873 | TEST AUC 0.920 AP 0.912\n",
      "ep 050 | loss 0.4847 | TR AUC 0.926 AP 0.912 | VAL AUC 0.948 AP 0.944 | TEST AUC 0.938 AP 0.925\n",
      "ep 060 | loss 0.4781 | TR AUC 0.937 AP 0.921 | VAL AUC 0.935 AP 0.923 | TEST AUC 0.934 AP 0.911\n",
      "ep 070 | loss 0.4928 | TR AUC 0.931 AP 0.915 | VAL AUC 0.929 AP 0.916 | TEST AUC 0.939 AP 0.926\n",
      "ep 080 | loss 0.4684 | TR AUC 0.946 AP 0.934 | VAL AUC 0.944 AP 0.937 | TEST AUC 0.944 AP 0.936\n",
      "ep 090 | loss 0.4755 | TR AUC 0.942 AP 0.930 | VAL AUC 0.950 AP 0.941 | TEST AUC 0.925 AP 0.904\n",
      "ep 100 | loss 0.4539 | TR AUC 0.944 AP 0.927 | VAL AUC 0.940 AP 0.915 | TEST AUC 0.949 AP 0.940\n",
      "ep 110 | loss 0.4732 | TR AUC 0.949 AP 0.935 | VAL AUC 0.947 AP 0.938 | TEST AUC 0.951 AP 0.943\n",
      "ep 120 | loss 0.4757 | TR AUC 0.947 AP 0.931 | VAL AUC 0.966 AP 0.961 | TEST AUC 0.954 AP 0.946\n",
      "Embeddings guardados en visit_embeddings_ssl.csv\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "tmp = df.reset_index()[[\"index\", \"RID\", date_col]].dropna(subset=[date_col]).sort_values([\"RID\", date_col])\n",
    "pos_src, pos_dst = [], []\n",
    "for rid, g in tmp.groupby(\"RID\"):\n",
    "    ids = g[\"index\"].tolist()\n",
    "    for a, b in zip(ids[:-1], ids[1:]):\n",
    "        pos_src += [a, b]   # bidireccional\n",
    "        pos_dst += [b, a]\n",
    "pos_edge = torch.tensor([pos_src, pos_dst], dtype=torch.long)\n",
    "print(f\"[SSL] Positives (temporal): {pos_edge.size(1)}\")\n",
    "\n",
    "rids = df[\"RID\"].dropna().unique()\n",
    "rng = np.random.default_rng(123)\n",
    "rng.shuffle(rids)\n",
    "n = len(rids)\n",
    "tr_n, va_n = max(1, int(0.7*n)), max(1, int(0.15*n))\n",
    "if tr_n + va_n > n - 1: va_n = max(1, n - 1 - tr_n)\n",
    "train_rids = set(rids[:tr_n]); val_rids = set(rids[tr_n:tr_n+va_n]); test_rids = set(rids[tr_n+va_n:])\n",
    "\n",
    "node_split = np.full(len(df), \"train\", dtype=object)\n",
    "node_rids = df[\"RID\"].to_numpy()\n",
    "node_split[np.isin(node_rids, list(val_rids))]  = \"val\"\n",
    "node_split[np.isin(node_rids, list(test_rids))] = \"test\"\n",
    "\n",
    "def mask_edges_by_split(e, split):\n",
    "    src, dst = e[0].numpy(), e[1].numpy()\n",
    "    keep = (np.array([node_split[s] for s in src]) == split) & (np.array([node_split[d] for d in dst]) == split)\n",
    "    keep = torch.tensor(keep)\n",
    "    return e[:, keep]\n",
    "\n",
    "pos_train = mask_edges_by_split(pos_edge, \"train\")\n",
    "pos_val   = mask_edges_by_split(pos_edge, \"val\")\n",
    "pos_test  = mask_edges_by_split(pos_edge, \"test\")\n",
    "print(f\"[SSL] Pos train:{pos_train.size(1)} val:{pos_val.size(1)} test:{pos_test.size(1)}\")\n",
    "\n",
    "pos_set = set((int(s), int(d)) for s, d in zip(pos_edge[0].tolist(), pos_edge[1].tolist()))\n",
    "\n",
    "def sample_negatives(num_edges, split):\n",
    "    idxs = np.where(node_split == split)[0]\n",
    "    neg = []\n",
    "    tries = 0\n",
    "    needed = num_edges\n",
    "    while len(neg) < needed and tries < needed * 50:\n",
    "        a, b = np.random.choice(idxs, size=2, replace=False)\n",
    "        tries += 1\n",
    "        if (a, b) in pos_set or (b, a) in pos_set: \n",
    "            continue\n",
    "        neg.append((a, b))\n",
    "    if not neg:\n",
    "        return torch.empty((2,0), dtype=torch.long)\n",
    "    arr = np.array(neg[:needed])\n",
    "    return torch.tensor(arr.T, dtype=torch.long)\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, hid)\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x  # embeddings\n",
    "\n",
    "def dot_decode(z, edges):\n",
    "    return (z[edges[0]] * z[edges[1]]).sum(dim=1)  # logits\n",
    "\n",
    "encoder = GCNEncoder(in_ch=X_clean.shape[1], hid=64, dropout=0.4).to(device)\n",
    "x = torch.tensor(X_clean.values, dtype=torch.float32, device=device)\n",
    "ei = edge_index.to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(encoder.parameters(), lr=5e-3, weight_decay=2e-4)\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def eval_link(z, pos_e, split):\n",
    "    if pos_e.size(1) == 0:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "    neg_e = sample_negatives(pos_e.size(1), split)\n",
    "    if neg_e.numel() == 0:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "    neg_e = neg_e.to(z.device)\n",
    "    with torch.no_grad():\n",
    "        pos_s = torch.sigmoid(dot_decode(z, pos_e.to(z.device))).cpu().numpy()\n",
    "        neg_s = torch.sigmoid(dot_decode(z, neg_e)).cpu().numpy()\n",
    "    y_true = np.concatenate([np.ones_like(pos_s), np.zeros_like(neg_s)])\n",
    "    y_score = np.concatenate([pos_s, neg_s])\n",
    "    return roc_auc_score(y_true, y_score), average_precision_score(y_true, y_score)\n",
    "\n",
    "best_val_auc, patience, bad = -1.0, 10, 0\n",
    "best_state = None\n",
    "\n",
    "for ep in range(1, 121):\n",
    "    encoder.train(); opt.zero_grad()\n",
    "    z = encoder(x, ei)\n",
    "    neg_train = sample_negatives(pos_train.size(1), \"train\").to(device)\n",
    "    if neg_train.numel() == 0:\n",
    "        break\n",
    "    pos_logits = dot_decode(z, pos_train.to(device))\n",
    "    neg_logits = dot_decode(z, neg_train)\n",
    "    y = torch.cat([torch.ones_like(pos_logits), torch.zeros_like(neg_logits)])\n",
    "    logits = torch.cat([pos_logits, neg_logits])\n",
    "    loss = bce(logits, y)\n",
    "    loss.backward(); opt.step()\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            z = encoder(x, ei)\n",
    "        tr_auc, tr_ap = eval_link(z, pos_train, \"train\")\n",
    "        va_auc, va_ap = eval_link(z, pos_val,   \"val\")\n",
    "        te_auc, te_ap = eval_link(z, pos_test,  \"test\")\n",
    "        print(f\"ep {ep:03d} | loss {loss.item():.4f} \"\n",
    "              f\"| TR AUC {tr_auc:.3f} AP {tr_ap:.3f} \"\n",
    "              f\"| VAL AUC {va_auc:.3f} AP {va_ap:.3f} \"\n",
    "              f\"| TEST AUC {te_auc:.3f} AP {te_ap:.3f}\")\n",
    "\n",
    "        if np.isfinite(va_auc) and va_auc > best_val_auc + 1e-4:\n",
    "            best_val_auc, bad = va_auc, 0\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in encoder.state_dict().items()}\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "if best_state is not None:\n",
    "    encoder.load_state_dict(best_state)\n",
    "\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    Z = encoder(x, ei).cpu().numpy()\n",
    "\n",
    "emb_df = pd.DataFrame(Z, index=df.index)\n",
    "emb_df.insert(0, \"RID\", df[\"RID\"].values)\n",
    "emb_df.insert(1, \"VISDATE\", pd.to_datetime(df[date_col]).dt.date.astype(\"string\"))\n",
    "emb_df.to_csv(\"visit_embeddings_ssl.csv\", index=False)\n",
    "print(\"Embeddings guardados en visit_embeddings_ssl.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a22a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}