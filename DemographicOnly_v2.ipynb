{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# GNN con Datos DemogrÃ¡ficos (Baseline)\n",
    "Este notebook es el **modelo baseline** que usa Ãºnicamente datos demogrÃ¡ficos:\n",
    "- AGE_AT_VISIT (Edad en la visita)\n",
    "- PTEDUCAT (AÃ±os de educaciÃ³n)\n",
    "- PTGENDER (GÃ©nero)\n",
    "- PTMARRY (Estado civil)\n",
    "\n",
    "**Objetivo**: Predecir aÃ±os hasta el inicio de sÃ­ntomas de Alzheimer usando GNN\n",
    "\n",
    "Este modelo servirÃ¡ como **referencia** para comparar con modelos que incluyen biomarcadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics loaded: (6210, 84)\n",
      "Columns: ['PHASE', 'PTID', 'RID', 'VISCODE', 'VISCODE2', 'VISDATE', 'PTSOURCE', 'PTGENDER', 'PTDOB', 'PTDOBYY']...\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"./data/adni/demographics/PTDEMOG.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Demographics loaded: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "utilities",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined.\n"
     ]
    }
   ],
   "source": [
    "def norm_codes_to_labels(s: pd.Series, mapping: dict) -> pd.Series:\n",
    "    out = s.astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    out = out.map(mapping)\n",
    "    return out\n",
    "\n",
    "gender_map = {\"1\":\"male\",\"2\":\"female\",\"male\":\"male\",\"female\":\"female\",\"m\":\"male\",\"f\":\"female\"}\n",
    "marry_map  = {\"1\":\"married\",\"2\":\"widowed\",\"3\":\"divorced\",\"4\":\"never_married\",\"6\":\"domestic_partnership\"}\n",
    "\n",
    "def to_year(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s.where((s >= 1900) & (s <= 2100))\n",
    "    return s\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "process_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data processed. Shape: (6210, 85)\n",
      "Patients with YEAR_ONSET: 2908\n"
     ]
    }
   ],
   "source": [
    "\n",
    "onset_cols = [c for c in [\"PTCOGBEG\",\"PTADBEG\",\"PTADDX\"] if c in df.columns]\n",
    "for c in onset_cols:\n",
    "    df[c] = to_year(df[c])\n",
    "\n",
    "def row_min_nonnull(row):\n",
    "    vals = [row[c] for c in onset_cols if pd.notna(row[c])]\n",
    "    return min(vals) if vals else np.nan\n",
    "\n",
    "df[\"YEAR_ONSET\"] = df.apply(row_min_nonnull, axis=1) if onset_cols else np.nan\n",
    "df[\"YEAR_ONSET\"] = to_year(df[\"YEAR_ONSET\"])\n",
    "\n",
    "for c in [\"PTDOBYY\",\"PTEDUCAT\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "if \"PTGENDER\" in df.columns:\n",
    "    df[\"PTGENDER\"] = norm_codes_to_labels(df[\"PTGENDER\"], gender_map)\n",
    "if \"PTMARRY\" in df.columns:\n",
    "    df[\"PTMARRY\"]  = norm_codes_to_labels(df[\"PTMARRY\"], marry_map)\n",
    "\n",
    "print(f\"\\nData processed. Shape: {df.shape}\")\n",
    "print(f\"Patients with YEAR_ONSET: {df['YEAR_ONSET'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prepare_visits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visits prepared: 6210\n",
      "Labeled visits (with YEARS_TO_ONSET): 82\n",
      "Using date column: VISDATE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "date_col = \"EXAMDATE\" if \"EXAMDATE\" in df.columns else (\"VISDATE\" if \"VISDATE\" in df.columns else None)\n",
    "if not date_col:\n",
    "    raise ValueError(\"No se encuentra columna de fecha (VISDATE/EXAMDATE)\")\n",
    "\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "df[\"EXAM_YEAR\"] = to_year(df[date_col].dt.year)\n",
    "\n",
    "df[\"AGE_AT_VISIT\"] = np.where(\n",
    "    df[\"EXAM_YEAR\"].notna() & df[\"PTDOBYY\"].notna(),\n",
    "    df[\"EXAM_YEAR\"] - df[\"PTDOBYY\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df[\"YEARS_TO_ONSET\"] = np.where(\n",
    "    df[\"YEAR_ONSET\"].notna() & df[\"EXAM_YEAR\"].notna(),\n",
    "    df[\"YEAR_ONSET\"] - df[\"EXAM_YEAR\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df.loc[(df[\"YEARS_TO_ONSET\"] < 0) & df[\"YEAR_ONSET\"].notna(), \"YEARS_TO_ONSET\"] = np.nan\n",
    "df.loc[df[\"YEARS_TO_ONSET\"] > 50, \"YEARS_TO_ONSET\"] = np.nan\n",
    "\n",
    "df[\"HAS_LABEL\"] = df[\"YEARS_TO_ONSET\"].notna()\n",
    "\n",
    "print(f\"\\nVisits prepared: {len(df)}\")\n",
    "print(f\"Labeled visits (with YEARS_TO_ONSET): {df['HAS_LABEL'].sum()}\")\n",
    "print(f\"Using date column: {date_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "build_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features (2): ['AGE_AT_VISIT', 'PTEDUCAT']\n",
      "Categorical features (2): ['PTGENDER', 'PTMARRY'] -> 7 one-hot columns\n",
      "\n",
      "ðŸ“Š BASELINE Feature matrix (Demographics only): (6210, 9)\n",
      "Total features: 9\n",
      "Feature list: ['AGE_AT_VISIT', 'PTEDUCAT', 'PTGENDER_female', 'PTGENDER_male', 'PTMARRY_divorced', 'PTMARRY_domestic_partnership', 'PTMARRY_married', 'PTMARRY_never_married', 'PTMARRY_widowed']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_cols = [c for c in [\"AGE_AT_VISIT\", \"PTEDUCAT\"] if c in df.columns]\n",
    "\n",
    "cat_cols = [c for c in [\"PTGENDER\", \"PTMARRY\"] if c in df.columns]\n",
    "\n",
    "for c in num_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(df[c].median())\n",
    "\n",
    "parts = []\n",
    "\n",
    "if num_cols:\n",
    "    scaler = StandardScaler()\n",
    "    X_num = pd.DataFrame(\n",
    "        scaler.fit_transform(df[num_cols]),\n",
    "        columns=num_cols,\n",
    "        index=df.index\n",
    "    )\n",
    "    parts.append(X_num)\n",
    "    print(f\"Numeric features ({len(num_cols)}): {num_cols}\")\n",
    "\n",
    "if cat_cols:\n",
    "    X_cat = pd.get_dummies(df[cat_cols], prefix=cat_cols, drop_first=False, dtype=float)\n",
    "    parts.append(X_cat)\n",
    "    print(f\"Categorical features ({len(cat_cols)}): {cat_cols} -> {X_cat.shape[1]} one-hot columns\")\n",
    "\n",
    "if not parts:\n",
    "    raise ValueError(\"No features available\")\n",
    "\n",
    "X = pd.concat(parts, axis=1).astype(float)\n",
    "X_clean = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "print(f\"\\nðŸ“Š BASELINE Feature matrix (Demographics only): {X_clean.shape}\")\n",
    "print(f\"Total features: {X_clean.shape[1]}\")\n",
    "print(f\"Feature list: {list(X_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "build_graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph constructed:\n",
      "  Nodes: 6210\n",
      "  kNN edges: 49680\n",
      "  Temporal edges: 1264\n",
      "  Total edges (undirected, unique): 72424\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_samples = X_clean.shape[0]\n",
    "k = min(8, max(1, n_samples - 1))\n",
    "n_neighbors = min(n_samples, k + 1)\n",
    "\n",
    "if n_samples >= 2:\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "    nbrs.fit(X_clean.values)\n",
    "    _, idx = nbrs.kneighbors(X_clean.values)\n",
    "    src_knn, dst_knn = [], []\n",
    "    for i in range(idx.shape[0]):\n",
    "        for j in idx[i, 1:]:\n",
    "            src_knn.append(i)\n",
    "            dst_knn.append(j)\n",
    "    edge_knn = torch.tensor([src_knn, dst_knn], dtype=torch.long)\n",
    "else:\n",
    "    edge_knn = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "tmp = df.reset_index()[[\"index\", \"RID\", date_col]].dropna(subset=[date_col]).sort_values([\"RID\", date_col])\n",
    "src_tmp, dst_tmp = [], []\n",
    "for rid, g in tmp.groupby(\"RID\"):\n",
    "    ids = g[\"index\"].tolist()\n",
    "    for a, b in zip(ids[:-1], ids[1:]):\n",
    "        src_tmp.append(a)\n",
    "        dst_tmp.append(b)\n",
    "edge_tmp = torch.tensor([src_tmp, dst_tmp], dtype=torch.long) if src_tmp else torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "def undirected(e):\n",
    "    return torch.cat([e, e.flip(0)], dim=1) if e.numel() else e\n",
    "\n",
    "edges = []\n",
    "if edge_knn.numel(): edges.append(undirected(edge_knn))\n",
    "if edge_tmp.numel(): edges.append(undirected(edge_tmp))\n",
    "edge_index = torch.cat(edges, dim=1) if edges else torch.empty((2,0), dtype=torch.long)\n",
    "if edge_index.numel():\n",
    "    edge_index = torch.unique(edge_index, dim=1)\n",
    "\n",
    "print(f\"\\nGraph constructed:\")\n",
    "print(f\"  Nodes: {len(df)}\")\n",
    "print(f\"  kNN edges: {edge_knn.size(1)}\")\n",
    "print(f\"  Temporal edges: {edge_tmp.size(1)}\")\n",
    "print(f\"  Total edges (undirected, unique): {edge_index.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "splits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splits created:\n",
      "  RIDs with labels: 82\n",
      "  Train RIDs: 57\n",
      "  Val RIDs: 12\n",
      "  Test RIDs: 13\n",
      "\n",
      "  Labeled nodes:\n",
      "    Train: 57\n",
      "    Val: 12\n",
      "    Test: 13\n",
      "\n",
      "  Edges (intra-split): 71756\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df[\"USE_FOR_LABEL\"] = False\n",
    "if df[\"HAS_LABEL\"].any():\n",
    "    idx_last_pre = df.loc[df[\"HAS_LABEL\"]].groupby(\"RID\")[date_col].idxmax()\n",
    "    df.loc[idx_last_pre, \"USE_FOR_LABEL\"] = True\n",
    "\n",
    "rids_with_label = df.loc[df[\"USE_FOR_LABEL\"], \"RID\"].dropna().unique()\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(rids_with_label)\n",
    "n_lab_rids = len(rids_with_label)\n",
    "\n",
    "tr_n = max(1, int(0.7 * n_lab_rids))\n",
    "va_n = max(0, int(0.15 * n_lab_rids))\n",
    "if tr_n + va_n > max(0, n_lab_rids - 1):\n",
    "    va_n = max(0, n_lab_rids - 1 - tr_n)\n",
    "\n",
    "train_rids = set(rids_with_label[:tr_n])\n",
    "val_rids   = set(rids_with_label[tr_n:tr_n+va_n])\n",
    "test_rids  = set(rids_with_label[tr_n+va_n:])\n",
    "\n",
    "node_split = np.full(len(df), \"train\", dtype=object)\n",
    "node_rids = df[\"RID\"].to_numpy()\n",
    "node_split[np.isin(node_rids, list(val_rids))]  = \"val\"\n",
    "node_split[np.isin(node_rids, list(test_rids))] = \"test\"\n",
    "\n",
    "use_for_label = df[\"USE_FOR_LABEL\"].to_numpy()\n",
    "train_mask_np = (node_split == \"train\") & use_for_label\n",
    "val_mask_np   = (node_split == \"val\")   & use_for_label\n",
    "test_mask_np  = (node_split == \"test\")  & use_for_label\n",
    "\n",
    "split_map = {\"train\":0, \"val\":1, \"test\":2}\n",
    "split_idx = np.vectorize(split_map.get)(node_split)\n",
    "src_np = edge_index[0].cpu().numpy()\n",
    "dst_np = edge_index[1].cpu().numpy()\n",
    "keep_edges = split_idx[src_np] == split_idx[dst_np]\n",
    "edge_index = edge_index[:, torch.tensor(keep_edges)]\n",
    "\n",
    "print(f\"\\nSplits created:\")\n",
    "print(f\"  RIDs with labels: {n_lab_rids}\")\n",
    "print(f\"  Train RIDs: {len(train_rids)}\")\n",
    "print(f\"  Val RIDs: {len(val_rids)}\")\n",
    "print(f\"  Test RIDs: {len(test_rids)}\")\n",
    "print(f\"\\n  Labeled nodes:\")\n",
    "print(f\"    Train: {train_mask_np.sum()}\")\n",
    "print(f\"    Val: {val_mask_np.sum()}\")\n",
    "print(f\"    Test: {test_mask_np.sum()}\")\n",
    "print(f\"\\n  Edges (intra-split): {edge_index.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prepare_tensors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data object created:\n",
      "  x.shape: torch.Size([6210, 9])\n",
      "  edge_index.shape: torch.Size([2, 71756])\n",
      "  y.shape: torch.Size([6210])\n",
      "  Target scaling: mean=0.018, std=0.131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_full = df[\"YEARS_TO_ONSET\"].astype(float)\n",
    "y_mu  = float(y_full[train_mask_np].mean()) if train_mask_np.any() else 0.0\n",
    "y_std = float(y_full[train_mask_np].std(ddof=0)) if train_mask_np.any() else 1.0\n",
    "if not np.isfinite(y_std) or y_std == 0.0:\n",
    "    y_std = 1.0\n",
    "\n",
    "y_scaled = (y_full - y_mu) / y_std\n",
    "y_t = torch.tensor(y_scaled.fillna(0).values, dtype=torch.float32)\n",
    "\n",
    "x = torch.tensor(X_clean.values, dtype=torch.float32)\n",
    "train_mask = torch.tensor(train_mask_np, dtype=torch.bool)\n",
    "val_mask   = torch.tensor(val_mask_np,   dtype=torch.bool)\n",
    "test_mask  = torch.tensor(test_mask_np,  dtype=torch.bool)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_t,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "print(f\"\\nData object created:\")\n",
    "print(f\"  x.shape: {data.x.shape}\")\n",
    "print(f\"  edge_index.shape: {data.edge_index.shape}\")\n",
    "print(f\"  y.shape: {data.y.shape}\")\n",
    "print(f\"  Target scaling: mean={y_mu:.3f}, std={y_std:.3f}\")\n",
    "\n",
    "Y_MEAN_TRAIN, Y_STD_TRAIN = y_mu, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Model created (BASELINE - Demographics only):\n",
      "  Input features: 9\n",
      "  Hidden dim: 64\n",
      "  Architecture: GCNConv -> ReLU -> Dropout -> GCNConv -> Output\n",
      "  Model parameters: 705\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_ch, hid)\n",
    "        self.c2 = GCNConv(hid, 1)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.c2(x, edge_index)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = GNNRegressor(in_ch=data.num_node_features, hid=64, dropout=0.3).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "print(f\"\\nðŸ§  Model created (BASELINE - Demographics only):\")\n",
    "print(f\"  Input features: {data.num_node_features}\")\n",
    "print(f\"  Hidden dim: 64\")\n",
    "print(f\"  Architecture: GCNConv -> ReLU -> Dropout -> GCNConv -> Output\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting training...\n",
      "\n",
      "ep 010 | train_loss(MSE) 0.8084 | TR MAE 0.046y RMSE 0.119y | VAL MAE 0.039y RMSE 0.055y | TEST MAE 0.035y RMSE 0.060y\n",
      "ep 020 | train_loss(MSE) 0.7994 | TR MAE 0.044y RMSE 0.114y | VAL MAE 0.036y RMSE 0.053y | TEST MAE 0.042y RMSE 0.074y\n",
      "ep 030 | train_loss(MSE) 0.7035 | TR MAE 0.042y RMSE 0.109y | VAL MAE 0.043y RMSE 0.068y | TEST MAE 0.049y RMSE 0.096y\n",
      "ep 040 | train_loss(MSE) 0.6928 | TR MAE 0.039y RMSE 0.104y | VAL MAE 0.056y RMSE 0.088y | TEST MAE 0.052y RMSE 0.115y\n",
      "ep 050 | train_loss(MSE) 0.5927 | TR MAE 0.038y RMSE 0.099y | VAL MAE 0.065y RMSE 0.105y | TEST MAE 0.060y RMSE 0.136y\n",
      "ep 060 | train_loss(MSE) 0.5710 | TR MAE 0.035y RMSE 0.095y | VAL MAE 0.070y RMSE 0.120y | TEST MAE 0.061y RMSE 0.156y\n",
      "ep 070 | train_loss(MSE) 0.5550 | TR MAE 0.035y RMSE 0.092y | VAL MAE 0.065y RMSE 0.122y | TEST MAE 0.067y RMSE 0.167y\n",
      "ep 080 | train_loss(MSE) 0.5292 | TR MAE 0.034y RMSE 0.089y | VAL MAE 0.075y RMSE 0.139y | TEST MAE 0.069y RMSE 0.180y\n",
      "ep 090 | train_loss(MSE) 0.4469 | TR MAE 0.032y RMSE 0.087y | VAL MAE 0.075y RMSE 0.155y | TEST MAE 0.074y RMSE 0.195y\n",
      "ep 100 | train_loss(MSE) 0.4526 | TR MAE 0.038y RMSE 0.087y | VAL MAE 0.090y RMSE 0.175y | TEST MAE 0.083y RMSE 0.205y\n",
      "\n",
      "âœ… Training completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def eval_metrics(split=\"val\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        mask = data.train_mask if split==\"train\" else (data.val_mask if split==\"val\" else data.test_mask)\n",
    "        if not mask.any():\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "        mae_scaled = torch.mean(torch.abs(out[mask] - data.y[mask])).item()\n",
    "        rmse_scaled = torch.sqrt(loss_fn(out[mask], data.y[mask])).item()\n",
    "        return mae_scaled * Y_STD_TRAIN, rmse_scaled * Y_STD_TRAIN\n",
    "\n",
    "print(\"\\nðŸš€ Starting training...\\n\")\n",
    "\n",
    "for ep in range(1, 101):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    \n",
    "    if data.train_mask.any():\n",
    "        loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            raise RuntimeError(\"Loss NaN/Inf\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_val = loss.detach().item()\n",
    "    else:\n",
    "        loss_val = float(\"nan\")\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "        tr_mae, tr_rmse = eval_metrics(\"train\")\n",
    "        va_mae, va_rmse = eval_metrics(\"val\")\n",
    "        te_mae, te_rmse = eval_metrics(\"test\")\n",
    "        print(\n",
    "            f\"ep {ep:03d} | train_loss(MSE) {loss_val:.4f} \"\n",
    "            f\"| TR MAE {tr_mae:.3f}y RMSE {tr_rmse:.3f}y \"\n",
    "            f\"| VAL MAE {va_mae:.3f}y RMSE {va_rmse:.3f}y \"\n",
    "            f\"| TEST MAE {te_mae:.3f}y RMSE {te_rmse:.3f}y\"\n",
    "        )\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "final_eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“Š BASELINE RESULTS (Demographics Only)\n",
      "======================================================================\n",
      "Features used: AGE_AT_VISIT, PTEDUCAT, PTGENDER, PTMARRY\n",
      "Total features: 9\n",
      "\n",
      "Performance:\n",
      "  TRAIN | MAE: 0.038 years | RMSE: 0.087 years\n",
      "  VAL   | MAE: 0.090 years | RMSE: 0.175 years\n",
      "  TEST  | MAE: 0.083 years | RMSE: 0.205 years\n",
      "======================================================================\n",
      "\n",
      "ðŸ’¡ Next step: Run DemoAndBiomarkers.ipynb to see improvement with CSF biomarkers!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_out = model(data.x, data.edge_index)\n",
    "\n",
    "tr_mae, tr_rmse = eval_metrics(\"train\")\n",
    "va_mae, va_rmse = eval_metrics(\"val\")\n",
    "te_mae, te_rmse = eval_metrics(\"test\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š BASELINE RESULTS (Demographics Only)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Features used: AGE_AT_VISIT, PTEDUCAT, PTGENDER, PTMARRY\")\n",
    "print(f\"Total features: {data.num_node_features}\")\n",
    "print(\"\\nPerformance:\")\n",
    "print(f\"  TRAIN | MAE: {tr_mae:.3f} years | RMSE: {tr_rmse:.3f} years\")\n",
    "print(f\"  VAL   | MAE: {va_mae:.3f} years | RMSE: {va_rmse:.3f} years\")\n",
    "print(f\"  TEST  | MAE: {te_mae:.3f} years | RMSE: {te_rmse:.3f} years\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "baseline_results = {\n",
    "    'model': 'Demographics Only (Baseline)',\n",
    "    'features': data.num_node_features,\n",
    "    'train_mae': tr_mae,\n",
    "    'train_rmse': tr_rmse,\n",
    "    'val_mae': va_mae,\n",
    "    'val_rmse': va_rmse,\n",
    "    'test_mae': te_mae,\n",
    "    'test_rmse': te_rmse\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ’¡ Next step: Run DemoAndBiomarkers.ipynb to see improvement with CSF biomarkers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "save_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Results saved to: baseline_demographics_results.json\n",
      "\n",
      "You can compare these results with DemoAndBiomarkers.ipynb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "with open('baseline_demographics_results.json', 'w') as f:\n",
    "    json.dump(baseline_results, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Results saved to: baseline_demographics_results.json\")\n",
    "print(\"\\nYou can compare these results with DemoAndBiomarkers.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}